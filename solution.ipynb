{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import Levenshtein\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "JACCARD_MODE = \"ngram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregando os Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "user_input       0\n",
      "uf               0\n",
      "razaosocial      0\n",
      "nome_fantasia    0\n",
      "dtype: int64\n",
      "\n",
      "Total rows with any missing values: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "confs = yaml.safe_load(open(\"confs.yaml\"))\n",
    "predictors = confs[\"predictors\"] ### Importante! O cientista poderá usar apenas estas features para criar/aperfeiçoar o modelo\n",
    "text_target = confs[\"text_target\"]\n",
    "cols_to_keep = predictors + text_target\n",
    "df = pd.read_parquet(\"dados/train.parquet\")[cols_to_keep]\n",
    "df.to_csv(\"data.csv\")\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal rows with any missing values: {df.isnull().any(axis=1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpando os Dados\n",
    "\n",
    "Iremos remover palavras como \"S.A.\", \"LTDA\", \"LTDA.\", \"S/A\", \"S.A\", \"Ltda\", \"Ltda.\", \"S/A.\", \"S.A.\", \"S.A\", \"Ltda\" e \"Ltda\" dos nomes reais das empresas a serem previstos, usando a seguinte suposicao:\n",
    "\n",
    "- Suposicao 1: usuários tem o hábito de pesquisar por nomes de empresas sem essas palavras, então elas não devem ser consideradas na previsão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_text_cleaning(text, \n",
    "                               remove_accents=True,\n",
    "                               remove_stop_words=True, \n",
    "                               remove_company_suffixes=True,\n",
    "                               custom_stop_words=None,\n",
    "                               to_lowercase=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "    remove_accents (bool): Remove accents and normalize characters\n",
    "    remove_stop_words (bool): Remove Portuguese stop words\n",
    "    remove_company_suffixes (bool): Remove common company suffixes\n",
    "    custom_stop_words (set): Additional stop words to remove\n",
    "    to_lowercase (bool): Convert to lowercase\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. Remove accents and normalize characters\n",
    "    if remove_accents:\n",
    "        # Normalize unicode\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        \n",
    "        # Handle specific cases\n",
    "        text = text.replace('ç', 'c').replace('Ç', 'C')\n",
    "    \n",
    "    # 2. Convert to lowercase\n",
    "    if to_lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # 3. Remove company suffixes\n",
    "    if remove_company_suffixes:\n",
    "        patterns_to_remove = [\n",
    "        r'\\bS\\.?A\\.?\\b',           # S.A, SA, S.A., SA.\n",
    "        r'\\bS/A\\.?\\b',             # S/A, S/A.\n",
    "        r'\\bLTDA\\.?\\b',            # LTDA, LTDA.\n",
    "        r'\\bLIMITADA\\b',           # LIMITADA\n",
    "        r'\\bCIA\\.?\\b',             # CIA, CIA.\n",
    "        r'\\bCOMPANHIA\\b',          # COMPANHIA\n",
    "        r'\\bEMPRESA\\b',            # EMPRESA\n",
    "        r'\\bCOMERCIO\\b',           # COMERCIO\n",
    "        r'\\bSERVICOS?\\b',          # SERVICO, SERVICOS\n",
    "        r'\\bME\\b',                 # ME (Microempresa)\n",
    "        r'\\bEPP\\b',                # EPP (Empresa de Pequeno Porte)\n",
    "        r'\\bEIRELI\\b',             # EIRELI\n",
    "        r'\\bSOCIEDADE\\b',          # SOCIEDADE\n",
    "        r'ADMINISTRADORA\\b',       # ADMINISTRADORA\n",
    "        r'GERAL\\b',                # GERAL\n",
    "    ]\n",
    "        \n",
    "        for pattern in patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    if remove_stop_words:\n",
    "        portuguese_stop_words = {\n",
    "            'a', 'ao', 'aos', 'as', 'da', 'das', 'de', 'do', 'dos', 'e', 'em', 'na', \n",
    "            'nas', 'no', 'nos', 'o', 'os', 'para', 'por', 'com', 'um', 'uma', 'uns', \n",
    "            'umas', 'se', 'que', 'ou', 'mas', 'como', 'mais', 'muito', 'sua', 'seu',\n",
    "            'seus', 'suas', 'este', 'esta', 'estes', 'estas', 'esse', 'essa', 'esses',\n",
    "            'essas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'isso', 'aquilo'\n",
    "        }\n",
    "        \n",
    "        if custom_stop_words:\n",
    "            portuguese_stop_words.update(custom_stop_words)\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.lower() not in portuguese_stop_words]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    # 5. Clean up extra whitespace and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Multiple spaces to single space\n",
    "    text = text.strip()                   # Remove leading/trailing spaces\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "df['razaosocial'] = df['razaosocial'].apply(comprehensive_text_cleaning)\n",
    "df['nome_fantasia'] = df['nome_fantasia'].apply(comprehensive_text_cleaning)\n",
    "df['user_input'] = df['user_input'].apply(comprehensive_text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Análise de Métricas de Character Error Rate (CER), Word Error Rate (WER) e Distancia de Levenshtein\n",
    "\n",
    "- **Word Error Rate (WER)**: fórmula para calcular a taxa de erro a nível de palavras: \n",
    "  $$WER = \\frac{S + D + I}{N}$$\n",
    "  onde:\n",
    "  - $S$ é o número de substituições. Por exemplo, se o usuário digitou \"Empresa X\" e a referência é \"Empresa Y\", então há uma substituição.\n",
    "  - $D$ é o número de deleções. Por exemplo, se o usuário digitou \"Empresa\" e a referência é \"Empresa X\", então há uma deleção.\n",
    "  - $I$ é o número de inserções. Por exemplo, se o usuário digitou \"Empresa X Y\" e a referência é \"Empresa X\", então há uma inserção.\n",
    "  - $N$ é o número total de palavras na referência. Por exemplo, se a referência é \"Empresa X\", então $N$ é 2.\n",
    "\n",
    "- **Character Error Rate (CER)**: fórmula para calcular a taxa de erro a nível de caracteres:\n",
    "  $$CER = \\frac{S + D + I}{N}$$\n",
    "  onde:\n",
    "  - $S$ é o número de substituições. Por exemplo, se o usuário digitou \"EmpresaXY\" e a referência é \"EmpresaXZ\", então há uma substituição.\n",
    "  - $D$ é o número de deleções. Por exemplo, se o usuário digitou \"Empresa\" e a referência é \"EmpresaX\", então há uma deleção.\n",
    "  - $I$ é o número de inserções. Por exemplo, se o usuário digitou \"Empresa XY\" e a referência é \"Empresa X\", então há uma inserção.\n",
    "  - $N$ é o número total de caracteres na referência. Por exemplo, se a referência é \"Empresa X\", então $N$ é 9 (contando espaços).\n",
    "\n",
    "- **Distância de Levenshtein**: é uma métrica que mede a diferença entre duas sequências. É definida como o número mínimo de operações de edição (inserções, deleções ou substituições) necessárias para transformar uma sequência em outra.\n",
    "\n",
    "- **Similaridade de Jaccard**: é uma métrica que mede a similaridade entre dois conjuntos. É definida como o tamanho da interseção dividido pelo tamanho da união dos conjuntos.\n",
    "  $$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "\n",
    "Essas métricas serao úteis para avaliar o quão diferente os inputs de usuário (`user_input`) são dos outputs esperados `razaosocial` e `nome_fantasia` e também dos outputs nao esperados, i.e., de todas as empresas que nao correspondem ao input do usuário. \n",
    "\n",
    "Caso o `CER` e/ou  `WER` entre o `user_input` e dos outputs nao esperados seja significativamente maior do que o `CER` e/ou `WER` entre o `user_input` e dos outputs esperados, podemos concluir que o input do usuário é mais próximo dos outputs esperados do que dos outputs não esperados e utilizar a minimização de `CER` e `WER` como critério para selecionar a empresa correta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Calculando o CER, WER e a Distância de Levenshtein\n",
    "\n",
    "Vamos usar a implementação do pacote já importado `jiwer` para calcular CER e WER. Para a Distância de Levenshtein, vamos usar a função `distance` do pacote `Levenshtein`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Set, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from functools import wraps\n",
    "\n",
    "def validate_inputs(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator to handle input validation and error handling for text comparison metrics.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(reference: Union[str, float], hypothesis: Union[str, float], *args, **kwargs) -> float:\n",
    "        try:\n",
    "            # Handle NaN values\n",
    "            if pd.isna(reference) or pd.isna(hypothesis):\n",
    "                return np.nan\n",
    "            \n",
    "            # Convert to string and clean\n",
    "            reference = str(reference).strip()\n",
    "            hypothesis = str(hypothesis).strip()\n",
    "            \n",
    "            # Handle empty strings\n",
    "            if len(reference) == 0 or len(hypothesis) == 0:\n",
    "                return np.nan\n",
    "                \n",
    "            return func(reference, hypothesis, *args, **kwargs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating {func.__name__}: {e}\")\n",
    "            return np.nan\n",
    "            \n",
    "    return wrapper\n",
    "\n",
    "class TextMetrics:\n",
    "    \"\"\"\n",
    "    A class containing various text comparison metrics with input validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_cer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Character Error Rate.\"\"\"\n",
    "        return cer(reference, hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_wer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Word Error Rate.\"\"\"\n",
    "        return wer(reference, hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_normalized_levenshtein(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate normalized Levenshtein distance (0-1).\n",
    "        Returns:\n",
    "            float: Normalized Levenshtein distance between 0 and 1\n",
    "        \"\"\"\n",
    "        max_len = max(len(reference), len(hypothesis))\n",
    "        if max_len == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        distance = Levenshtein.distance(reference, hypothesis)\n",
    "        return distance / max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_character_set(text: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Convert text to a set of characters.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Set[str]: Set of characters from the input text\n",
    "        \"\"\"\n",
    "        return set(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_word_set(text: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Convert text to a set of words.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Set[str]: Set of words from the input text\n",
    "        \"\"\"\n",
    "        return set(text.lower().split())\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity between two sets.\n",
    "        \n",
    "        Args:\n",
    "            set1 (Set[str]): First set\n",
    "            set2 (Set[str]): Second set\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        if not set1 and not set2:  # Both sets are empty\n",
    "            return 1.0\n",
    "        if not set1 or not set2:   # One set is empty\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union\n",
    "\n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_jaccard_similarity_chars(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity based on character sets.\n",
    "        \n",
    "        Args:\n",
    "            reference (str): Reference text\n",
    "            hypothesis (str): Hypothesis text\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        ref_chars = TextMetrics._get_character_set(reference)\n",
    "        hyp_chars = TextMetrics._get_character_set(hypothesis)\n",
    "        return TextMetrics._calculate_jaccard_similarity(ref_chars, hyp_chars)\n",
    "\n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_jaccard_similarity_words(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity based on word sets.\n",
    "        \n",
    "        Args:\n",
    "            reference (str): Reference text\n",
    "            hypothesis (str): Hypothesis text\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        ref_words = TextMetrics._get_word_set(reference)\n",
    "        hyp_words = TextMetrics._get_word_set(hypothesis)\n",
    "        return TextMetrics._calculate_jaccard_similarity(ref_words, hyp_words)\n",
    "\n",
    "    @staticmethod \n",
    "    def _ngram_jaccard_similarity(reference: str, hypothesis: str, n=2):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity using character n-grams.\n",
    "        This handles inversions and some misspellings well.\n",
    "        \n",
    "        Args:\n",
    "            str1, str2: Input strings\n",
    "            n: N-gram size (2=bigrams, 3=trigrams, etc.)\n",
    "        \"\"\"\n",
    "        def get_ngrams(text, n):\n",
    "            \"\"\"Generate n-grams from text with padding.\"\"\"\n",
    "            # Add padding to capture beginning/end patterns\n",
    "            padded = '#' * (n-1) + text.lower() + '#' * (n-1)\n",
    "            return set(padded[i:i+n] for i in range(len(padded) - n + 1))\n",
    "        \n",
    "        ngrams1 = get_ngrams(reference, n)\n",
    "        ngrams2 = get_ngrams(hypothesis, n)\n",
    "        \n",
    "        intersection = len(ngrams1 & ngrams2)\n",
    "        union = len(ngrams1 | ngrams2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 1.0 if len(reference) == len(hypothesis) == 0 else 0.0\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def multi_ngram_jaccard_similarity(reference: str, hypothesis: str, ngram_sizes=[2, 3], weights=None):\n",
    "        \"\"\"\n",
    "        Combine multiple n-gram sizes for better robustness.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0] * len(ngram_sizes)\n",
    "        \n",
    "        if len(weights) != len(ngram_sizes):\n",
    "            raise ValueError(\"Number of weights must match number of n-gram sizes\")\n",
    "        \n",
    "        total_score = 0\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        for size, weight in zip(ngram_sizes, weights):\n",
    "            score = TextMetrics._ngram_jaccard_similarity(reference, hypothesis, size)\n",
    "            total_score += score * weight\n",
    "        \n",
    "        return total_score / total_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CER, WER e Distancia de Levenshtein: `user_input` vs Ground Truth (`razaosocial` e `nome_fantasia`)\n",
    "\n",
    "Vamos calcular o CER, o WER  e a distancia de Levenshtein entre o `user_input` e as colunas `razaosocial` e `nome_fantasia` do DataFrame e adicionar essas métricas como novas colunas no DataFrame. \n",
    "\n",
    "\n",
    "- `cer_razaosocial`: CER entre `user_input` e `razaosocial`\n",
    "- `wer_razaosocial`: WER entre `user_input` e `razaosocial`\n",
    "- `lev_dist__razaosocial`: Distância de Levenshtein entre `user_input` e `razaosocial`\n",
    "- `cer_nome_fantasia`: CER entre `user_input` e `nome_fantasia`\n",
    "- `wer_nome_fantasia`: WER entre `user_input` e `nome_fantasia`\n",
    "- `lev_dist__nome_fantasia`: Distância de Levenshtein entre `user_input` e `nome_fantasia`\n",
    "- `jac_sim_razaosocial`: Similaridade de Jaccard entre `user_input` e `razaosocial`\n",
    "- `jac_sim_nome_fantasia`: Similaridade de Jaccard entre `user_input` e `nome_fantasia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating CER, WER, Levenshtein distance, and Jaccard similarity...\n",
      "Calculating metrics for user_input vs razaosocial...\n",
      "Calculating metrics for user_input vs nome_fantasia...\n",
      "Calculations completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def calculate_metrics(row: pd.Series,\n",
    "                      reference_col: str,\n",
    "                      hypothesis_col: str,\n",
    "                      metrics: TextMetrics,\n",
    "                      jacc_mode: str) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate CER, WER, Levenshtein distance, and Jaccard similarity for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame\n",
    "        reference_col (str): Name of the column containing the reference text\n",
    "        hypothesis_col (str): Name of the column containing the hypothesis text\n",
    "        metrics (TextMetrics): An instance of the TextMetrics class\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: CER, WER, Levenshtein distance, and Jaccard similarity\n",
    "    \"\"\"\n",
    "    reference = row[reference_col]\n",
    "    hypothesis = row[hypothesis_col]\n",
    "    \n",
    "    cer = metrics.calculate_cer(reference, hypothesis)\n",
    "    wer = metrics.calculate_wer(reference, hypothesis)\n",
    "    levenshtein = metrics.calculate_normalized_levenshtein(reference, hypothesis)\n",
    "    if jacc_mode == \"char\":\n",
    "        jaccard = metrics.calculate_jaccard_similarity_chars(reference, hypothesis)\n",
    "    elif jacc_mode == \"word\":\n",
    "        jaccard = metrics.calculate_jaccard_similarity_words(reference, hypothesis)\n",
    "    elif jacc_mode == \"ngram\":\n",
    "        jaccard = metrics.multi_ngram_jaccard_similarity(reference, hypothesis, ngram_sizes=[2, 3], weights=[0.5, 0.5])\n",
    "    \n",
    "    return cer, wer, levenshtein, jaccard\n",
    "\n",
    "def apply_metrics(df: pd.DataFrame,\n",
    "                  reference_cols: List[str],\n",
    "                  hypothesis_col: str,\n",
    "                  metrics: TextMetrics,\n",
    "                  jacc_mode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply text metrics to multiple reference columns against a single hypothesis column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        reference_cols (List[str]): List of column names to use as reference texts\n",
    "        hypothesis_col (str): Column name to use as hypothesis text\n",
    "        metrics (TextMetrics): An instance of the TextMetrics class\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added metric columns\n",
    "    \"\"\"\n",
    "    for ref_col in reference_cols:\n",
    "        print(f\"Calculating metrics for {hypothesis_col} vs {ref_col}...\")\n",
    "        \n",
    "        cer_col = f'cer_{ref_col}'\n",
    "        wer_col = f'wer_{ref_col}'\n",
    "        lev_dist_col = f'lev_dist_{ref_col}'\n",
    "        jaccard_col = f'jacc_sim_{jacc_mode}_{ref_col}'\n",
    "        \n",
    "        df[[cer_col, wer_col, lev_dist_col, jaccard_col]] = df.apply(\n",
    "            lambda row: calculate_metrics(row, ref_col, hypothesis_col, metrics, jacc_mode),\n",
    "            axis=1,\n",
    "            result_type='expand'\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_error_rates(df: pd.DataFrame,\n",
    "                          jacc_mode: str = \"word\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate CER, WER, Levenshtein distance, and Jaccard \n",
    "    similarity for user_input vs razaosocial and nome_fantasia.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added metric columns\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating CER, WER, Levenshtein distance, and Jaccard similarity...\")\n",
    "    \n",
    "    metrics = TextMetrics()\n",
    "    reference_columns = ['razaosocial', 'nome_fantasia']\n",
    "    hypothesis_column = 'user_input'\n",
    "    \n",
    "    df = apply_metrics(df, reference_columns, hypothesis_column, metrics, jacc_mode=jacc_mode)\n",
    "    \n",
    "    print(\"Calculations completed!\")\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df = calculate_error_rates(df, jacc_mode=JACCARD_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AVERAGE ERROR RATES\n",
      "==================================================\n",
      "Average CER (user_input vs razaosocial): 0.6507\n",
      "Average WER (user_input vs razaosocial): 0.8398\n",
      "Average Levenshtein (user_input vs razaosocial): 0.6018\n",
      "Average CER (user_input vs nome_fantasia): 0.7442\n",
      "Average WER (user_input vs nome_fantasia): 0.8768\n",
      "Average Levenshtein (user_input vs nome_fantasia): 0.5012\n",
      "Average Jaccard Similarity (ngram level) (user_input vs razaosocial): 0.3157\n",
      "Average Jaccard Similarity (ngram level) (user_input vs nome_fantasia): 0.4107\n",
      "\n",
      "==================================================\n",
      "DETAILED STATISTICS\n",
      "==================================================\n",
      "\n",
      "CER Statistics:\n",
      "       cer_razaosocial  cer_nome_fantasia\n",
      "count    255450.000000      255081.000000\n",
      "mean          0.650650           0.744236\n",
      "std           3.815269          15.901897\n",
      "min           0.000000           0.000000\n",
      "25%           0.470588           0.333333\n",
      "50%           0.666667           0.588235\n",
      "75%           0.794872           0.791667\n",
      "max        1771.071429        7819.333333\n",
      "\n",
      "WER Statistics:\n",
      "       wer_razaosocial  wer_nome_fantasia\n",
      "count    255450.000000      255081.000000\n",
      "mean          0.839806           0.876791\n",
      "std           5.549818          23.454431\n",
      "min           0.000000           0.000000\n",
      "25%           0.600000           0.500000\n",
      "50%           0.833333           0.750000\n",
      "75%           1.000000           1.000000\n",
      "max        2345.800000       11729.000000\n",
      "\n",
      "Levenshtein Statistics:\n",
      "       lev_dist_razaosocial  lev_dist_nome_fantasia\n",
      "count         255450.000000           255081.000000\n",
      "mean               0.601753                0.501210\n",
      "std                0.230562                0.255569\n",
      "min                0.000000                0.000000\n",
      "25%                0.461538                0.333333\n",
      "50%                0.655172                0.538462\n",
      "75%                0.775000                0.708333\n",
      "max                1.000000                1.000000\n",
      "\n",
      "Jaccard Similarity Statistics:\n",
      "       jacc_sim_ngram_razaosocial  jacc_sim_ngram_nome_fantasia\n",
      "count               255471.000000                 255471.000000\n",
      "mean                     0.315650                      0.410746\n",
      "std                      0.244469                      0.258088\n",
      "min                      0.000000                      0.000000\n",
      "25%                      0.118823                      0.216667\n",
      "50%                      0.271825                      0.378247\n",
      "75%                      0.461353                      0.557276\n",
      "max                      1.000000                      1.000000\n"
     ]
    }
   ],
   "source": [
    "def analyze_metrics_statistics(df: pd.DataFrame):\n",
    "    \"\"\"Calculate and display statistical summaries\"\"\"\n",
    "    # Calculate average metrics\n",
    "    avg_cer_razaosocial = df['cer_razaosocial'].mean()\n",
    "    avg_wer_razaosocial = df['wer_razaosocial'].mean()\n",
    "    avg_lev_dist_razaosocial = df['lev_dist_razaosocial'].mean()\n",
    "    avg_cer_nome_fantasia = df['cer_nome_fantasia'].mean()\n",
    "    avg_wer_nome_fantasia = df['wer_nome_fantasia'].mean()\n",
    "    avg_lev_dist_nome_fantasia = df['lev_dist_nome_fantasia'].mean()\n",
    "    avg_jac_sim_razaosocial = df[f'jacc_sim_{JACCARD_MODE}_razaosocial'].mean()\n",
    "    avg_jac_sim_nome_fantasia = df[f'jacc_sim_{JACCARD_MODE}_nome_fantasia'].mean()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AVERAGE ERROR RATES\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Average CER (user_input vs razaosocial): {avg_cer_razaosocial:.4f}\")\n",
    "    print(f\"Average WER (user_input vs razaosocial): {avg_wer_razaosocial:.4f}\")\n",
    "    print(f\"Average Levenshtein (user_input vs razaosocial): {avg_lev_dist_razaosocial:.4f}\")\n",
    "    print(f\"Average CER (user_input vs nome_fantasia): {avg_cer_nome_fantasia:.4f}\")\n",
    "    print(f\"Average WER (user_input vs nome_fantasia): {avg_wer_nome_fantasia:.4f}\")\n",
    "    print(f\"Average Levenshtein (user_input vs nome_fantasia): {avg_lev_dist_nome_fantasia:.4f}\")\n",
    "    print(f\"Average Jaccard Similarity ({JACCARD_MODE} level) (user_input vs razaosocial): {avg_jac_sim_razaosocial:.4f}\")\n",
    "    print(f\"Average Jaccard Similarity ({JACCARD_MODE} level) (user_input vs nome_fantasia): {avg_jac_sim_nome_fantasia:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DETAILED STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nCER Statistics:\")\n",
    "    print(df[['cer_razaosocial', 'cer_nome_fantasia']].describe())\n",
    "    \n",
    "    print(\"\\nWER Statistics:\")\n",
    "    print(df[['wer_razaosocial', 'wer_nome_fantasia']].describe())\n",
    "\n",
    "    print(\"\\nLevenshtein Statistics:\")\n",
    "    print(df[['lev_dist_razaosocial', 'lev_dist_nome_fantasia']].describe())\n",
    "\n",
    "    print(\"\\nJaccard Similarity Statistics:\")\n",
    "    print(df[[f'jacc_sim_{JACCARD_MODE}_razaosocial', f'jacc_sim_{JACCARD_MODE}_nome_fantasia']].describe())\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'avg_cer_razaosocial': avg_cer_razaosocial,\n",
    "        'avg_wer_razaosocial': avg_wer_razaosocial,\n",
    "        'avg_cer_nome_fantasia': avg_cer_nome_fantasia,\n",
    "        'avg_wer_nome_fantasia': avg_wer_nome_fantasia,\n",
    "        'avg_lev_dist_razaosocial': avg_lev_dist_razaosocial,\n",
    "        'avg_lev_dist_nome_fantasia': avg_lev_dist_nome_fantasia,\n",
    "        'avg_jac_sim_razaosocial': avg_jac_sim_razaosocial,\n",
    "        'avg_jac_sim_nome_fantasia': avg_jac_sim_nome_fantasia\n",
    "    }\n",
    "\n",
    "metrics_stats = analyze_metrics_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 CER, WER e Distancia de Levshenstein: `user_input` vs Outras Empresas (Outputs Não Esperados)\n",
    "\n",
    "Vamos calcular CER, WER e a Distancia de Levshenstein entre o `user_input` e as colunas `razaosocial` e `nome_fantasia` de todas as outras empresas (outputs não esperados) e adicionar essas métricas como novas colunas no DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRetrieval:\n",
    "    \"\"\"\n",
    "    A class for text retrieval using various similarity metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_cer(user_input: str, candidates: List[str], top_k: int = 1) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using Character Error Rate (CER).\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their CER scores\n",
    "        \"\"\"\n",
    "        matches_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if pd.isna(candidate):\n",
    "                continue\n",
    "                \n",
    "            cer_score = TextMetrics.calculate_cer(candidate, user_input)\n",
    "            if not pd.isna(cer_score):\n",
    "                matches_scores.append((candidate, cer_score))\n",
    "        \n",
    "        # Sort by CER score (ascending - lower is better)\n",
    "        matches_scores.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores[:top_k]\n",
    "        best_matches = [match for match, _ in top_matches]\n",
    "        best_scores = [score for _, score in top_matches]\n",
    "        \n",
    "        return best_matches, best_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_matches_wer(user_input: str, candidates: List[str], top_k: int = 1) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using Word Error Rate (WER).\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their WER scores\n",
    "        \"\"\"\n",
    "        matches_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if pd.isna(candidate):\n",
    "                continue\n",
    "                \n",
    "            wer_score = TextMetrics.calculate_wer(candidate, user_input)\n",
    "            if not pd.isna(wer_score):\n",
    "                matches_scores.append((candidate, wer_score))\n",
    "        \n",
    "        # Sort by WER score (ascending - lower is better)\n",
    "        matches_scores.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores[:top_k]\n",
    "        best_matches = [match for match, _ in top_matches]\n",
    "        best_scores = [score for _, score in top_matches]\n",
    "        \n",
    "        return best_matches, best_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_matches_levenshtein(user_input: str, candidates: List[str], top_k: int = 1) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using normalized Levenshtein distance.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their Levenshtein distances\n",
    "        \"\"\"\n",
    "        matches_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if pd.isna(candidate):\n",
    "                continue\n",
    "                \n",
    "            distance = TextMetrics.calculate_normalized_levenshtein(candidate, user_input)\n",
    "            if not pd.isna(distance):\n",
    "                matches_scores.append((candidate, distance))\n",
    "        \n",
    "        # Sort by distance (ascending - lower is better)\n",
    "        matches_scores.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores[:top_k]\n",
    "        best_matches = [match for match, _ in top_matches]\n",
    "        best_scores = [score for _, score in top_matches]\n",
    "        \n",
    "        return best_matches, best_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_matches_jaccard_chars(user_input: str, candidates: List[str], top_k: int = 1) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using character-level Jaccard similarity.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their Jaccard similarity scores\n",
    "        \"\"\"\n",
    "        matches_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if pd.isna(candidate):\n",
    "                continue\n",
    "                \n",
    "            similarity = TextMetrics.calculate_jaccard_similarity_chars(candidate, user_input)\n",
    "            if not pd.isna(similarity):\n",
    "                matches_scores.append((candidate, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending - higher is better)\n",
    "        matches_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores[:top_k]\n",
    "        best_matches = [match for match, _ in top_matches]\n",
    "        best_scores = [score for _, score in top_matches]\n",
    "        \n",
    "        return best_matches, best_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_matches_jaccard_words(user_input: str, candidates: List[str], top_k: int = 1) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using word-level Jaccard similarity.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their Jaccard similarity scores\n",
    "        \"\"\"\n",
    "        matches_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if pd.isna(candidate):\n",
    "                continue\n",
    "                \n",
    "            similarity = TextMetrics.calculate_jaccard_similarity_words(candidate, user_input)\n",
    "            if not pd.isna(similarity):\n",
    "                matches_scores.append((candidate, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending - higher is better)\n",
    "        matches_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores[:top_k]\n",
    "        best_matches = [match for match, _ in top_matches]\n",
    "        best_scores = [score for _, score in top_matches]\n",
    "        \n",
    "        return best_matches, best_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_matches_ngram_jaccard(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using n-gram Jaccard similarity.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their Jaccard similarity scores\n",
    "        \"\"\"\n",
    "        \n",
    "        razao_social_cands = []\n",
    "        nome_fantasia_cands = []\n",
    "        razao_social_cand_sims = []\n",
    "        nome_fantasia_cand_sims = []\n",
    "        max_sims = []\n",
    "        for _, row in df.iterrows():\n",
    "            razaosocial_cand = row['razaosocial']\n",
    "            nome_fantasia_cand = row['nome_fantasia']\n",
    "            if pd.isna(razaosocial_cand) and pd.isna(nome_fantasia_cand):\n",
    "                continue\n",
    "                \n",
    "            razaosocial_cand_sim = TextMetrics.multi_ngram_jaccard_similarity(razaosocial_cand, user_input, ngram_sizes=[2, 3], weights=[0.5, 0.5])\n",
    "            nome_fantasia_cand_sim = TextMetrics.multi_ngram_jaccard_similarity(nome_fantasia_cand, user_input, ngram_sizes=[2, 3], weights=[0.5, 0.5])\n",
    "            max_similarity = max(razaosocial_cand_sim, nome_fantasia_cand_sim)\n",
    "            if razaosocial_cand_sim is not None and nome_fantasia_cand_sim is not None:\n",
    "                razao_social_cands.append(razaosocial_cand)\n",
    "                nome_fantasia_cands.append(nome_fantasia_cand)\n",
    "                razao_social_cand_sims.append(razaosocial_cand_sim)\n",
    "                nome_fantasia_cand_sims.append(nome_fantasia_cand_sim)\n",
    "                max_sims.append(max_similarity)     \n",
    "        \n",
    "        matches_scores = pd.DataFrame({\n",
    "            'razaosocial_cand': razao_social_cands,\n",
    "            'nome_fantasia_cand': nome_fantasia_cands,\n",
    "            'razaosocial_cand_sim': razao_social_cand_sims,\n",
    "            'nome_fantasia_cand_sim': nome_fantasia_cand_sims,\n",
    "            'max_similarity': max_sims\n",
    "        })\n",
    "        # Sort by similarity (descending - higher is better)\n",
    "        matches_scores.sort_values(by='max_similarity', ascending=False, inplace=True)\n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores.head(top_k)\n",
    "        best_razao_matches = top_matches['razaosocial_cand'].tolist()\n",
    "        best_nome_fantasia_matches = top_matches['nome_fantasia_cand'].tolist()\n",
    "        best_razao_scores = top_matches['razaosocial_cand_sim'].tolist()\n",
    "        best_nome_fantasia_scores = top_matches['nome_fantasia_cand_sim'].tolist()\n",
    "        \n",
    "        return best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores\n",
    "    @staticmethod\n",
    "    def find_best_matches_combined(user_input: str, candidates: List[str], top_k: int = 1, \n",
    "                                 weights: dict = None) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using a combination of multiple metrics.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            candidates (List[str]): List of candidate texts\n",
    "            top_k (int): Number of top matches to return\n",
    "            weights (dict): Weights for different metrics. Default uses equal weights.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[float]]: Best matches and their combined scores\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {\n",
    "                'cer': 0.0,\n",
    "                'wer': 0.0,\n",
    "                'levenshtein': 0.5,\n",
    "                'jaccard_chars': 0.0,\n",
    "                'jaccard_words': 0.0,  # Set to 0 if not used\n",
    "                'ngram_jaccard': 0.5    # Set to 0 if not used\n",
    "            }\n",
    "        \n",
    "        matches_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if pd.isna(candidate):\n",
    "                continue\n",
    "            \n",
    "            # Calculate all metrics\n",
    "            cer_score = TextMetrics.calculate_cer(candidate, user_input)\n",
    "            wer_score = TextMetrics.calculate_wer(candidate, user_input)\n",
    "            lev_score = TextMetrics.calculate_normalized_levenshtein(candidate, user_input)\n",
    "            jac_score = TextMetrics.calculate_jaccard_similarity_chars(candidate, user_input)\n",
    "            \n",
    "            # Skip if any metric failed\n",
    "            if any(pd.isna(score) for score in [cer_score, wer_score, lev_score, jac_score]):\n",
    "                continue\n",
    "            \n",
    "            # Normalize scores (convert distance metrics to similarity)\n",
    "            # For CER, WER, and Levenshtein: lower is better, so we use (1 - score)\n",
    "            # For Jaccard: higher is better, so we use score directly\n",
    "            normalized_cer = max(0, 1 - cer_score)\n",
    "            normalized_wer = max(0, 1 - wer_score)\n",
    "            normalized_lev = max(0, 1 - lev_score)\n",
    "            normalized_jac = jac_score\n",
    "            \n",
    "            # Calculate weighted combined score\n",
    "            combined_score = (\n",
    "                weights.get('cer', 0) * normalized_cer +\n",
    "                weights.get('wer', 0) * normalized_wer +\n",
    "                weights.get('levenshtein', 0) * normalized_lev +\n",
    "                weights.get('jaccard_chars', 0) * normalized_jac\n",
    "            )\n",
    "            \n",
    "            matches_scores.append((candidate, combined_score))\n",
    "        \n",
    "        # Sort by combined score (descending - higher is better)\n",
    "        matches_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores[:top_k]\n",
    "        best_matches = [match for match, _ in top_matches]\n",
    "        best_scores = [score for _, score in top_matches]\n",
    "        \n",
    "        return best_matches, best_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_accuracy_top_k(df: pd.DataFrame,\n",
    "                                      unique_razaosocial_dict: Dict[str, List[str]],\n",
    "                                      unique_nome_fantasia_dict: Dict[str, List[str]],\n",
    "                                      top_k: int=1,\n",
    "                                      sample_size: int=None,\n",
    "                                      include_jaccard: bool=True,\n",
    "                                      include_combined: bool=True,\n",
    "                                      combined_weights: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced evaluation with all available metrics including Jaccard similarity and combined metrics\"\"\"\n",
    "    \n",
    "    # Sample data if specified\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Using sample of {sample_size} records for evaluation\")\n",
    "    else:\n",
    "        df_sample = df\n",
    "        print(f\"Using all {len(df_sample)} records for evaluation\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Define base metrics\n",
    "    metrics_to_evaluate = ['cer', 'wer', 'levenshtein']\n",
    "    \n",
    "    # Add Jaccard metrics if requested\n",
    "    if include_jaccard:\n",
    "        metrics_to_evaluate.extend(['jaccard_chars', 'jaccard_words', 'jaccard_ngram'])\n",
    "    \n",
    "    # Add combined metric if requested\n",
    "    if include_combined:\n",
    "        metrics_to_evaluate.append('combined')\n",
    "    \n",
    "    # Set default weights for combined metric if not provided\n",
    "    if combined_weights is None:\n",
    "        if include_jaccard:\n",
    "            combined_weights = {\n",
    "                'cer': 0.2,\n",
    "                'wer': 0.2,\n",
    "                'levenshtein': 0.2,\n",
    "                'jaccard_chars': 0.2,\n",
    "                'jaccard_words': 0.2\n",
    "            }\n",
    "        else:\n",
    "            combined_weights = {\n",
    "                'cer': 0.33,\n",
    "                'wer': 0.33,\n",
    "                'levenshtein': 0.34\n",
    "            }\n",
    "    \n",
    "    print(f\"Evaluating retrieval accuracy with top-{top_k} results using metrics: {metrics_to_evaluate}\")\n",
    "    if include_combined:\n",
    "        print(f\"Combined metric weights: {combined_weights}\")\n",
    "    \n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Processing\"):\n",
    "        user_input = row['user_input']\n",
    "        uf = row['uf']\n",
    "        true_razaosocial = row['razaosocial']\n",
    "        true_nome_fantasia = row['nome_fantasia']\n",
    "        unique_razaosocial = unique_razaosocial_dict.get(uf, [])\n",
    "        unique_nome_fantasia = unique_nome_fantasia_dict.get(uf, [])\n",
    "        unique_nome_fantasia = df_sample[df_sample['uf'] == uf]['nome_fantasia'].unique().tolist()\n",
    "        result_row = {\n",
    "            'user_input': user_input,\n",
    "            'uf': uf,\n",
    "            'true_razaosocial': true_razaosocial,\n",
    "            'true_nome_fantasia': true_nome_fantasia,\n",
    "        }\n",
    "        \n",
    "        # Dictionary to store all matches for each metric\n",
    "        all_matches = {}\n",
    "        \n",
    "        # Find matches using individual metrics\n",
    "        all_matches['cer'] = {\n",
    "            'razaosocial': TextRetrieval.find_best_matches_cer(user_input, unique_razaosocial, top_k),\n",
    "            'nome_fantasia': TextRetrieval.find_best_matches_cer(user_input, unique_nome_fantasia, top_k)\n",
    "        }\n",
    "        \n",
    "        all_matches['wer'] = {\n",
    "            'razaosocial': TextRetrieval.find_best_matches_wer(user_input, unique_razaosocial, top_k),\n",
    "            'nome_fantasia': TextRetrieval.find_best_matches_wer(user_input, unique_nome_fantasia, top_k)\n",
    "        }\n",
    "        \n",
    "        all_matches['levenshtein'] = {\n",
    "            'razaosocial': TextRetrieval.find_best_matches_levenshtein(user_input, unique_razaosocial, top_k),\n",
    "            'nome_fantasia': TextRetrieval.find_best_matches_levenshtein(user_input, unique_nome_fantasia, top_k)\n",
    "        }\n",
    "        \n",
    "        if include_jaccard:\n",
    "            all_matches['jaccard_chars'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_jaccard_chars(user_input, unique_razaosocial, top_k),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_jaccard_chars(user_input, unique_nome_fantasia, top_k)\n",
    "            }\n",
    "            \n",
    "            all_matches['jaccard_words'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_jaccard_words(user_input, unique_razaosocial, top_k),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_jaccard_words(user_input, unique_nome_fantasia, top_k)\n",
    "            }\n",
    "            all_matches['jaccard_ngram'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_ngram_jaccard(user_input, unique_razaosocial, top_k),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_ngram_jaccard(user_input, unique_nome_fantasia, top_k)\n",
    "            }\n",
    "        \n",
    "        # Find matches using combined metric\n",
    "        if include_combined:\n",
    "            all_matches['combined'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_combined(user_input, unique_razaosocial, top_k, combined_weights),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_combined(user_input, unique_nome_fantasia, top_k, combined_weights)\n",
    "            }\n",
    "        \n",
    "        # Process results for each metric\n",
    "        for metric in metrics_to_evaluate:\n",
    "            for field in ['razaosocial', 'nome_fantasia']:\n",
    "                matches, scores = all_matches[metric][field]\n",
    "                true_value = true_razaosocial if field == 'razaosocial' else true_nome_fantasia\n",
    "                \n",
    "                # Top-k accuracy\n",
    "                top_k_pred = true_value in matches\n",
    "                metric_col = f'{field}_top_{top_k}_{metric}_pred'\n",
    "                result_row[metric_col] = top_k_pred\n",
    "\n",
    "                # Top-1 accuracy\n",
    "                top_1_pred = matches[0] == true_value if matches else False\n",
    "                metric_col = f'{field}_top_1_{metric}_pred'\n",
    "                result_row[metric_col] = top_1_pred\n",
    "                \n",
    "                # Ranking\n",
    "                rank = matches.index(true_value) + 1 if true_value in matches else 0\n",
    "                result_row[f'{field}_rank_{metric}'] = rank\n",
    "                \n",
    "                # Store the actual scores for analysis\n",
    "                if matches:\n",
    "                    result_row[f'best_{metric}_score_{field}'] = scores[0] if scores else None\n",
    "        \n",
    "        results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_combined_metric_performance(results_df: pd.DataFrame, \n",
    "                                        top_k: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze the performance of different metrics including the combined metric\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame returned from evaluate_retrieval_accuracy_top_k\n",
    "        top_k: Which top-k accuracy to analyze\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with performance metrics for each method\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract available metrics from column names\n",
    "    available_metrics = set()\n",
    "    \n",
    "    # Look for columns that match the pattern: {field}_top_{k}_{metric}_pred\n",
    "    for col in results_df.columns:\n",
    "        if '_top_' in col and col.endswith('_pred'):\n",
    "            # Split the column name to extract the metric\n",
    "            parts = col.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                # Find the position of 'top' in the parts\n",
    "                try:\n",
    "                    top_idx = parts.index('top')\n",
    "                    if top_idx + 2 < len(parts):  # Ensure we have enough parts after 'top'\n",
    "                        # Extract metric name (everything between the number and 'pred')\n",
    "                        metric_parts = parts[top_idx + 2:-1]  # Skip 'top', number, and 'pred'\n",
    "                        metric = '_'.join(metric_parts)\n",
    "                        available_metrics.add(metric)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Found metrics: {sorted(available_metrics)}\")\n",
    "    \n",
    "    performance_results = []\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        # Construct column names\n",
    "        razaosocial_top_k_col = f'razaosocial_top_{top_k}_{metric}_pred'\n",
    "        nome_fantasia_top_k_col = f'nome_fantasia_top_{top_k}_{metric}_pred'\n",
    "        razaosocial_top_1_col = f'razaosocial_top_1_{metric}_pred'\n",
    "        nome_fantasia_top_1_col = f'nome_fantasia_top_1_{metric}_pred'\n",
    "        razaosocial_rank_col = f'razaosocial_rank_{metric}'\n",
    "        nome_fantasia_rank_col = f'nome_fantasia_rank_{metric}'\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_cols = [razaosocial_top_k_col, nome_fantasia_top_k_col, \n",
    "                        razaosocial_top_1_col, nome_fantasia_top_1_col,\n",
    "                        razaosocial_rank_col, nome_fantasia_rank_col]\n",
    "        \n",
    "        missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing columns for metric '{metric}': {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Calculate top-k accuracy\n",
    "            razaosocial_top_k_accuracy = results_df[razaosocial_top_k_col].mean()\n",
    "            nome_fantasia_top_k_accuracy = results_df[nome_fantasia_top_k_col].mean()\n",
    "            overall_top_k_accuracy = (razaosocial_top_k_accuracy + nome_fantasia_top_k_accuracy) / 2\n",
    "            \n",
    "            # Calculate top-1 accuracy\n",
    "            razaosocial_top_1_accuracy = results_df[razaosocial_top_1_col].mean()\n",
    "            nome_fantasia_top_1_accuracy = results_df[nome_fantasia_top_1_col].mean()\n",
    "            overall_top_1_accuracy = (razaosocial_top_1_accuracy + nome_fantasia_top_1_accuracy) / 2\n",
    "            \n",
    "            # Calculate mean rank (lower is better, 0 means not found)\n",
    "            # Only consider non-zero ranks for mean calculation\n",
    "            razaosocial_ranks = results_df[razaosocial_rank_col]\n",
    "            nome_fantasia_ranks = results_df[nome_fantasia_rank_col]\n",
    "            \n",
    "            # Calculate mean rank excluding 0s (not found cases)\n",
    "            razaosocial_found_ranks = razaosocial_ranks[razaosocial_ranks > 0]\n",
    "            nome_fantasia_found_ranks = nome_fantasia_ranks[nome_fantasia_ranks > 0]\n",
    "            \n",
    "            mean_razaosocial_rank = razaosocial_found_ranks.mean() if len(razaosocial_found_ranks) > 0 else float('inf')\n",
    "            mean_nome_fantasia_rank = nome_fantasia_found_ranks.mean() if len(nome_fantasia_found_ranks) > 0 else float('inf')\n",
    "            \n",
    "            # Overall mean rank\n",
    "            if mean_razaosocial_rank != float('inf') and mean_nome_fantasia_rank != float('inf'):\n",
    "                overall_mean_rank = (mean_razaosocial_rank + mean_nome_fantasia_rank) / 2\n",
    "            elif mean_razaosocial_rank != float('inf'):\n",
    "                overall_mean_rank = mean_razaosocial_rank\n",
    "            elif mean_nome_fantasia_rank != float('inf'):\n",
    "                overall_mean_rank = mean_nome_fantasia_rank\n",
    "            else:\n",
    "                overall_mean_rank = float('inf')\n",
    "            \n",
    "            # Calculate retrieval rate (percentage of cases where target was found in top-k)\n",
    "            razaosocial_retrieval_rate = (razaosocial_ranks > 0).mean()\n",
    "            nome_fantasia_retrieval_rate = (nome_fantasia_ranks > 0).mean()\n",
    "            overall_retrieval_rate = (razaosocial_retrieval_rate + nome_fantasia_retrieval_rate) / 2\n",
    "            \n",
    "            performance_results.append({\n",
    "                'metric': metric,\n",
    "                f'razaosocial_top_{top_k}_accuracy': razaosocial_top_k_accuracy,\n",
    "                f'nome_fantasia_top_{top_k}_accuracy': nome_fantasia_top_k_accuracy,\n",
    "                f'overall_top_{top_k}_accuracy': overall_top_k_accuracy,\n",
    "                'razaosocial_top_1_accuracy': razaosocial_top_1_accuracy,\n",
    "                'nome_fantasia_top_1_accuracy': nome_fantasia_top_1_accuracy,\n",
    "                'overall_top_1_accuracy': overall_top_1_accuracy,\n",
    "                'mean_razaosocial_rank': mean_razaosocial_rank,\n",
    "                'mean_nome_fantasia_rank': mean_nome_fantasia_rank,\n",
    "                'overall_mean_rank': overall_mean_rank,\n",
    "                'razaosocial_retrieval_rate': razaosocial_retrieval_rate,\n",
    "                'nome_fantasia_retrieval_rate': nome_fantasia_retrieval_rate,\n",
    "                'overall_retrieval_rate': overall_retrieval_rate\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing metric '{metric}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not performance_results:\n",
    "        print(\"No valid metrics found for analysis\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_results)\n",
    "    \n",
    "    # Sort by overall top-1 accuracy (descending)\n",
    "    performance_df = performance_df.sort_values('overall_top_1_accuracy', ascending=False)\n",
    "    \n",
    "    # Round numeric columns for better readability\n",
    "    numeric_columns = performance_df.select_dtypes(include=[np.number]).columns\n",
    "    performance_df[numeric_columns] = performance_df[numeric_columns].round(4)\n",
    "    \n",
    "    return performance_df\n",
    "\n",
    "def print_performance_summary(performance_df: pd.DataFrame, top_k: int = 1):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the performance analysis\n",
    "    \n",
    "    Args:\n",
    "        performance_df: DataFrame from analyze_combined_metric_performance\n",
    "        top_k: The top-k value used in analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    if performance_df.empty:\n",
    "        print(\"No performance data to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PERFORMANCE SUMMARY - TOP-{top_k} RETRIEVAL ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\n📊 RANKING BY OVERALL TOP-1 ACCURACY:\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in performance_df.iterrows():\n",
    "        print(f\"{idx+1:2d}. {row['metric']:<15} | Top-1: {row['overall_top_1_accuracy']:.3f} | \"\n",
    "              f\"Top-{top_k}: {row[f'overall_top_{top_k}_accuracy']:.3f} | \"\n",
    "              f\"Avg Rank: {row['overall_mean_rank']:.2f}\")\n",
    "    \n",
    "    print(f\"\\n🎯 DETAILED BREAKDOWN:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in performance_df.head(5).iterrows():  # Show top 5 metrics\n",
    "        print(f\"\\n{row['metric'].upper()}:\")\n",
    "        print(f\"  • Razão Social    - Top-1: {row['razaosocial_top_1_accuracy']:.3f}, \"\n",
    "              f\"Top-{top_k}: {row[f'razaosocial_top_{top_k}_accuracy']:.3f}, \"\n",
    "              f\"Retrieval Rate: {row['razaosocial_retrieval_rate']:.3f}\")\n",
    "        print(f\"  • Nome Fantasia   - Top-1: {row['nome_fantasia_top_1_accuracy']:.3f}, \"\n",
    "              f\"Top-{top_k}: {row[f'nome_fantasia_top_{top_k}_accuracy']:.3f}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matches(all_matches: Dict[str, Tuple[List[str], List[float]]],\n",
    "                    uf: str,\n",
    "                    df: pd.DataFrame) -> Dict[str, Tuple[List[str], List[float]]]:\n",
    "    \"\"\"\n",
    "    Process matches to determine if the true values are in the top-k results.\n",
    "    \n",
    "        Dict: Dictionary with top-k and top-1 predictions for each metric and field\n",
    "    \"\"\" \n",
    "\n",
    "    for match, score in zip(*all_matches['razaosocial']):\n",
    "        if score == 0:\n",
    "            all_matches['razaosocial'] = ([match], [score])\n",
    "            all_matches['nome_fantasia'] = (df[(df['uf'] == uf) & (df['razaosocial'] == match)]['nome_fantasia'].tolist(), [0])\n",
    "            return all_matches\n",
    "    for match, score in zip(*all_matches['nome_fantasia']):\n",
    "        if score == 0:\n",
    "            all_matches['nome_fantasia'] = ([match], [score])\n",
    "            all_matches['razaosocial'] = (df[(df['uf'] == uf) & (df['nome_fantasia'] == match)]['razaosocial'].tolist(), [0])\n",
    "            return all_matches\n",
    "    return all_matches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_accuracy_top_k(df: pd.DataFrame,\n",
    "                                      unique_razaosocial_dict: Dict[str, List[str]],\n",
    "                                      unique_nome_fantasia_dict: Dict[str, List[str]],\n",
    "                                      top_k: int=1,\n",
    "                                      sample_size: int=None,\n",
    "                                      include_jaccard: bool=True,\n",
    "                                      include_combined: bool=True,\n",
    "                                      combined_weights: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced evaluation with all available metrics including Jaccard similarity and combined metrics\"\"\"\n",
    "    \n",
    "    # Sample data if specified\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Using sample of {sample_size} records for evaluation\")\n",
    "    else:\n",
    "        df_sample = df\n",
    "        print(f\"Using all {len(df_sample)} records for evaluation\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Define base metrics\n",
    "    metrics_to_evaluate = ['cer', 'wer', 'levenshtein']\n",
    "    \n",
    "    # Add Jaccard metrics if requested\n",
    "    if include_jaccard:\n",
    "        metrics_to_evaluate.extend(['jaccard_chars', 'jaccard_words', 'jaccard_ngram'])\n",
    "    \n",
    "    # Add combined metric if requested\n",
    "    if include_combined:\n",
    "        metrics_to_evaluate.append('combined')\n",
    "    \n",
    "    # Set default weights for combined metric if not provided\n",
    "    if combined_weights is None:\n",
    "        if include_jaccard:\n",
    "            combined_weights = {\n",
    "                'cer': 0.2,\n",
    "                'wer': 0.2,\n",
    "                'levenshtein': 0.2,\n",
    "                'jaccard_chars': 0.2,\n",
    "                'jaccard_words': 0.2\n",
    "            }\n",
    "        else:\n",
    "            combined_weights = {\n",
    "                'cer': 0.33,\n",
    "                'wer': 0.33,\n",
    "                'levenshtein': 0.34\n",
    "            }\n",
    "    \n",
    "    print(f\"Evaluating retrieval accuracy with top-{top_k} results using metrics: {metrics_to_evaluate}\")\n",
    "    if include_combined:\n",
    "        print(f\"Combined metric weights: {combined_weights}\")\n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Processing\"):\n",
    "        user_input = row['user_input']\n",
    "        uf = row['uf']\n",
    "        true_razaosocial = row['razaosocial']\n",
    "        true_nome_fantasia = row['nome_fantasia']\n",
    "        unique_razaosocial = unique_razaosocial_dict.get(uf, [])\n",
    "        unique_nome_fantasia = unique_nome_fantasia_dict.get(uf, [])\n",
    "        unique_nome_fantasia = df_sample[df_sample['uf'] == uf]['nome_fantasia'].unique().tolist()\n",
    "        result_row = {\n",
    "            'user_input': user_input,\n",
    "            'uf': uf,\n",
    "            'true_razaosocial': true_razaosocial,\n",
    "            'true_nome_fantasia': true_nome_fantasia,\n",
    "        }\n",
    "        \n",
    "        # Dictionary to store all matches for each metric\n",
    "        all_matches = {}\n",
    "        \n",
    "        # Find matches using individual metrics\n",
    "        all_matches['cer'] = {\n",
    "            'razaosocial': TextRetrieval.find_best_matches_cer(user_input, unique_razaosocial, top_k),\n",
    "            'nome_fantasia': TextRetrieval.find_best_matches_cer(user_input, unique_nome_fantasia, top_k),\n",
    "            'uf': uf\n",
    "        }\n",
    "        \n",
    "        all_matches['wer'] = {\n",
    "            'razaosocial': TextRetrieval.find_best_matches_wer(user_input, unique_razaosocial, top_k),\n",
    "            'nome_fantasia': TextRetrieval.find_best_matches_wer(user_input, unique_nome_fantasia, top_k),\n",
    "            'uf': uf\n",
    "        }\n",
    "        \n",
    "        all_matches['levenshtein'] = {\n",
    "            'razaosocial': TextRetrieval.find_best_matches_levenshtein(user_input, unique_razaosocial, top_k),\n",
    "            'nome_fantasia': TextRetrieval.find_best_matches_levenshtein(user_input, unique_nome_fantasia, top_k),\n",
    "            'uf': uf\n",
    "        }\n",
    "        \n",
    "        if include_jaccard:\n",
    "            all_matches['jaccard_chars'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_jaccard_chars(user_input, unique_razaosocial, top_k),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_jaccard_chars(user_input, unique_nome_fantasia, top_k),\n",
    "                'uf': uf\n",
    "            }\n",
    "            \n",
    "            all_matches['jaccard_words'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_jaccard_words(user_input, unique_razaosocial, top_k),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_jaccard_words(user_input, unique_nome_fantasia, top_k),\n",
    "                'uf': uf\n",
    "            }\n",
    "\n",
    "            df_uf = df_sample[df_sample['uf'] == uf]\n",
    "            best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores = TextRetrieval.find_best_matches_ngram_jaccard(user_input, df_uf, top_k)\n",
    "            all_matches['jaccard_ngram'] = {\n",
    "                'razaosocial': (best_razao_matches, best_razao_scores),\n",
    "                'nome_fantasia': (best_nome_fantasia_matches, best_nome_fantasia_scores),\n",
    "                'uf': uf\n",
    "            }\n",
    "        \n",
    "        # Find matches using combined metric\n",
    "        if include_combined:\n",
    "            all_matches['combined'] = {\n",
    "                'razaosocial': TextRetrieval.find_best_matches_combined(user_input, unique_razaosocial, top_k, combined_weights),\n",
    "                'nome_fantasia': TextRetrieval.find_best_matches_combined(user_input, unique_nome_fantasia, top_k, combined_weights),\n",
    "                'uf': uf\n",
    "            }\n",
    "        # Process results for each metric\n",
    "        for metric in metrics_to_evaluate:\n",
    "            for field in ['razaosocial', 'nome_fantasia']:\n",
    "                matches, scores = all_matches[metric][field]\n",
    "                true_value = true_razaosocial if field == 'razaosocial' else true_nome_fantasia\n",
    "                \n",
    "                # Top-k accuracy\n",
    "                top_k_pred = true_value in matches\n",
    "                metric_col = f'{field}_top_{top_k}_{metric}_pred'\n",
    "                result_row[metric_col] = top_k_pred\n",
    "\n",
    "                # Top-1 accuracy\n",
    "                top_1_pred = matches[0] == true_value if matches else False\n",
    "                metric_col = f'{field}_top_1_{metric}_pred'\n",
    "                result_row[metric_col] = top_1_pred\n",
    "                \n",
    "                # Ranking\n",
    "                rank = matches.index(true_value) + 1 if true_value in matches else 0\n",
    "                result_row[f'{field}_rank_{metric}'] = rank\n",
    "                \n",
    "                # Store the actual scores for analysis\n",
    "                if matches:\n",
    "                    result_row[f'best_{metric}_score_{field}'] = scores[0] if scores else None\n",
    "        \n",
    "        results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results), matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all 255471 records for evaluation\n",
      "Evaluating retrieval accuracy with top-5 results using metrics: ['cer', 'wer', 'levenshtein', 'jaccard_chars', 'jaccard_words', 'jaccard_ngram', 'combined']\n",
      "Combined metric weights: {'cer': 0.2, 'wer': 0.2, 'levenshtein': 0.2, 'jaccard_chars': 0.2, 'jaccard_words': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 9/255471 [00:15<118:51:30,  1.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m top_k = \u001b[32m5\u001b[39m\n\u001b[32m      7\u001b[39m combined_weights = {\n\u001b[32m      8\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mcer\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.1\u001b[39m,\n\u001b[32m      9\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mjaccard_words\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.4\u001b[39m,\n\u001b[32m     13\u001b[39m             }\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m retrieved, matches = \u001b[43mevaluate_retrieval_accuracy_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43munique_razaosocial_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_razaosocial_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43munique_nome_fantasia_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_nome_fantasia_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m                                               \u001b[38;5;66;03m#combined_weights=combined_weights)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mevaluate_retrieval_accuracy_top_k\u001b[39m\u001b[34m(df, unique_razaosocial_dict, unique_nome_fantasia_dict, top_k, sample_size, include_jaccard, include_combined, combined_weights)\u001b[39m\n\u001b[32m     96\u001b[39m     all_matches[\u001b[33m'\u001b[39m\u001b[33mjaccard_words\u001b[39m\u001b[33m'\u001b[39m] = {\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mrazaosocial\u001b[39m\u001b[33m'\u001b[39m: TextRetrieval.find_best_matches_jaccard_words(user_input, unique_razaosocial, top_k),\n\u001b[32m     98\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnome_fantasia\u001b[39m\u001b[33m'\u001b[39m: TextRetrieval.find_best_matches_jaccard_words(user_input, unique_nome_fantasia, top_k),\n\u001b[32m     99\u001b[39m         \u001b[33m'\u001b[39m\u001b[33muf\u001b[39m\u001b[33m'\u001b[39m: uf\n\u001b[32m    100\u001b[39m     }\n\u001b[32m    102\u001b[39m     df_uf = df_sample[df_sample[\u001b[33m'\u001b[39m\u001b[33muf\u001b[39m\u001b[33m'\u001b[39m] == uf]\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores = \u001b[43mTextRetrieval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_best_matches_ngram_jaccard\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_uf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     all_matches[\u001b[33m'\u001b[39m\u001b[33mjaccard_ngram\u001b[39m\u001b[33m'\u001b[39m] = {\n\u001b[32m    105\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mrazaosocial\u001b[39m\u001b[33m'\u001b[39m: (best_razao_matches, best_razao_scores),\n\u001b[32m    106\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnome_fantasia\u001b[39m\u001b[33m'\u001b[39m: (best_nome_fantasia_matches, best_nome_fantasia_scores),\n\u001b[32m    107\u001b[39m         \u001b[33m'\u001b[39m\u001b[33muf\u001b[39m\u001b[33m'\u001b[39m: uf\n\u001b[32m    108\u001b[39m     }\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Find matches using combined metric\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mTextRetrieval.find_best_matches_ngram_jaccard\u001b[39m\u001b[34m(user_input, df, top_k)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    196\u001b[39m razaosocial_cand_sim = TextMetrics.multi_ngram_jaccard_similarity(razaosocial_cand, user_input, ngram_sizes=[\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m], weights=[\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m nome_fantasia_cand_sim = \u001b[43mTextMetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_ngram_jaccard_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnome_fantasia_cand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m max_similarity = \u001b[38;5;28mmax\u001b[39m(razaosocial_cand_sim, nome_fantasia_cand_sim)\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m razaosocial_cand_sim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m nome_fantasia_cand_sim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mTextMetrics.multi_ngram_jaccard_similarity\u001b[39m\u001b[34m(reference, hypothesis, ngram_sizes, weights)\u001b[39m\n\u001b[32m    184\u001b[39m total_weight = \u001b[38;5;28msum\u001b[39m(weights)\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ngram_sizes, weights):\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     score = \u001b[43mTextMetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ngram_jaccard_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     total_score += score * weight\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_score / total_weight\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mTextMetrics._ngram_jaccard_similarity\u001b[39m\u001b[34m(reference, hypothesis, n)\u001b[39m\n\u001b[32m    160\u001b[39m     padded = \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m * (n-\u001b[32m1\u001b[39m) + text.lower() + \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m * (n-\u001b[32m1\u001b[39m)\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(padded[i:i+n] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(padded) - n + \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m ngrams1 = \u001b[43mget_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m ngrams2 = get_ngrams(hypothesis, n)\n\u001b[32m    166\u001b[39m intersection = \u001b[38;5;28mlen\u001b[39m(ngrams1 & ngrams2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mTextMetrics._ngram_jaccard_similarity.<locals>.get_ngrams\u001b[39m\u001b[34m(text, n)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Add padding to capture beginning/end patterns\u001b[39;00m\n\u001b[32m    160\u001b[39m padded = \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m * (n-\u001b[32m1\u001b[39m) + text.lower() + \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m * (n-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(padded[i:i+n] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(padded) - n + \u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Add padding to capture beginning/end patterns\u001b[39;00m\n\u001b[32m    160\u001b[39m padded = \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m * (n-\u001b[32m1\u001b[39m) + text.lower() + \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m * (n-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(padded[i:i+n] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(padded) - n + \u001b[32m1\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Carregando todos as razoes sociais e nomes fantasia únicos\n",
    "unique_razaosocial_dict = {uf: df[df['uf'] == uf]['razaosocial'].dropna().unique().tolist() for uf in df['uf'].unique()}\n",
    "# Carregando todos os nomes fantasia únicos\n",
    "unique_nome_fantasia_dict = {uf: df[df['uf'] == uf]['nome_fantasia'].dropna().unique().tolist() for uf in df['uf'].unique()}\n",
    "sample_size = None  # Define a sample size for evaluation, or set to None to use all data\n",
    "top_k = 5\n",
    "combined_weights = {\n",
    "                'cer': 0.1,\n",
    "                'wer': 0.2,\n",
    "                'levenshtein': 0.2,\n",
    "                'jaccard_chars': 0.1,\n",
    "                'jaccard_words': 0.4,\n",
    "            }\n",
    "retrieved, matches = evaluate_retrieval_accuracy_top_k(df=df,\n",
    "                                              top_k=top_k,\n",
    "                                              unique_razaosocial_dict=unique_razaosocial_dict,\n",
    "                                              unique_nome_fantasia_dict=unique_nome_fantasia_dict,\n",
    "                                              sample_size=sample_size,)\n",
    "                                              #combined_weights=combined_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found metrics: ['cer', 'combined', 'jaccard_chars', 'jaccard_ngram', 'jaccard_words', 'levenshtein', 'wer']\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY - TOP-5 RETRIEVAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📊 RANKING BY OVERALL TOP-1 ACCURACY:\n",
      "--------------------------------------------------\n",
      " 4. jaccard_ngram   | Top-1: 0.891 | Top-5: 0.977 | Avg Rank: 1.15\n",
      " 5. jaccard_words   | Top-1: 0.588 | Top-5: 0.700 | Avg Rank: 1.29\n",
      " 7. combined        | Top-1: 0.538 | Top-5: 0.693 | Avg Rank: 1.45\n",
      " 6. levenshtein     | Top-1: 0.484 | Top-5: 0.653 | Avg Rank: 1.52\n",
      " 1. wer             | Top-1: 0.467 | Top-5: 0.578 | Avg Rank: 1.37\n",
      " 2. cer             | Top-1: 0.444 | Top-5: 0.590 | Avg Rank: 1.51\n",
      " 3. jaccard_chars   | Top-1: 0.381 | Top-5: 0.553 | Avg Rank: 1.65\n",
      "\n",
      "🎯 DETAILED BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "JACCARD_NGRAM:\n",
      "  • Razão Social    - Top-1: 0.935, Top-5: 0.989, Retrieval Rate: 0.989\n",
      "  • Nome Fantasia   - Top-1: 0.848, Top-5: 0.964, \n",
      "\n",
      "JACCARD_WORDS:\n",
      "  • Razão Social    - Top-1: 0.526, Top-5: 0.641, Retrieval Rate: 0.641\n",
      "  • Nome Fantasia   - Top-1: 0.649, Top-5: 0.758, \n",
      "\n",
      "COMBINED:\n",
      "  • Razão Social    - Top-1: 0.449, Top-5: 0.607, Retrieval Rate: 0.607\n",
      "  • Nome Fantasia   - Top-1: 0.627, Top-5: 0.778, \n",
      "\n",
      "LEVENSHTEIN:\n",
      "  • Razão Social    - Top-1: 0.359, Top-5: 0.509, Retrieval Rate: 0.509\n",
      "  • Nome Fantasia   - Top-1: 0.609, Top-5: 0.797, \n",
      "\n",
      "WER:\n",
      "  • Razão Social    - Top-1: 0.423, Top-5: 0.541, Retrieval Rate: 0.541\n",
      "  • Nome Fantasia   - Top-1: 0.511, Top-5: 0.615, \n"
     ]
    }
   ],
   "source": [
    "performance_analysis = analyze_combined_metric_performance(retrieved, top_k=top_k)\n",
    "print_performance_summary(performance_analysis, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieved.to_csv(\"10000_rand_42_retrieved.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(evaluation_df):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1 score from evaluation results\n",
    "    \n",
    "    Parameters:\n",
    "    evaluation_df (pd.DataFrame): Output from evaluate_retrieval_accuracy function\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing all metrics for different scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics_results = {}\n",
    "    \n",
    "    # Define the scenarios to evaluate\n",
    "    scenarios = [\n",
    "        ('razaosocial_top_k_cer', 'razaosocial_top_k_cer_pred'),\n",
    "        ('nome_fantasia_top_k_cer', 'nome_fantasia_top_k_cer_pred'),\n",
    "        ('razaosocial_top_k_wer', 'razaosocial_top_k_wer_pred'),\n",
    "        ('nome_fantasia_top_k_wer', 'nome_fantasia_top_k_wer_pred'),\n",
    "        ('razaosocial_top_k_levenshtein', 'razaosocial_top_k_lev_dist_pred'),\n",
    "        ('nome_fantasia_top_k_levenshtein', 'nome_fantasia_top_k_lev_dist_pred'),\n",
    "\n",
    "        ('razaosocial_top_1_cer', 'razaosocial_top_1_cer_pred'),\n",
    "        ('nome_fantasia_top_1_cer', 'nome_fantasia_top_1_cer_pred'),\n",
    "        ('razaosocial_top_1_wer', 'razaosocial_top_1_wer_pred'),\n",
    "        ('nome_fantasia_top_1_wer', 'nome_fantasia_top_1_wer_pred'),\n",
    "        ('razaosocial_top_1_levenshtein', 'razaosocial_top_1_lev_dist_pred'),\n",
    "        ('nome_fantasia_top_1_levenshtein', 'nome_fantasia_top_1_lev_dist_pred')\n",
    "    ]\n",
    "    \n",
    "    print(\"Classification Metrics Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for scenario_name, correct_column in scenarios:\n",
    "        print(f\"\\n{scenario_name.upper()} Results:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Get true labels (1 for correct, 0 for incorrect)\n",
    "        y_true = [1] * len(evaluation_df)\n",
    "        # Get predicted labels (1 for correct prediction, 0 for incorrect)\n",
    "        y_pred = evaluation_df[correct_column].astype(int).tolist()\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "        \n",
    "        metrics_results[scenario_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'correct_predictions': sum(y_pred),\n",
    "            'total_predictions': len(y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy:  {accuracy:.4f} ({sum(y_pred)}/{len(y_pred)})\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate all metrics\n",
    "comprehensive_metrics = calculate_classification_metrics(retrieved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
