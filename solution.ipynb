{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import Levenshtein\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "JACCARD_MODE = \"ngram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregando os Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "user_input       0\n",
      "uf               0\n",
      "razaosocial      0\n",
      "nome_fantasia    0\n",
      "dtype: int64\n",
      "\n",
      "Total rows with any missing values: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "confs = yaml.safe_load(open(\"confs.yaml\"))\n",
    "predictors = confs[\"predictors\"] ### Importante! O cientista poderÃ¡ usar apenas estas features para criar/aperfeiÃ§oar o modelo\n",
    "text_target = confs[\"text_target\"]\n",
    "cols_to_keep = predictors + text_target\n",
    "df = pd.read_parquet(\"dados/train.parquet\")[cols_to_keep]\n",
    "df.to_csv(\"data.csv\")\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal rows with any missing values: {df.isnull().any(axis=1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpando os Dados\n",
    "\n",
    "Iremos remover palavras como \"S.A.\", \"LTDA\", \"LTDA.\", \"S/A\", \"S.A\", \"Ltda\", \"Ltda.\", \"S/A.\", \"S.A.\", \"S.A\", \"Ltda\" e \"Ltda\" dos nomes reais das empresas a serem previstos, usando a seguinte suposicao:\n",
    "\n",
    "- Suposicao 1: usuÃ¡rios tem o hÃ¡bito de pesquisar por nomes de empresas sem essas palavras, entÃ£o elas nÃ£o devem ser consideradas na previsÃ£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_text_cleaning(text, \n",
    "                               remove_accents=True,\n",
    "                               remove_stop_words=True, \n",
    "                               remove_company_suffixes=True,\n",
    "                               custom_stop_words=None,\n",
    "                               to_lowercase=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "    remove_accents (bool): Remove accents and normalize characters\n",
    "    remove_stop_words (bool): Remove Portuguese stop words\n",
    "    remove_company_suffixes (bool): Remove common company suffixes\n",
    "    custom_stop_words (set): Additional stop words to remove\n",
    "    to_lowercase (bool): Convert to lowercase\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. Remove accents and normalize characters\n",
    "    if remove_accents:\n",
    "        # Normalize unicode\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        \n",
    "        # Handle specific cases\n",
    "        text = text.replace('Ã§', 'c').replace('Ã‡', 'C')\n",
    "    \n",
    "    # 2. Convert to lowercase\n",
    "    if to_lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # 3. Remove company suffixes\n",
    "    if remove_company_suffixes:\n",
    "        patterns_to_remove = [\n",
    "        r'\\bS\\.?A\\.?\\b',           # S.A, SA, S.A., SA.\n",
    "        r'\\bS/A\\.?\\b',             # S/A, S/A.\n",
    "        r'\\bLTDA\\.?\\b',            # LTDA, LTDA.\n",
    "        r'\\bLIMITADA\\b',           # LIMITADA\n",
    "        r'\\bCIA\\.?\\b',             # CIA, CIA.\n",
    "        r'\\bCOMPANHIA\\b',          # COMPANHIA\n",
    "        r'\\bEMPRESA\\b',            # EMPRESA\n",
    "        r'\\bCOMERCIO\\b',           # COMERCIO\n",
    "        r'\\bSERVICOS?\\b',          # SERVICO, SERVICOS\n",
    "        r'\\bME\\b',                 # ME (Microempresa)\n",
    "        r'\\bEPP\\b',                # EPP (Empresa de Pequeno Porte)\n",
    "        r'\\bEIRELI\\b',             # EIRELI\n",
    "        r'\\bSOCIEDADE\\b',          # SOCIEDADE\n",
    "        r'ADMINISTRADORA\\b',       # ADMINISTRADORA\n",
    "        r'GERAL\\b',                # GERAL\n",
    "    ]\n",
    "        \n",
    "        for pattern in patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    if remove_stop_words:\n",
    "        portuguese_stop_words = {\n",
    "            'a', 'ao', 'aos', 'as', 'da', 'das', 'de', 'do', 'dos', 'e', 'em', 'na', \n",
    "            'nas', 'no', 'nos', 'o', 'os', 'para', 'por', 'com', 'um', 'uma', 'uns', \n",
    "            'umas', 'se', 'que', 'ou', 'mas', 'como', 'mais', 'muito', 'sua', 'seu',\n",
    "            'seus', 'suas', 'este', 'esta', 'estes', 'estas', 'esse', 'essa', 'esses',\n",
    "            'essas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'isso', 'aquilo'\n",
    "        }\n",
    "        \n",
    "        if custom_stop_words:\n",
    "            portuguese_stop_words.update(custom_stop_words)\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.lower() not in portuguese_stop_words]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    # 5. Clean up extra whitespace and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Multiple spaces to single space\n",
    "    text = text.strip()                   # Remove leading/trailing spaces\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "df['razaosocial'] = df['razaosocial'].apply(comprehensive_text_cleaning)\n",
    "df['nome_fantasia'] = df['nome_fantasia'].apply(comprehensive_text_cleaning)\n",
    "df['user_input'] = df['user_input'].apply(comprehensive_text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AnÃ¡lise de MÃ©tricas de Character Error Rate (CER), Word Error Rate (WER) e Distancia de Levenshtein\n",
    "\n",
    "- **Word Error Rate (WER)**: fÃ³rmula para calcular a taxa de erro a nÃ­vel de palavras: \n",
    "  $$WER = \\frac{S + D + I}{N}$$\n",
    "  onde:\n",
    "  - $S$ Ã© o nÃºmero de substituiÃ§Ãµes. Por exemplo, se o usuÃ¡rio digitou \"Empresa X\" e a referÃªncia Ã© \"Empresa Y\", entÃ£o hÃ¡ uma substituiÃ§Ã£o.\n",
    "  - $D$ Ã© o nÃºmero de deleÃ§Ãµes. Por exemplo, se o usuÃ¡rio digitou \"Empresa\" e a referÃªncia Ã© \"Empresa X\", entÃ£o hÃ¡ uma deleÃ§Ã£o.\n",
    "  - $I$ Ã© o nÃºmero de inserÃ§Ãµes. Por exemplo, se o usuÃ¡rio digitou \"Empresa X Y\" e a referÃªncia Ã© \"Empresa X\", entÃ£o hÃ¡ uma inserÃ§Ã£o.\n",
    "  - $N$ Ã© o nÃºmero total de palavras na referÃªncia. Por exemplo, se a referÃªncia Ã© \"Empresa X\", entÃ£o $N$ Ã© 2.\n",
    "\n",
    "- **Character Error Rate (CER)**: fÃ³rmula para calcular a taxa de erro a nÃ­vel de caracteres:\n",
    "  $$CER = \\frac{S + D + I}{N}$$\n",
    "  onde:\n",
    "  - $S$ Ã© o nÃºmero de substituiÃ§Ãµes. Por exemplo, se o usuÃ¡rio digitou \"EmpresaXY\" e a referÃªncia Ã© \"EmpresaXZ\", entÃ£o hÃ¡ uma substituiÃ§Ã£o.\n",
    "  - $D$ Ã© o nÃºmero de deleÃ§Ãµes. Por exemplo, se o usuÃ¡rio digitou \"Empresa\" e a referÃªncia Ã© \"EmpresaX\", entÃ£o hÃ¡ uma deleÃ§Ã£o.\n",
    "  - $I$ Ã© o nÃºmero de inserÃ§Ãµes. Por exemplo, se o usuÃ¡rio digitou \"Empresa XY\" e a referÃªncia Ã© \"Empresa X\", entÃ£o hÃ¡ uma inserÃ§Ã£o.\n",
    "  - $N$ Ã© o nÃºmero total de caracteres na referÃªncia. Por exemplo, se a referÃªncia Ã© \"Empresa X\", entÃ£o $N$ Ã© 9 (contando espaÃ§os).\n",
    "\n",
    "- **DistÃ¢ncia de Levenshtein**: Ã© uma mÃ©trica que mede a diferenÃ§a entre duas sequÃªncias. Ã‰ definida como o nÃºmero mÃ­nimo de operaÃ§Ãµes de ediÃ§Ã£o (inserÃ§Ãµes, deleÃ§Ãµes ou substituiÃ§Ãµes) necessÃ¡rias para transformar uma sequÃªncia em outra.\n",
    "\n",
    "- **Similaridade de Jaccard**: Ã© uma mÃ©trica que mede a similaridade entre dois conjuntos. Ã‰ definida como o tamanho da interseÃ§Ã£o dividido pelo tamanho da uniÃ£o dos conjuntos.\n",
    "  $$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "\n",
    "Essas mÃ©tricas serao Ãºteis para avaliar o quÃ£o diferente os inputs de usuÃ¡rio (`user_input`) sÃ£o dos outputs esperados `razaosocial` e `nome_fantasia` e tambÃ©m dos outputs nao esperados, i.e., de todas as empresas que nao correspondem ao input do usuÃ¡rio. \n",
    "\n",
    "Caso o `CER` e/ou  `WER` entre o `user_input` e dos outputs nao esperados seja significativamente maior do que o `CER` e/ou `WER` entre o `user_input` e dos outputs esperados, podemos concluir que o input do usuÃ¡rio Ã© mais prÃ³ximo dos outputs esperados do que dos outputs nÃ£o esperados e utilizar a minimizaÃ§Ã£o de `CER` e `WER` como critÃ©rio para selecionar a empresa correta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Calculando o CER, WER e a DistÃ¢ncia de Levenshtein\n",
    "\n",
    "Vamos usar a implementaÃ§Ã£o do pacote jÃ¡ importado `jiwer` para calcular CER e WER. Para a DistÃ¢ncia de Levenshtein, vamos usar a funÃ§Ã£o `distance` do pacote `Levenshtein`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Set, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from functools import wraps\n",
    "\n",
    "def validate_inputs(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator to handle input validation and error handling for text comparison metrics.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(reference: Union[str, float], hypothesis: Union[str, float], *args, **kwargs) -> float:\n",
    "        try:\n",
    "            # Handle NaN values\n",
    "            if pd.isna(reference) or pd.isna(hypothesis):\n",
    "                return np.nan\n",
    "            \n",
    "            # Convert to string and clean\n",
    "            reference = str(reference).strip()\n",
    "            hypothesis = str(hypothesis).strip()\n",
    "            \n",
    "            # Handle empty strings\n",
    "            if len(reference) == 0 or len(hypothesis) == 0:\n",
    "                return np.nan\n",
    "                \n",
    "            return func(reference, hypothesis, *args, **kwargs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating {func.__name__}: {e}\")\n",
    "            return np.nan\n",
    "            \n",
    "    return wrapper\n",
    "\n",
    "class TextMetrics:\n",
    "    \"\"\"\n",
    "    A class containing various text comparison metrics with input validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_cer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Character Error Rate.\"\"\"\n",
    "        return cer(reference, hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_wer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Word Error Rate.\"\"\"\n",
    "        return wer(reference, hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_normalized_levenshtein(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate normalized Levenshtein distance (0-1).\n",
    "        Returns:\n",
    "            float: Normalized Levenshtein distance between 0 and 1\n",
    "        \"\"\"\n",
    "        max_len = max(len(reference), len(hypothesis))\n",
    "        if max_len == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        distance = Levenshtein.distance(reference, hypothesis)\n",
    "        return distance / max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_character_set(text: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Convert text to a set of characters.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Set[str]: Set of characters from the input text\n",
    "        \"\"\"\n",
    "        return set(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_word_set(text: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Convert text to a set of words.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Set[str]: Set of words from the input text\n",
    "        \"\"\"\n",
    "        return set(text.lower().split())\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity between two sets.\n",
    "        \n",
    "        Args:\n",
    "            set1 (Set[str]): First set\n",
    "            set2 (Set[str]): Second set\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        if not set1 and not set2:  # Both sets are empty\n",
    "            return 1.0\n",
    "        if not set1 or not set2:   # One set is empty\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union\n",
    "\n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_jaccard_similarity_chars(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity based on character sets.\n",
    "        \n",
    "        Args:\n",
    "            reference (str): Reference text\n",
    "            hypothesis (str): Hypothesis text\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        ref_chars = TextMetrics._get_character_set(reference)\n",
    "        hyp_chars = TextMetrics._get_character_set(hypothesis)\n",
    "        return TextMetrics._calculate_jaccard_similarity(ref_chars, hyp_chars)\n",
    "\n",
    "    @staticmethod\n",
    "    @validate_inputs\n",
    "    def calculate_jaccard_similarity_words(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity based on word sets.\n",
    "        \n",
    "        Args:\n",
    "            reference (str): Reference text\n",
    "            hypothesis (str): Hypothesis text\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        ref_words = TextMetrics._get_word_set(reference)\n",
    "        hyp_words = TextMetrics._get_word_set(hypothesis)\n",
    "        return TextMetrics._calculate_jaccard_similarity(ref_words, hyp_words)\n",
    "\n",
    "    @staticmethod \n",
    "    def _ngram_jaccard_similarity(reference: str, hypothesis: str, n=2):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity using character n-grams.\n",
    "        This handles inversions and some misspellings well.\n",
    "        \n",
    "        Args:\n",
    "            str1, str2: Input strings\n",
    "            n: N-gram size (2=bigrams, 3=trigrams, etc.)\n",
    "        \"\"\"\n",
    "        def get_ngrams(text, n):\n",
    "            \"\"\"Generate n-grams from text with padding.\"\"\"\n",
    "            # Add padding to capture beginning/end patterns\n",
    "            padded = '#' * (n-1) + text.lower() + '#' * (n-1)\n",
    "            return set(padded[i:i+n] for i in range(len(padded) - n + 1))\n",
    "        \n",
    "        ngrams1 = get_ngrams(reference, n)\n",
    "        ngrams2 = get_ngrams(hypothesis, n)\n",
    "        \n",
    "        intersection = len(ngrams1 & ngrams2)\n",
    "        union = len(ngrams1 | ngrams2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 1.0 if len(reference) == len(hypothesis) == 0 else 0.0\n",
    " \n",
    "    @staticmethod\n",
    "    def multi_ngram_jaccard_similarity(reference: str, hypothesis: str, ngram_sizes=[2, 3], weights=None):\n",
    "        \"\"\"\n",
    "        Combine multiple n-gram sizes for better robustness.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0] * len(ngram_sizes)\n",
    "        \n",
    "        if len(weights) != len(ngram_sizes):\n",
    "            raise ValueError(\"Number of weights must match number of n-gram sizes\")\n",
    "        \n",
    "        total_score = 0\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        for size, weight in zip(ngram_sizes, weights):\n",
    "            score = TextMetrics._ngram_jaccard_similarity(reference, hypothesis, size)\n",
    "            total_score += score * weight\n",
    "        \n",
    "        return total_score / total_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CER, WER e Distancia de Levenshtein: `user_input` vs Ground Truth (`razaosocial` e `nome_fantasia`)\n",
    "\n",
    "Vamos calcular o CER, o WER  e a distancia de Levenshtein entre o `user_input` e as colunas `razaosocial` e `nome_fantasia` do DataFrame e adicionar essas mÃ©tricas como novas colunas no DataFrame. \n",
    "\n",
    "\n",
    "- `cer_razaosocial`: CER entre `user_input` e `razaosocial`\n",
    "- `wer_razaosocial`: WER entre `user_input` e `razaosocial`\n",
    "- `lev_dist__razaosocial`: DistÃ¢ncia de Levenshtein entre `user_input` e `razaosocial`\n",
    "- `cer_nome_fantasia`: CER entre `user_input` e `nome_fantasia`\n",
    "- `wer_nome_fantasia`: WER entre `user_input` e `nome_fantasia`\n",
    "- `lev_dist__nome_fantasia`: DistÃ¢ncia de Levenshtein entre `user_input` e `nome_fantasia`\n",
    "- `jac_sim_razaosocial`: Similaridade de Jaccard entre `user_input` e `razaosocial`\n",
    "- `jac_sim_nome_fantasia`: Similaridade de Jaccard entre `user_input` e `nome_fantasia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def calculate_metrics(row: pd.Series,\n",
    "                      reference_col: str,\n",
    "                      hypothesis_col: str,\n",
    "                      metrics: TextMetrics,\n",
    "                      jacc_mode: str) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate CER, WER, Levenshtein distance, and Jaccard similarity for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame\n",
    "        reference_col (str): Name of the column containing the reference text\n",
    "        hypothesis_col (str): Name of the column containing the hypothesis text\n",
    "        metrics (TextMetrics): An instance of the TextMetrics class\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: CER, WER, Levenshtein distance, and Jaccard similarity\n",
    "    \"\"\"\n",
    "    reference = row[reference_col]\n",
    "    hypothesis = row[hypothesis_col]\n",
    "    \n",
    "    cer = metrics.calculate_cer(reference, hypothesis)\n",
    "    wer = metrics.calculate_wer(reference, hypothesis)\n",
    "    levenshtein = metrics.calculate_normalized_levenshtein(reference, hypothesis)\n",
    "    if jacc_mode == \"char\":\n",
    "        jaccard = metrics.calculate_jaccard_similarity_chars(reference, hypothesis)\n",
    "    elif jacc_mode == \"word\":\n",
    "        jaccard = metrics.calculate_jaccard_similarity_words(reference, hypothesis)\n",
    "    elif jacc_mode == \"ngram\":\n",
    "        jaccard = metrics.multi_ngram_jaccard_similarity(reference, hypothesis, ngram_sizes=[2, 3], weights=[0.5, 0.5])\n",
    "    \n",
    "    return cer, wer, levenshtein, jaccard\n",
    "\n",
    "def apply_metrics(df: pd.DataFrame,\n",
    "                  reference_cols: List[str],\n",
    "                  hypothesis_col: str,\n",
    "                  metrics: TextMetrics,\n",
    "                  jacc_mode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply text metrics to multiple reference columns against a single hypothesis column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        reference_cols (List[str]): List of column names to use as reference texts\n",
    "        hypothesis_col (str): Column name to use as hypothesis text\n",
    "        metrics (TextMetrics): An instance of the TextMetrics class\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added metric columns\n",
    "    \"\"\"\n",
    "    for ref_col in reference_cols:\n",
    "        print(f\"Calculating metrics for {hypothesis_col} vs {ref_col}...\")\n",
    "        \n",
    "        cer_col = f'cer_{ref_col}'\n",
    "        wer_col = f'wer_{ref_col}'\n",
    "        lev_dist_col = f'lev_dist_{ref_col}'\n",
    "        jaccard_col = f'jacc_sim_{jacc_mode}_{ref_col}'\n",
    "        \n",
    "        df[[cer_col, wer_col, lev_dist_col, jaccard_col]] = df.apply(\n",
    "            lambda row: calculate_metrics(row, ref_col, hypothesis_col, metrics, jacc_mode),\n",
    "            axis=1,\n",
    "            result_type='expand'\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_error_rates(df: pd.DataFrame,\n",
    "                          jacc_mode: str = \"word\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate CER, WER, Levenshtein distance, and Jaccard \n",
    "    similarity for user_input vs razaosocial and nome_fantasia.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added metric columns\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating CER, WER, Levenshtein distance, and Jaccard similarity...\")\n",
    "    \n",
    "    metrics = TextMetrics()\n",
    "    reference_columns = ['razaosocial', 'nome_fantasia']\n",
    "    hypothesis_column = 'user_input'\n",
    "    \n",
    "    df = apply_metrics(df, reference_columns, hypothesis_column, metrics, jacc_mode=jacc_mode)\n",
    "    \n",
    "    print(\"Calculations completed!\")\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df = calculate_error_rates(df, jacc_mode=JACCARD_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metrics_statistics(df: pd.DataFrame):\n",
    "    \"\"\"Calculate and display statistical summaries\"\"\"\n",
    "    # Calculate average metrics\n",
    "    avg_cer_razaosocial = df['cer_razaosocial'].mean()\n",
    "    avg_wer_razaosocial = df['wer_razaosocial'].mean()\n",
    "    avg_lev_dist_razaosocial = df['lev_dist_razaosocial'].mean()\n",
    "    avg_cer_nome_fantasia = df['cer_nome_fantasia'].mean()\n",
    "    avg_wer_nome_fantasia = df['wer_nome_fantasia'].mean()\n",
    "    avg_lev_dist_nome_fantasia = df['lev_dist_nome_fantasia'].mean()\n",
    "    avg_jac_sim_razaosocial = df[f'jacc_sim_{JACCARD_MODE}_razaosocial'].mean()\n",
    "    avg_jac_sim_nome_fantasia = df[f'jacc_sim_{JACCARD_MODE}_nome_fantasia'].mean()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AVERAGE ERROR RATES\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Average CER (user_input vs razaosocial): {avg_cer_razaosocial:.4f}\")\n",
    "    print(f\"Average WER (user_input vs razaosocial): {avg_wer_razaosocial:.4f}\")\n",
    "    print(f\"Average Levenshtein (user_input vs razaosocial): {avg_lev_dist_razaosocial:.4f}\")\n",
    "    print(f\"Average CER (user_input vs nome_fantasia): {avg_cer_nome_fantasia:.4f}\")\n",
    "    print(f\"Average WER (user_input vs nome_fantasia): {avg_wer_nome_fantasia:.4f}\")\n",
    "    print(f\"Average Levenshtein (user_input vs nome_fantasia): {avg_lev_dist_nome_fantasia:.4f}\")\n",
    "    print(f\"Average Jaccard Similarity ({JACCARD_MODE} level) (user_input vs razaosocial): {avg_jac_sim_razaosocial:.4f}\")\n",
    "    print(f\"Average Jaccard Similarity ({JACCARD_MODE} level) (user_input vs nome_fantasia): {avg_jac_sim_nome_fantasia:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DETAILED STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nCER Statistics:\")\n",
    "    print(df[['cer_razaosocial', 'cer_nome_fantasia']].describe())\n",
    "    \n",
    "    print(\"\\nWER Statistics:\")\n",
    "    print(df[['wer_razaosocial', 'wer_nome_fantasia']].describe())\n",
    "\n",
    "    print(\"\\nLevenshtein Statistics:\")\n",
    "    print(df[['lev_dist_razaosocial', 'lev_dist_nome_fantasia']].describe())\n",
    "\n",
    "    print(\"\\nJaccard Similarity Statistics:\")\n",
    "    print(df[[f'jacc_sim_{JACCARD_MODE}_razaosocial', f'jacc_sim_{JACCARD_MODE}_nome_fantasia']].describe())\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'avg_cer_razaosocial': avg_cer_razaosocial,\n",
    "        'avg_wer_razaosocial': avg_wer_razaosocial,\n",
    "        'avg_cer_nome_fantasia': avg_cer_nome_fantasia,\n",
    "        'avg_wer_nome_fantasia': avg_wer_nome_fantasia,\n",
    "        'avg_lev_dist_razaosocial': avg_lev_dist_razaosocial,\n",
    "        'avg_lev_dist_nome_fantasia': avg_lev_dist_nome_fantasia,\n",
    "        'avg_jac_sim_razaosocial': avg_jac_sim_razaosocial,\n",
    "        'avg_jac_sim_nome_fantasia': avg_jac_sim_nome_fantasia\n",
    "    }\n",
    "\n",
    "metrics_stats = analyze_metrics_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 CER, WER e Distancia de Levshenstein: `user_input` vs Outras Empresas (Outputs NÃ£o Esperados)\n",
    "\n",
    "Vamos calcular CER, WER e a Distancia de Levshenstein entre o `user_input` e as colunas `razaosocial` e `nome_fantasia` de todas as outras empresas (outputs nÃ£o esperados) e adicionar essas mÃ©tricas como novas colunas no DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "\n",
    "class SimilarityMetric(Enum):\n",
    "    \"\"\"Enum for available similarity metrics.\"\"\"\n",
    "    CER = \"cer\"\n",
    "    WER = \"wer\"\n",
    "    LEVENSHTEIN = \"levenshtein\"\n",
    "    JACCARD_CHARS = \"jaccard_chars\"\n",
    "    JACCARD_WORDS = \"jaccard_words\"\n",
    "    NGRAM_JACCARD = \"ngram_jaccard\"\n",
    "\n",
    "class TextRetrieval:\n",
    "    \"\"\"\n",
    "    A modular class for text retrieval using various similarity metrics.\n",
    "    All methods return matches for both 'razaosocial' and 'nome_fantasia' columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define metric configurations: (metric_function, reverse_sort)\n",
    "    # reverse_sort=False for distance metrics (lower is better)\n",
    "    # reverse_sort=True for similarity metrics (higher is better)\n",
    "    _METRIC_CONFIG = {\n",
    "        SimilarityMetric.CER: (TextMetrics.calculate_cer, False),\n",
    "        SimilarityMetric.WER: (TextMetrics.calculate_wer, False),\n",
    "        SimilarityMetric.LEVENSHTEIN: (TextMetrics.calculate_normalized_levenshtein, False),\n",
    "        SimilarityMetric.JACCARD_CHARS: (TextMetrics.calculate_jaccard_similarity_chars, True),\n",
    "        SimilarityMetric.JACCARD_WORDS: (TextMetrics.calculate_jaccard_similarity_words, True),\n",
    "        SimilarityMetric.NGRAM_JACCARD: (\n",
    "            lambda text1, text2: TextMetrics.multi_ngram_jaccard_similarity(\n",
    "                text1, text2, ngram_sizes=[2, 3], weights=[0.5, 0.5]\n",
    "            ), \n",
    "            True\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches(\n",
    "        user_input: str, \n",
    "        df: pd.DataFrame, \n",
    "        metric: SimilarityMetric, \n",
    "        top_k: int = 1\n",
    "    ) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Find the top-k best matches using the specified similarity metric.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            df (pd.DataFrame): DataFrame containing 'razaosocial' and 'nome_fantasia' columns\n",
    "            metric (SimilarityMetric): The similarity metric to use\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[float], List[float]]: \n",
    "                razaosocial matches, nome_fantasia matches, razaosocial scores, nome_fantasia scores\n",
    "                \n",
    "        Raises:\n",
    "            ValueError: If the metric is not supported\n",
    "        \"\"\"\n",
    "        if metric not in TextRetrieval._METRIC_CONFIG:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "        \n",
    "        metric_func, reverse_sort = TextRetrieval._METRIC_CONFIG[metric]\n",
    "        \n",
    "        return TextRetrieval._find_matches_with_metric_vectorized(\n",
    "            user_input, df, metric_func, reverse_sort, top_k\n",
    "        )\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _find_matches_with_metric_vectorized(\n",
    "        user_input: str,\n",
    "        df: pd.DataFrame,\n",
    "        metric_func: Callable[[str, str], float],\n",
    "        reverse_sort: bool,\n",
    "        top_k: int\n",
    "    ) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Vectorized version using pandas operations for better performance.\n",
    "        \"\"\"\n",
    "        # Filter out rows where both columns are NaN\n",
    "        valid_mask = ~(df['razaosocial'].isna() & df['nome_fantasia'].isna())\n",
    "        if not valid_mask.any():\n",
    "            return [], [], [], []\n",
    "        \n",
    "        df_valid = df[valid_mask].copy()\n",
    "        \n",
    "        # Vectorized score calculation\n",
    "        razao_scores = df_valid['razaosocial'].apply(\n",
    "            lambda x: metric_func(x, user_input) if pd.notna(x) else None\n",
    "        )\n",
    "        nome_scores = df_valid['nome_fantasia'].apply(\n",
    "            lambda x: metric_func(x, user_input) if pd.notna(x) else None\n",
    "        )\n",
    "        \n",
    "        # Calculate max scores efficiently\n",
    "        if reverse_sort:  # Similarity metrics (higher is better)\n",
    "            max_scores = np.maximum(\n",
    "                razao_scores.fillna(-np.inf), \n",
    "                nome_scores.fillna(-np.inf)\n",
    "            )\n",
    "        else:  # Distance metrics (lower is better)\n",
    "            max_scores = np.minimum(\n",
    "                razao_scores.fillna(np.inf), \n",
    "                nome_scores.fillna(np.inf)\n",
    "            )\n",
    "        \n",
    "        # Sort indices by max scores\n",
    "        sorted_indices = np.argsort(max_scores)\n",
    "        if reverse_sort:\n",
    "            sorted_indices = sorted_indices[::-1]\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = sorted_indices[:top_k]\n",
    "        \n",
    "        best_razao_matches = df_valid.iloc[top_indices]['razaosocial'].tolist()\n",
    "        best_nome_fantasia_matches = df_valid.iloc[top_indices]['nome_fantasia'].tolist()\n",
    "        best_razao_scores = razao_scores.iloc[top_indices].tolist()\n",
    "        best_nome_fantasia_scores = nome_scores.iloc[top_indices].tolist()\n",
    "        \n",
    "        return best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def _find_matches_with_metric(\n",
    "        user_input: str,\n",
    "        df: pd.DataFrame,\n",
    "        metric_func: Callable[[str, str], float],\n",
    "        reverse_sort: bool,\n",
    "        top_k: int\n",
    "    ) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Core method to find matches using any metric function.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Input text to match against\n",
    "            df (pd.DataFrame): DataFrame containing 'razaosocial' and 'nome_fantasia' columns\n",
    "            metric_func (Callable): Function to calculate similarity/distance\n",
    "            reverse_sort (bool): Whether to sort in descending order (True for similarity, False for distance)\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[float], List[float]]: \n",
    "                razaosocial matches, nome_fantasia matches, razaosocial scores, nome_fantasia scores\n",
    "        \"\"\"\n",
    "        razao_social_cands = []\n",
    "        nome_fantasia_cands = []\n",
    "        razao_social_cand_sims = []\n",
    "        nome_fantasia_cand_sims = []\n",
    "        max_sims = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            razaosocial_cand = row['razaosocial']\n",
    "            nome_fantasia_cand = row['nome_fantasia']\n",
    "            \n",
    "            if pd.isna(razaosocial_cand) and pd.isna(nome_fantasia_cand):\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores for both columns\n",
    "            razaosocial_cand_sim = None\n",
    "            nome_fantasia_cand_sim = None\n",
    "            \n",
    "            if not pd.isna(razaosocial_cand):\n",
    "                razaosocial_cand_sim = metric_func(razaosocial_cand, user_input)\n",
    "            \n",
    "            if not pd.isna(nome_fantasia_cand):\n",
    "                nome_fantasia_cand_sim = metric_func(nome_fantasia_cand, user_input)\n",
    "            \n",
    "            # Calculate max similarity for ranking\n",
    "            valid_scores = [\n",
    "                score for score in [razaosocial_cand_sim, nome_fantasia_cand_sim] \n",
    "                if score is not None and not pd.isna(score)\n",
    "            ]\n",
    "            \n",
    "            if valid_scores:\n",
    "                if reverse_sort:  # For similarity metrics (higher is better)\n",
    "                    max_similarity = max(valid_scores)\n",
    "                else:  # For distance metrics (lower is better)\n",
    "                    max_similarity = min(valid_scores)\n",
    "                \n",
    "                razao_social_cands.append(razaosocial_cand)\n",
    "                nome_fantasia_cands.append(nome_fantasia_cand)\n",
    "                razao_social_cand_sims.append(razaosocial_cand_sim)\n",
    "                nome_fantasia_cand_sims.append(nome_fantasia_cand_sim)\n",
    "                max_sims.append(max_similarity)\n",
    "        \n",
    "        if not razao_social_cands:\n",
    "            return [], [], [], []\n",
    "        \n",
    "        # Create DataFrame for sorting\n",
    "        matches_scores = pd.DataFrame({\n",
    "            'razaosocial_cand': razao_social_cands,\n",
    "            'nome_fantasia_cand': nome_fantasia_cands,\n",
    "            'razaosocial_cand_sim': razao_social_cand_sims,\n",
    "            'nome_fantasia_cand_sim': nome_fantasia_cand_sims,\n",
    "            'max_similarity': max_sims\n",
    "        })\n",
    "        \n",
    "        # Sort by max similarity\n",
    "        matches_scores.sort_values(\n",
    "            by='max_similarity', \n",
    "            ascending=not reverse_sort, \n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        # Return top-k matches\n",
    "        top_matches = matches_scores.head(top_k)\n",
    "        best_razao_matches = top_matches['razaosocial_cand'].tolist()\n",
    "        best_nome_fantasia_matches = top_matches['nome_fantasia_cand'].tolist()\n",
    "        best_razao_scores = top_matches['razaosocial_cand_sim'].tolist()\n",
    "        best_nome_fantasia_scores = top_matches['nome_fantasia_cand_sim'].tolist()\n",
    "        \n",
    "        return best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores\n",
    "    \n",
    "    # All metric-specific methods now return the same 4-element tuple\n",
    "    @staticmethod\n",
    "    def find_best_matches_cer(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using Character Error Rate (CER).\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.CER, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_wer(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using Word Error Rate (WER).\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.WER, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_levenshtein(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using normalized Levenshtein distance.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.LEVENSHTEIN, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_jaccard_chars(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using character-level Jaccard similarity.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.JACCARD_CHARS, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_jaccard_words(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using word-level Jaccard similarity.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.JACCARD_WORDS, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_ngram_jaccard(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using n-gram Jaccard similarity.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.NGRAM_JACCARD, top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_combined_metric_performance(results_df: pd.DataFrame, \n",
    "                                        top_k: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze the performance of different metrics including the combined metric\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame returned from evaluate_retrieval_accuracy_top_k\n",
    "        top_k: Which top-k accuracy to analyze\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with performance metrics for each method\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract available metrics from column names\n",
    "    available_metrics = set()\n",
    "    \n",
    "    # Look for columns that match the pattern: {field}_top_{k}_{metric}_pred\n",
    "    for col in results_df.columns:\n",
    "        if '_top_' in col and col.endswith('_pred'):\n",
    "            # Split the column name to extract the metric\n",
    "            parts = col.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                # Find the position of 'top' in the parts\n",
    "                try:\n",
    "                    top_idx = parts.index('top')\n",
    "                    if top_idx + 2 < len(parts):  # Ensure we have enough parts after 'top'\n",
    "                        # Extract metric name (everything between the number and 'pred')\n",
    "                        metric_parts = parts[top_idx + 2:-1]  # Skip 'top', number, and 'pred'\n",
    "                        metric = '_'.join(metric_parts)\n",
    "                        available_metrics.add(metric)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Found metrics: {sorted(available_metrics)}\")\n",
    "    \n",
    "    performance_results = []\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        # Construct column names\n",
    "        razaosocial_top_k_col = f'razaosocial_top_{top_k}_{metric}_pred'\n",
    "        nome_fantasia_top_k_col = f'nome_fantasia_top_{top_k}_{metric}_pred'\n",
    "        razaosocial_top_1_col = f'razaosocial_top_1_{metric}_pred'\n",
    "        nome_fantasia_top_1_col = f'nome_fantasia_top_1_{metric}_pred'\n",
    "        razaosocial_rank_col = f'razaosocial_rank_{metric}'\n",
    "        nome_fantasia_rank_col = f'nome_fantasia_rank_{metric}'\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_cols = [razaosocial_top_k_col, nome_fantasia_top_k_col, \n",
    "                        razaosocial_top_1_col, nome_fantasia_top_1_col,\n",
    "                        razaosocial_rank_col, nome_fantasia_rank_col]\n",
    "        \n",
    "        missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing columns for metric '{metric}': {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Calculate top-k accuracy\n",
    "            razaosocial_top_k_accuracy = results_df[razaosocial_top_k_col].mean()\n",
    "            nome_fantasia_top_k_accuracy = results_df[nome_fantasia_top_k_col].mean()\n",
    "            overall_top_k_accuracy = (razaosocial_top_k_accuracy + nome_fantasia_top_k_accuracy) / 2\n",
    "            \n",
    "            # Calculate top-1 accuracy\n",
    "            razaosocial_top_1_accuracy = results_df[razaosocial_top_1_col].mean()\n",
    "            nome_fantasia_top_1_accuracy = results_df[nome_fantasia_top_1_col].mean()\n",
    "            overall_top_1_accuracy = (razaosocial_top_1_accuracy + nome_fantasia_top_1_accuracy) / 2\n",
    "            \n",
    "            # Calculate mean rank (lower is better, 0 means not found)\n",
    "            # Only consider non-zero ranks for mean calculation\n",
    "            razaosocial_ranks = results_df[razaosocial_rank_col]\n",
    "            nome_fantasia_ranks = results_df[nome_fantasia_rank_col]\n",
    "            \n",
    "            # Calculate mean rank excluding 0s (not found cases)\n",
    "            razaosocial_found_ranks = razaosocial_ranks[razaosocial_ranks > 0]\n",
    "            nome_fantasia_found_ranks = nome_fantasia_ranks[nome_fantasia_ranks > 0]\n",
    "            \n",
    "            mean_razaosocial_rank = razaosocial_found_ranks.mean() if len(razaosocial_found_ranks) > 0 else float('inf')\n",
    "            mean_nome_fantasia_rank = nome_fantasia_found_ranks.mean() if len(nome_fantasia_found_ranks) > 0 else float('inf')\n",
    "            \n",
    "            # Overall mean rank\n",
    "            if mean_razaosocial_rank != float('inf') and mean_nome_fantasia_rank != float('inf'):\n",
    "                overall_mean_rank = (mean_razaosocial_rank + mean_nome_fantasia_rank) / 2\n",
    "            elif mean_razaosocial_rank != float('inf'):\n",
    "                overall_mean_rank = mean_razaosocial_rank\n",
    "            elif mean_nome_fantasia_rank != float('inf'):\n",
    "                overall_mean_rank = mean_nome_fantasia_rank\n",
    "            else:\n",
    "                overall_mean_rank = float('inf')\n",
    "            \n",
    "            # Calculate retrieval rate (percentage of cases where target was found in top-k)\n",
    "            razaosocial_retrieval_rate = (razaosocial_ranks > 0).mean()\n",
    "            nome_fantasia_retrieval_rate = (nome_fantasia_ranks > 0).mean()\n",
    "            overall_retrieval_rate = (razaosocial_retrieval_rate + nome_fantasia_retrieval_rate) / 2\n",
    "            \n",
    "            performance_results.append({\n",
    "                'metric': metric,\n",
    "                f'razaosocial_top_{top_k}_accuracy': razaosocial_top_k_accuracy,\n",
    "                f'nome_fantasia_top_{top_k}_accuracy': nome_fantasia_top_k_accuracy,\n",
    "                f'overall_top_{top_k}_accuracy': overall_top_k_accuracy,\n",
    "                'razaosocial_top_1_accuracy': razaosocial_top_1_accuracy,\n",
    "                'nome_fantasia_top_1_accuracy': nome_fantasia_top_1_accuracy,\n",
    "                'overall_top_1_accuracy': overall_top_1_accuracy,\n",
    "                'mean_razaosocial_rank': mean_razaosocial_rank,\n",
    "                'mean_nome_fantasia_rank': mean_nome_fantasia_rank,\n",
    "                'overall_mean_rank': overall_mean_rank,\n",
    "                'razaosocial_retrieval_rate': razaosocial_retrieval_rate,\n",
    "                'nome_fantasia_retrieval_rate': nome_fantasia_retrieval_rate,\n",
    "                'overall_retrieval_rate': overall_retrieval_rate\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing metric '{metric}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not performance_results:\n",
    "        print(\"No valid metrics found for analysis\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_results)\n",
    "    \n",
    "    # Sort by overall top-1 accuracy (descending)\n",
    "    performance_df = performance_df.sort_values('overall_top_1_accuracy', ascending=False)\n",
    "    \n",
    "    # Round numeric columns for better readability\n",
    "    numeric_columns = performance_df.select_dtypes(include=[np.number]).columns\n",
    "    performance_df[numeric_columns] = performance_df[numeric_columns].round(4)\n",
    "    \n",
    "    return performance_df\n",
    "\n",
    "def print_performance_summary(performance_df: pd.DataFrame, top_k: int = 1):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the performance analysis\n",
    "    \n",
    "    Args:\n",
    "        performance_df: DataFrame from analyze_combined_metric_performance\n",
    "        top_k: The top-k value used in analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    if performance_df.empty:\n",
    "        print(\"No performance data to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PERFORMANCE SUMMARY - TOP-{top_k} RETRIEVAL ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š RANKING BY OVERALL TOP-1 ACCURACY:\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in performance_df.iterrows():\n",
    "        print(f\"{idx+1:2d}. {row['metric']:<15} | Top-1: {row['overall_top_1_accuracy']:.3f} | \"\n",
    "              f\"Top-{top_k}: {row[f'overall_top_{top_k}_accuracy']:.3f} | \"\n",
    "              f\"Avg Rank: {row['overall_mean_rank']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ DETAILED BREAKDOWN:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in performance_df.head(5).iterrows():  # Show top 5 metrics\n",
    "        print(f\"\\n{row['metric'].upper()}:\")\n",
    "        print(f\"  â€¢ RazÃ£o Social    - Top-1: {row['razaosocial_top_1_accuracy']:.3f}, \"\n",
    "              f\"Top-{top_k}: {row[f'razaosocial_top_{top_k}_accuracy']:.3f}, \"\n",
    "              f\"Retrieval Rate: {row['razaosocial_retrieval_rate']:.3f}\")\n",
    "        print(f\"  â€¢ Nome Fantasia   - Top-1: {row['nome_fantasia_top_1_accuracy']:.3f}, \"\n",
    "              f\"Top-{top_k}: {row[f'nome_fantasia_top_{top_k}_accuracy']:.3f}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_accuracy_top_k(df: pd.DataFrame,\n",
    "                                      top_k: int=1,\n",
    "                                      sample_size: int=None) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced evaluation with all available metrics including Jaccard similarity and combined metrics\"\"\"\n",
    "    \n",
    "    # Sample data if specified\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Using sample of {sample_size} records for evaluation\")\n",
    "    else:\n",
    "        df_sample = df\n",
    "        print(f\"Using all {len(df_sample)} records for evaluation\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Define base metrics\n",
    "    metrics_to_evaluate = ['jaccard_ngram',] #'wer', 'cer', 'levenshtein', 'jaccard_chars', 'jaccard_words']\n",
    "    metrics_funcs = {\n",
    "        'cer': TextRetrieval.find_best_matches_cer,\n",
    "        'wer': TextRetrieval.find_best_matches_wer,\n",
    "        'levenshtein': TextRetrieval.find_best_matches_levenshtein,\n",
    "        'jaccard_chars': TextRetrieval.find_best_matches_jaccard_chars,\n",
    "        'jaccard_words': TextRetrieval.find_best_matches_jaccard_words,\n",
    "        'jaccard_ngram': TextRetrieval.find_best_matches_ngram_jaccard\n",
    "    }\n",
    "    print(f\"Evaluating retrieval accuracy with top-{top_k} results using metrics: {metrics_to_evaluate}\")\n",
    "    for _, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Processing\"):\n",
    "        user_input = row['user_input']\n",
    "        uf = row['uf']\n",
    "        true_razaosocial = row['razaosocial']\n",
    "        true_nome_fantasia = row['nome_fantasia']\n",
    "        result_row = {\n",
    "            'user_input': user_input,\n",
    "            'uf': uf,\n",
    "            'true_razaosocial': true_razaosocial,\n",
    "            'true_nome_fantasia': true_nome_fantasia,\n",
    "        }\n",
    "        df_uf = df_sample[df_sample['uf'] == uf]\n",
    "        \n",
    "        # Dictionary to store all matches for each metric\n",
    "        all_matches = {}\n",
    "\n",
    "        for metric in metrics_to_evaluate:\n",
    "            # Get the function for the current metric\n",
    "            metric_func = metrics_funcs[metric]\n",
    "            \n",
    "            # Find matches using the current metric\n",
    "            best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores = metric_func(user_input, df_uf, top_k)\n",
    "            \n",
    "            # Store results in the all_matches dictionary\n",
    "            all_matches[metric] = {\n",
    "                'razaosocial': (best_razao_matches, best_razao_scores),\n",
    "                'nome_fantasia': (best_nome_fantasia_matches, best_nome_fantasia_scores),\n",
    "                'uf': uf\n",
    "            }\n",
    "\n",
    "        # Process results for each metric\n",
    "        for metric in metrics_to_evaluate:\n",
    "            for field in ['razaosocial', 'nome_fantasia']:\n",
    "                matches, scores = all_matches[metric][field]\n",
    "                true_value = true_razaosocial if field == 'razaosocial' else true_nome_fantasia\n",
    "                \n",
    "                # Top-k accuracy\n",
    "                top_k_pred = true_value in matches\n",
    "                metric_col = f'{field}_top_{top_k}_{metric}_pred'\n",
    "                result_row[metric_col] = top_k_pred\n",
    "\n",
    "                # Top-1 accuracy\n",
    "                top_1_pred = matches[0] == true_value if matches else False\n",
    "                metric_col = f'{field}_top_1_{metric}_pred'\n",
    "                result_row[metric_col] = top_1_pred\n",
    "                \n",
    "                # Ranking\n",
    "                rank = matches.index(true_value) + 1 if true_value in matches else 0\n",
    "                result_row[f'{field}_rank_{metric}'] = rank\n",
    "                \n",
    "                # Store the actual scores for analysis\n",
    "                if matches:\n",
    "                    result_row[f'best_{metric}_score_{field}'] = scores[0] if scores else None\n",
    "        \n",
    "        results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results), matches\n",
    "\n",
    "\n",
    "import json, os\n",
    "def evaluate_retrieval_accuracy_top_k_with_checkpoint(df: pd.DataFrame,\n",
    "                                                     top_k: int=1,\n",
    "                                                     sample_size: int=None,\n",
    "                                                     output_file: str=\"retrieval_results.json\",\n",
    "                                                     checkpoint_every: int=10) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Optimized version that writes results iteratively to JSON and can resume from checkpoints\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with test data\n",
    "        top_k: Number of top matches to return\n",
    "        sample_size: Number of samples to use (None for all)\n",
    "        output_file: Path to JSON file for storing results\n",
    "        checkpoint_every: Save to file every N records\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample data if specified\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Using sample of {sample_size} records for evaluation\")\n",
    "    else:\n",
    "        df_sample = df.reset_index(drop=True)\n",
    "        print(f\"Using all {len(df_sample)} records for evaluation\")\n",
    "    \n",
    "    # Load existing results if file exists\n",
    "    processed_indices = set()\n",
    "    existing_results = []\n",
    "    existing_matches = {}\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                existing_results = data.get('results', [])\n",
    "                existing_matches = data.get('matches', {})\n",
    "                processed_indices = {item['test_index'] for item in existing_results}\n",
    "            print(f\"Loaded {len(existing_results)} existing results from {output_file}\")\n",
    "            print(f\"Resuming from index {max(processed_indices) + 1 if processed_indices else 0}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing file: {e}. Starting fresh.\")\n",
    "            processed_indices = set()\n",
    "            existing_results = []\n",
    "            existing_matches = {}\n",
    "    \n",
    "    # Group by UF once to avoid repeated filtering\n",
    "    uf_groups = df_sample.groupby('uf')\n",
    "    \n",
    "    results = existing_results.copy()\n",
    "    all_matches = existing_matches.copy()\n",
    "    \n",
    "    metrics_to_evaluate = ['jaccard_ngram']\n",
    "    metrics_funcs = {\n",
    "        'jaccard_ngram': TextRetrieval.find_best_matches_ngram_jaccard\n",
    "    }\n",
    "    \n",
    "    print(f\"Evaluating retrieval accuracy with top-{top_k} results using metrics: {metrics_to_evaluate}\")\n",
    "    \n",
    "    records_processed = 0\n",
    "    total_new_records = 0\n",
    "    \n",
    "    # Count total records to process\n",
    "    for uf, uf_group in uf_groups:\n",
    "        for idx in uf_group.index:\n",
    "            if idx not in processed_indices:\n",
    "                total_new_records += 1\n",
    "    \n",
    "    print(f\"Total new records to process: {total_new_records}\")\n",
    "    \n",
    "    def save_checkpoint():\n",
    "        \"\"\"Save current progress to JSON file\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'metadata': {\n",
    "                'total_records': len(df_sample),\n",
    "                'processed_records': len(results),\n",
    "                'top_k': top_k,\n",
    "                'metrics': metrics_to_evaluate,\n",
    "                'last_updated': pd.Timestamp.now().isoformat()\n",
    "            },\n",
    "            'results': results,\n",
    "            'matches': all_matches\n",
    "        }\n",
    "        \n",
    "        # Write to temporary file first, then rename (atomic operation)\n",
    "        temp_file = output_file + '.tmp'\n",
    "        with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)\n",
    "        os.rename(temp_file, output_file)\n",
    "    \n",
    "    # Process each UF group separately\n",
    "    try:\n",
    "        with tqdm(total=total_new_records, desc=\"Processing records\") as pbar:\n",
    "            for uf, uf_group in uf_groups:\n",
    "                # Process all records in this UF group\n",
    "                for original_idx in uf_group.index:\n",
    "                    # Skip if already processed\n",
    "                    if original_idx in processed_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    row = df_sample.loc[original_idx]\n",
    "                    user_input = row['user_input']\n",
    "                    true_razaosocial = row['razaosocial']\n",
    "                    true_nome_fantasia = row['nome_fantasia']\n",
    "                    \n",
    "                    # Create a unique key for this record\n",
    "                    record_key = f\"test_{original_idx}\"\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'test_index': original_idx,\n",
    "                        'record_key': record_key,\n",
    "                        'user_input': user_input,\n",
    "                        'uf': uf,\n",
    "                        'true_razaosocial': true_razaosocial,\n",
    "                        'true_nome_fantasia': true_nome_fantasia,\n",
    "                        'processed_at': pd.Timestamp.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Initialize matches dictionary for this record\n",
    "                    all_matches[record_key] = {}\n",
    "                    \n",
    "                    # Process metrics\n",
    "                    for metric in metrics_to_evaluate:\n",
    "                        metric_func = metrics_funcs[metric]\n",
    "                        \n",
    "                        try:\n",
    "                            # Find matches using the current metric\n",
    "                            best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores = metric_func(user_input, uf_group, top_k)\n",
    "                            \n",
    "                            # Store matches for this record and metric\n",
    "                            all_matches[record_key][metric] = {\n",
    "                                'razaosocial_matches': best_razao_matches,\n",
    "                                'nome_fantasia_matches': best_nome_fantasia_matches,\n",
    "                                'razaosocial_scores': best_razao_scores,\n",
    "                                'nome_fantasia_scores': best_nome_fantasia_scores,\n",
    "                                'uf': uf\n",
    "                            }\n",
    "                            \n",
    "                            # Process results for each field\n",
    "                            for field, matches, scores, true_value in [\n",
    "                                ('razaosocial', best_razao_matches, best_razao_scores, true_razaosocial),\n",
    "                                ('nome_fantasia', best_nome_fantasia_matches, best_nome_fantasia_scores, true_nome_fantasia)\n",
    "                            ]:\n",
    "                                # Top-k accuracy\n",
    "                                top_k_pred = true_value in matches\n",
    "                                result_row[f'{field}_top_{top_k}_{metric}_pred'] = top_k_pred\n",
    "                                \n",
    "                                # Top-1 accuracy\n",
    "                                top_1_pred = matches[0] == true_value if matches else False\n",
    "                                result_row[f'{field}_top_1_{metric}_pred'] = top_1_pred\n",
    "                                \n",
    "                                # Ranking\n",
    "                                rank = matches.index(true_value) + 1 if true_value in matches else 0\n",
    "                                result_row[f'{field}_rank_{metric}'] = rank\n",
    "                                \n",
    "                                # Store the actual scores\n",
    "                                result_row[f'best_{metric}_score_{field}'] = scores[0] if scores else None\n",
    "                                \n",
    "                                # Add detailed top-k and top-1 information to matches\n",
    "                                field_key = f'{field}_evaluation'\n",
    "                                if field_key not in all_matches[record_key][metric]:\n",
    "                                    all_matches[record_key][metric][field_key] = {}\n",
    "                                \n",
    "                                all_matches[record_key]['user_input'] = user_input  \n",
    "                                all_matches[record_key][metric][field_key] = {\n",
    "                                    'true_value': true_value,\n",
    "                                    'top_1': {\n",
    "                                        'predicted': matches[0] if matches else None,\n",
    "                                        'score': scores[0] if scores else None,\n",
    "                                        'is_correct': top_1_pred,\n",
    "                                        'rank': 1 if top_1_pred else (rank if rank > 0 else None)\n",
    "                                    },\n",
    "                                    f'top_{top_k}': {\n",
    "                                        'predicted_list': matches[:top_k] if matches else [],\n",
    "                                        'scores_list': scores[:top_k] if scores else [],\n",
    "                                        'is_correct': top_k_pred,\n",
    "                                        'rank': rank if rank > 0 else None,\n",
    "                                        'found_at_position': rank if top_k_pred else None\n",
    "                                    },\n",
    "                                    'all_matches': {\n",
    "                                        'candidates': matches,\n",
    "                                        'scores': scores,\n",
    "                                        'total_candidates': len(matches) if matches else 0\n",
    "                                    }\n",
    "                                }\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing record {original_idx} with metric {metric}: {e}\")\n",
    "                            # Store error information\n",
    "                            result_row[f'error_{metric}'] = str(e)\n",
    "                            all_matches[record_key][metric] = {'error': str(e)}\n",
    "                    \n",
    "                    results.append(result_row)\n",
    "                    records_processed += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # Save checkpoint periodically\n",
    "                    if records_processed % checkpoint_every == 0:\n",
    "                        save_checkpoint()\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted. Saving current progress...\")\n",
    "        save_checkpoint()\n",
    "        print(f\"Progress saved. Processed {records_processed} records.\")\n",
    "        raise\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        save_checkpoint()\n",
    "        print(\"Progress saved before error.\")\n",
    "        raise\n",
    "    \n",
    "    # Final save\n",
    "    save_checkpoint()\n",
    "    print(f\"Processing complete. Total records processed: {len(results)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df, all_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all 255471 records for evaluation\n",
      "Loaded 5581 existing results from retrieval_results.json\n",
      "Resuming from index 255459\n",
      "Evaluating retrieval accuracy with top-5 results using metrics: ['jaccard_ngram']\n",
      "Total new records to process: 249890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records:  18%|â–ˆâ–Š        | 44680/249890 [5:12:07<21:45:38,  2.62it/s]    "
     ]
    }
   ],
   "source": [
    "# Carregando todos as razoes sociais e nomes fantasia Ãºnicos\n",
    "unique_razaosocial_dict = {uf: df[df['uf'] == uf]['razaosocial'].dropna().unique().tolist() for uf in df['uf'].unique()}\n",
    "# Carregando todos os nomes fantasia Ãºnicos\n",
    "unique_nome_fantasia_dict = {uf: df[df['uf'] == uf]['nome_fantasia'].dropna().unique().tolist() for uf in df['uf'].unique()}\n",
    "sample_size = None  # Define a sample size for evaluation, or set to None to use all data\n",
    "top_k = 5\n",
    "retrieved, matches = evaluate_retrieval_accuracy_top_k_with_checkpoint(df=df, top_k=top_k,sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>uf</th>\n",
       "      <th>true_razaosocial</th>\n",
       "      <th>true_nome_fantasia</th>\n",
       "      <th>record_key</th>\n",
       "      <th>razaosocial_top_5_jaccard_ngram_pred</th>\n",
       "      <th>razaosocial_top_1_jaccard_ngram_pred</th>\n",
       "      <th>razaosocial_rank_jaccard_ngram</th>\n",
       "      <th>best_jaccard_ngram_score_razaosocial</th>\n",
       "      <th>nome_fantasia_top_5_jaccard_ngram_pred</th>\n",
       "      <th>nome_fantasia_top_1_jaccard_ngram_pred</th>\n",
       "      <th>nome_fantasia_rank_jaccard_ngram</th>\n",
       "      <th>best_jaccard_ngram_score_nome_fantasia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kadrangular ebanjeio</td>\n",
       "      <td>AC</td>\n",
       "      <td>igreja evangelho quadrangular</td>\n",
       "      <td>cruzada nacional evangelizacao</td>\n",
       "      <td>AC_0_kadrangular ebanjeio</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cruzada ebanjelica</td>\n",
       "      <td>AC</td>\n",
       "      <td>igreja evangelho quadrangular</td>\n",
       "      <td>cruzada nacional evangelizacao</td>\n",
       "      <td>AC_1_cruzada ebanjelica</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.295828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maceio auto</td>\n",
       "      <td>AL</td>\n",
       "      <td>m i s barbosa auto repasse</td>\n",
       "      <td>maceio auto repasse</td>\n",
       "      <td>AL_0_maceio auto</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.144963</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cr quatro quatro cinco zero</td>\n",
       "      <td>AL</td>\n",
       "      <td>sapore</td>\n",
       "      <td>cr 4450</td>\n",
       "      <td>AL_1_cr quatro quatro cinco zero</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cruzada ebanjelizacao</td>\n",
       "      <td>AL</td>\n",
       "      <td>igreja evangelho quadrangular</td>\n",
       "      <td>cruzada nacional evangelizacao</td>\n",
       "      <td>AL_2_cruzada ebanjelizacao</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066434</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>mcdonalds alimentos</td>\n",
       "      <td>SP</td>\n",
       "      <td>arcos dourados alimentos</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>SP_302_mcdonalds alimentos</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>teodoro business</td>\n",
       "      <td>SP</td>\n",
       "      <td>allpark empreendimentos participacoes</td>\n",
       "      <td>ed teodoro business</td>\n",
       "      <td>SP_303_teodoro business</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029499</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>wms</td>\n",
       "      <td>TO</td>\n",
       "      <td>wms supermercados brasil</td>\n",
       "      <td>atacadao</td>\n",
       "      <td>TO_0_wms</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>serrado jalapao</td>\n",
       "      <td>TO</td>\n",
       "      <td>pousada cerrado beach jalapao</td>\n",
       "      <td>cerrado beach</td>\n",
       "      <td>TO_1_serrado jalapao</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.411521</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>cruzada nacional</td>\n",
       "      <td>TO</td>\n",
       "      <td>igreja evangelho quadrangular</td>\n",
       "      <td>cruzada nacional evangelizacao</td>\n",
       "      <td>TO_2_cruzada nacional</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_input  uf                       true_razaosocial  \\\n",
       "0           kadrangular ebanjeio  AC          igreja evangelho quadrangular   \n",
       "1             cruzada ebanjelica  AC          igreja evangelho quadrangular   \n",
       "2                    maceio auto  AL             m i s barbosa auto repasse   \n",
       "3    cr quatro quatro cinco zero  AL                                 sapore   \n",
       "4          cruzada ebanjelizacao  AL          igreja evangelho quadrangular   \n",
       "..                           ...  ..                                    ...   \n",
       "995          mcdonalds alimentos  SP               arcos dourados alimentos   \n",
       "996             teodoro business  SP  allpark empreendimentos participacoes   \n",
       "997                          wms  TO               wms supermercados brasil   \n",
       "998              serrado jalapao  TO          pousada cerrado beach jalapao   \n",
       "999             cruzada nacional  TO          igreja evangelho quadrangular   \n",
       "\n",
       "                 true_nome_fantasia                        record_key  \\\n",
       "0    cruzada nacional evangelizacao         AC_0_kadrangular ebanjeio   \n",
       "1    cruzada nacional evangelizacao           AC_1_cruzada ebanjelica   \n",
       "2               maceio auto repasse                  AL_0_maceio auto   \n",
       "3                           cr 4450  AL_1_cr quatro quatro cinco zero   \n",
       "4    cruzada nacional evangelizacao        AL_2_cruzada ebanjelizacao   \n",
       "..                              ...                               ...   \n",
       "995                       mcdonalds        SP_302_mcdonalds alimentos   \n",
       "996             ed teodoro business           SP_303_teodoro business   \n",
       "997                        atacadao                          TO_0_wms   \n",
       "998                   cerrado beach              TO_1_serrado jalapao   \n",
       "999  cruzada nacional evangelizacao             TO_2_cruzada nacional   \n",
       "\n",
       "     razaosocial_top_5_jaccard_ngram_pred  \\\n",
       "0                                    True   \n",
       "1                                    True   \n",
       "2                                    True   \n",
       "3                                    True   \n",
       "4                                    True   \n",
       "..                                    ...   \n",
       "995                                  True   \n",
       "996                                  True   \n",
       "997                                  True   \n",
       "998                                  True   \n",
       "999                                  True   \n",
       "\n",
       "     razaosocial_top_1_jaccard_ngram_pred  razaosocial_rank_jaccard_ngram  \\\n",
       "0                                    True                               1   \n",
       "1                                    True                               1   \n",
       "2                                    True                               1   \n",
       "3                                    True                               1   \n",
       "4                                    True                               1   \n",
       "..                                    ...                             ...   \n",
       "995                                  True                               1   \n",
       "996                                  True                               1   \n",
       "997                                  True                               1   \n",
       "998                                  True                               1   \n",
       "999                                  True                               1   \n",
       "\n",
       "     best_jaccard_ngram_score_razaosocial  \\\n",
       "0                                0.222488   \n",
       "1                                0.069728   \n",
       "2                                0.144963   \n",
       "3                                0.000000   \n",
       "4                                0.066434   \n",
       "..                                    ...   \n",
       "995                              0.371429   \n",
       "996                              0.029499   \n",
       "997                              0.116071   \n",
       "998                              0.411521   \n",
       "999                              0.023810   \n",
       "\n",
       "     nome_fantasia_top_5_jaccard_ngram_pred  \\\n",
       "0                                      True   \n",
       "1                                      True   \n",
       "2                                      True   \n",
       "3                                      True   \n",
       "4                                      True   \n",
       "..                                      ...   \n",
       "995                                    True   \n",
       "996                                    True   \n",
       "997                                    True   \n",
       "998                                    True   \n",
       "999                                    True   \n",
       "\n",
       "     nome_fantasia_top_1_jaccard_ngram_pred  nome_fantasia_rank_jaccard_ngram  \\\n",
       "0                                      True                                 1   \n",
       "1                                      True                                 1   \n",
       "2                                      True                                 1   \n",
       "3                                      True                                 1   \n",
       "4                                      True                                 1   \n",
       "..                                      ...                               ...   \n",
       "995                                   False                                 2   \n",
       "996                                    True                                 1   \n",
       "997                                    True                                 1   \n",
       "998                                    True                                 1   \n",
       "999                                    True                                 1   \n",
       "\n",
       "     best_jaccard_ngram_score_nome_fantasia  \n",
       "0                                  0.090659  \n",
       "1                                  0.295828  \n",
       "2                                  0.514130  \n",
       "3                                  0.114224  \n",
       "4                                  0.470753  \n",
       "..                                      ...  \n",
       "995                                1.000000  \n",
       "996                                0.728778  \n",
       "997                                0.000000  \n",
       "998                                0.217593  \n",
       "999                                0.493915  \n",
       "\n",
       "[1000 rows x 13 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>uf</th>\n",
       "      <th>true_razaosocial</th>\n",
       "      <th>true_nome_fantasia</th>\n",
       "      <th>record_key</th>\n",
       "      <th>razaosocial_top_5_jaccard_ngram_pred</th>\n",
       "      <th>razaosocial_top_1_jaccard_ngram_pred</th>\n",
       "      <th>razaosocial_rank_jaccard_ngram</th>\n",
       "      <th>best_jaccard_ngram_score_razaosocial</th>\n",
       "      <th>nome_fantasia_top_5_jaccard_ngram_pred</th>\n",
       "      <th>nome_fantasia_top_1_jaccard_ngram_pred</th>\n",
       "      <th>nome_fantasia_rank_jaccard_ngram</th>\n",
       "      <th>best_jaccard_ngram_score_nome_fantasia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>bajir</td>\n",
       "      <td>BA</td>\n",
       "      <td>viacao aerea rio grandense falida</td>\n",
       "      <td>varig</td>\n",
       "      <td>BA_29_bajir</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>latino farmacia</td>\n",
       "      <td>PR</td>\n",
       "      <td>latino americana medicamentos</td>\n",
       "      <td>farmacia preco popular</td>\n",
       "      <td>PR_60_latino farmacia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>c</td>\n",
       "      <td>RJ</td>\n",
       "      <td>lojas cem</td>\n",
       "      <td>lojas cem</td>\n",
       "      <td>RJ_16_c</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>sia latino farmacia</td>\n",
       "      <td>RS</td>\n",
       "      <td>latino americana medicamentos</td>\n",
       "      <td>farmacia preco popular</td>\n",
       "      <td>RS_1_sia latino farmacia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.337469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>seeg</td>\n",
       "      <td>RS</td>\n",
       "      <td>cpfl transmissao</td>\n",
       "      <td>ceee gt</td>\n",
       "      <td>RS_66_seeg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>lojas</td>\n",
       "      <td>RS</td>\n",
       "      <td>solar agroindustria</td>\n",
       "      <td>lojas solar</td>\n",
       "      <td>RS_97_lojas</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077381</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>goban industria</td>\n",
       "      <td>SP</td>\n",
       "      <td>saint gobain brasil produtos industriais const...</td>\n",
       "      <td>sekurit</td>\n",
       "      <td>SP_33_goban industria</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>social</td>\n",
       "      <td>SP</td>\n",
       "      <td>sest social transporte</td>\n",
       "      <td>sao paulo sp unidade n 01</td>\n",
       "      <td>SP_40_social</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.231111</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>oab sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>ordem advogados brasil seccao sao paulo</td>\n",
       "      <td>sub seccao promissao</td>\n",
       "      <td>SP_122_oab sao paulo</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.417391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>mitra diosesana</td>\n",
       "      <td>SP</td>\n",
       "      <td>mitra diocesana campo limpo</td>\n",
       "      <td>paroquia santa luzia</td>\n",
       "      <td>SP_138_mitra diosesana</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_input  uf  \\\n",
       "53                 bajir  BA   \n",
       "360      latino farmacia  PR   \n",
       "452                    c  RJ   \n",
       "496  sia latino farmacia  RS   \n",
       "561                 seeg  RS   \n",
       "592                lojas  RS   \n",
       "726      goban industria  SP   \n",
       "733               social  SP   \n",
       "815        oab sao paulo  SP   \n",
       "831      mitra diosesana  SP   \n",
       "\n",
       "                                      true_razaosocial  \\\n",
       "53                   viacao aerea rio grandense falida   \n",
       "360                      latino americana medicamentos   \n",
       "452                                          lojas cem   \n",
       "496                      latino americana medicamentos   \n",
       "561                                   cpfl transmissao   \n",
       "592                                solar agroindustria   \n",
       "726  saint gobain brasil produtos industriais const...   \n",
       "733                             sest social transporte   \n",
       "815            ordem advogados brasil seccao sao paulo   \n",
       "831                        mitra diocesana campo limpo   \n",
       "\n",
       "            true_nome_fantasia                record_key  \\\n",
       "53                       varig               BA_29_bajir   \n",
       "360     farmacia preco popular     PR_60_latino farmacia   \n",
       "452                  lojas cem                   RJ_16_c   \n",
       "496     farmacia preco popular  RS_1_sia latino farmacia   \n",
       "561                    ceee gt                RS_66_seeg   \n",
       "592                lojas solar               RS_97_lojas   \n",
       "726                    sekurit     SP_33_goban industria   \n",
       "733  sao paulo sp unidade n 01              SP_40_social   \n",
       "815       sub seccao promissao      SP_122_oab sao paulo   \n",
       "831       paroquia santa luzia    SP_138_mitra diosesana   \n",
       "\n",
       "     razaosocial_top_5_jaccard_ngram_pred  \\\n",
       "53                                  False   \n",
       "360                                 False   \n",
       "452                                 False   \n",
       "496                                 False   \n",
       "561                                 False   \n",
       "592                                 False   \n",
       "726                                 False   \n",
       "733                                 False   \n",
       "815                                 False   \n",
       "831                                 False   \n",
       "\n",
       "     razaosocial_top_1_jaccard_ngram_pred  razaosocial_rank_jaccard_ngram  \\\n",
       "53                                  False                               0   \n",
       "360                                 False                               0   \n",
       "452                                 False                               0   \n",
       "496                                 False                               0   \n",
       "561                                 False                               0   \n",
       "592                                 False                               0   \n",
       "726                                 False                               0   \n",
       "733                                 False                               0   \n",
       "815                                 False                               0   \n",
       "831                                 False                               0   \n",
       "\n",
       "     best_jaccard_ngram_score_razaosocial  \\\n",
       "53                               0.103175   \n",
       "360                              0.000000   \n",
       "452                              0.062745   \n",
       "496                              0.000000   \n",
       "561                              0.000000   \n",
       "592                              0.077381   \n",
       "726                              0.318182   \n",
       "733                              0.231111   \n",
       "815                              0.417391   \n",
       "831                              0.532468   \n",
       "\n",
       "     nome_fantasia_top_5_jaccard_ngram_pred  \\\n",
       "53                                    False   \n",
       "360                                   False   \n",
       "452                                   False   \n",
       "496                                   False   \n",
       "561                                   False   \n",
       "592                                   False   \n",
       "726                                   False   \n",
       "733                                   False   \n",
       "815                                   False   \n",
       "831                                   False   \n",
       "\n",
       "     nome_fantasia_top_1_jaccard_ngram_pred  nome_fantasia_rank_jaccard_ngram  \\\n",
       "53                                    False                                 0   \n",
       "360                                   False                                 0   \n",
       "452                                   False                                 0   \n",
       "496                                   False                                 0   \n",
       "561                                   False                                 0   \n",
       "592                                   False                                 0   \n",
       "726                                   False                                 0   \n",
       "733                                   False                                 0   \n",
       "815                                   False                                 0   \n",
       "831                                   False                                 0   \n",
       "\n",
       "     best_jaccard_ngram_score_nome_fantasia  \n",
       "53                                 0.027526  \n",
       "360                                0.375652  \n",
       "452                                0.000000  \n",
       "496                                0.337469  \n",
       "561                                0.112500  \n",
       "592                                0.464286  \n",
       "726                                0.000000  \n",
       "733                                0.222527  \n",
       "815                                0.417391  \n",
       "831                                0.029337  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved[retrieved['razaosocial_top_5_jaccard_ngram_pred'] == False].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found metrics: ['jaccard_ngram']\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY - TOP-5 RETRIEVAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š RANKING BY OVERALL TOP-1 ACCURACY:\n",
      "--------------------------------------------------\n",
      " 1. jaccard_ngram   | Top-1: 0.890 | Top-5: 0.977 | Avg Rank: 1.15\n",
      "\n",
      "ðŸŽ¯ DETAILED BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "JACCARD_NGRAM:\n",
      "  â€¢ RazÃ£o Social    - Top-1: 0.938, Top-5: 0.989, Retrieval Rate: 0.989\n",
      "  â€¢ Nome Fantasia   - Top-1: 0.842, Top-5: 0.964, \n"
     ]
    }
   ],
   "source": [
    "performance_analysis = analyze_combined_metric_performance(retrieved, top_k=top_k)\n",
    "print_performance_summary(performance_analysis, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieved.to_csv(\"10000_rand_42_retrieved.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(evaluation_df):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1 score from evaluation results\n",
    "    \n",
    "    Parameters:\n",
    "    evaluation_df (pd.DataFrame): Output from evaluate_retrieval_accuracy function\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing all metrics for different scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics_results = {}\n",
    "    \n",
    "    # Define the scenarios to evaluate\n",
    "    scenarios = [\n",
    "        ('razaosocial_top_k_cer', 'razaosocial_top_k_cer_pred'),\n",
    "        ('nome_fantasia_top_k_cer', 'nome_fantasia_top_k_cer_pred'),\n",
    "        ('razaosocial_top_k_wer', 'razaosocial_top_k_wer_pred'),\n",
    "        ('nome_fantasia_top_k_wer', 'nome_fantasia_top_k_wer_pred'),\n",
    "        ('razaosocial_top_k_levenshtein', 'razaosocial_top_k_lev_dist_pred'),\n",
    "        ('nome_fantasia_top_k_levenshtein', 'nome_fantasia_top_k_lev_dist_pred'),\n",
    "\n",
    "        ('razaosocial_top_1_cer', 'razaosocial_top_1_cer_pred'),\n",
    "        ('nome_fantasia_top_1_cer', 'nome_fantasia_top_1_cer_pred'),\n",
    "        ('razaosocial_top_1_wer', 'razaosocial_top_1_wer_pred'),\n",
    "        ('nome_fantasia_top_1_wer', 'nome_fantasia_top_1_wer_pred'),\n",
    "        ('razaosocial_top_1_levenshtein', 'razaosocial_top_1_lev_dist_pred'),\n",
    "        ('nome_fantasia_top_1_levenshtein', 'nome_fantasia_top_1_lev_dist_pred')\n",
    "    ]\n",
    "    \n",
    "    print(\"Classification Metrics Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for scenario_name, correct_column in scenarios:\n",
    "        print(f\"\\n{scenario_name.upper()} Results:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Get true labels (1 for correct, 0 for incorrect)\n",
    "        y_true = [1] * len(evaluation_df)\n",
    "        # Get predicted labels (1 for correct prediction, 0 for incorrect)\n",
    "        y_pred = evaluation_df[correct_column].astype(int).tolist()\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "        \n",
    "        metrics_results[scenario_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'correct_predictions': sum(y_pred),\n",
    "            'total_predictions': len(y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy:  {accuracy:.4f} ({sum(y_pred)}/{len(y_pred)})\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate all metrics\n",
    "comprehensive_metrics = calculate_classification_metrics(retrieved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
