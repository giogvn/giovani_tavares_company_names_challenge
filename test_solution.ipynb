{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregando os Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insira o path para o arquivo de testes\n",
    "test_file_path = \"dados/train.parquet\"\n",
    "\n",
    "# Insira o nome da coluna que tem o input de usuário\n",
    "user_input_col = \"user_input\"\n",
    "\n",
    "# Insira o nome da coluna que tem o UF\n",
    "uf_col = \"uf\"\n",
    "\n",
    "# Insira o nome da coluna que tem a razaosocial\n",
    "razaosocial_col = \"razaosocial\"\n",
    "\n",
    "# Insira o nome da coluna que tem o nome_fantasia\n",
    "nome_fantasia_col = \"nome_fantasia\"\n",
    "\n",
    "# Insira o nome da coluna que tem o cnpj\n",
    "cnpj_col = \"cnpj\"\n",
    "\n",
    "cols_to_keep = [user_input_col, uf_col, razaosocial_col, nome_fantasia_col, cnpj_col]       \n",
    "df = pd.read_parquet(\"dados/train.parquet\")[cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparando os Dados - Rode todas as células dessa seção\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Remocao de Termos Irrelevantes\n",
    "\n",
    "Colunas com o sufixo `_cleaned` serao criadas para as colunas `user_input`, `razaosocial` e `nome_fantasia`. Nelas, termos como \"S.A.\", \"LTDA\", \"LTDA.\", \"S/A\", \"S.A\", \"Ltda\", \"Ltda.\", \"S/A.\", \"S.A.\", \"S.A\", \"Ltda\" e \"Ltda\" serao removidos.\n",
    "\n",
    "\n",
    "Essa remocao é importante, pois ao se calcular a similaridade entre um `user_input` e/ou `razaosocial` e `nome_fantasia`, as métricas de similaridade seriam prejudicadas na ausencia de tais termos no `user_input`. \n",
    "\n",
    "Também serao removidas acentos e stopwords da língua portuguesa, sobretudo preposicoes. Tais termos, por nao informarem sobre empresas específicas, podem prejudicar a acurácia das buscas/retrieval.\n",
    "Além disso, passaremos tudo para letras minúsculas, para evitar problemas de case-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_text_cleaning(text, \n",
    "                               remove_accents=True,\n",
    "                               remove_stop_words=True, \n",
    "                               remove_company_suffixes=True,\n",
    "                               custom_stop_words=None,\n",
    "                               to_lowercase=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "    remove_accents (bool): Remove accents and normalize characters\n",
    "    remove_stop_words (bool): Remove Portuguese stop words\n",
    "    remove_company_suffixes (bool): Remove common company suffixes\n",
    "    custom_stop_words (set): Additional stop words to remove\n",
    "    to_lowercase (bool): Convert to lowercase\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    if remove_accents:\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = text.replace('ç', 'c').replace('Ç', 'C')\n",
    "    \n",
    "    if to_lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if remove_company_suffixes:\n",
    "        patterns_to_remove = [\n",
    "        r'\\bS\\.?A\\.?\\b',           # S.A, SA, S.A., SA.\n",
    "        r'\\bS/A\\.?\\b',             # S/A, S/A.\n",
    "        r'\\bLTDA\\.?\\b',            # LTDA, LTDA.\n",
    "        r'\\bLIMITADA\\b',           # LIMITADA\n",
    "        r'\\bCIA\\.?\\b',             # CIA, CIA.\n",
    "        r'\\bCOMPANHIA\\b',          # COMPANHIA\n",
    "        r'\\bEMPRESA\\b',            # EMPRESA\n",
    "        r'\\bCOMERCIO\\b',           # COMERCIO\n",
    "        r'\\bSERVICOS?\\b',          # SERVICO, SERVICOS\n",
    "        r'\\bME\\b',                 # ME (Microempresa)\n",
    "        r'\\bEPP\\b',                # EPP (Empresa de Pequeno Porte)\n",
    "        r'\\bEIRELI\\b',             # EIRELI\n",
    "        r'\\bSOCIEDADE\\b',          # SOCIEDADE\n",
    "        r'ADMINISTRADORA\\b',       # ADMINISTRADORA\n",
    "        r'GERAL\\b',                # GERAL\n",
    "    ]\n",
    "        \n",
    "        for pattern in patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    if remove_stop_words:\n",
    "        portuguese_stop_words = {\n",
    "            'a', 'ao', 'aos', 'as', 'da', 'das', 'de', 'do', 'dos', 'e', 'em', 'na', \n",
    "            'nas', 'no', 'nos', 'o', 'os', 'para', 'por', 'com', 'um', 'uma', 'uns', \n",
    "            'umas', 'se', 'que', 'ou', 'mas', 'como', 'mais', 'muito', 'sua', 'seu',\n",
    "            'seus', 'suas', 'este', 'esta', 'estes', 'estas', 'esse', 'essa', 'esses',\n",
    "            'essas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'isso', 'aquilo'\n",
    "        }\n",
    "        \n",
    "        if custom_stop_words:\n",
    "            portuguese_stop_words.update(custom_stop_words)\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.lower() not in portuguese_stop_words]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Multiple spaces to single space\n",
    "    text = text.strip()                   # Remove leading/trailing spaces\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_input_cleaned'] = df[user_input_col].apply(comprehensive_text_cleaning)\n",
    "df['razaosocial_cleaned'] = df[razaosocial_col].apply(comprehensive_text_cleaning)\n",
    "df['nome_fantasia_cleaned'] = df[nome_fantasia_col].apply(comprehensive_text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retriever com TF-IDF e Similaridade do Cosseno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implementação do Retriever\n",
    "\n",
    "O retriever será implementado na classe `TextRetriever`.\n",
    "\n",
    "O método `TextRetrieval.find_best_matches` recebe um `user_input` e um dataframe contendo apenas as entradas correspondentes ao estado/`uf` daquele input e retorna os `k` `razaosocial` e `nome_fantasia` mais similares utilizando a técnica de TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class SimilarityMetric(Enum):\n",
    "    TFIDF = \"tfidf\"\n",
    "\n",
    "\n",
    "class TextRetrieval:\n",
    "    \"\"\"\n",
    "    A class for text retrieval using similarity metrics.\n",
    "    Here we only have the TFIDF metric, because it is the\n",
    "    best one found for the solution\n",
    "    \"\"\"\n",
    "\n",
    "    _tfidf_cache = {}\n",
    "    \n",
    "    _df_by_uf = {\n",
    "        uf: group.drop_duplicates(subset=['razaosocial_cleaned', 'nome_fantasia_cleaned','cnpj',])\n",
    "        for uf, group in df.groupby('uf')\n",
    "    }\n",
    "    \n",
    "    # The values of this dictionary are tuples in which the index 1 holds\n",
    "    # a boolean that tells whether a metric is such that the higher \n",
    "    # its value the more similar the strings being compared are\n",
    "    _METRIC_CONFIG = {\n",
    "        SimilarityMetric.TFIDF: (None, True),  \n",
    "    }\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_tfidf_cache_key(df: pd.DataFrame) -> str:\n",
    "        return str(hash(pd.util.hash_pandas_object(df[[\"razaosocial_cleaned\", \"nome_fantasia_cleaned\"]], index=False).sum()))\n",
    "\n",
    "    @classmethod\n",
    "    def _get_tfidf_cache(cls, df: pd.DataFrame):\n",
    "        key = cls._get_tfidf_cache_key(df)\n",
    "        if key not in cls._tfidf_cache:\n",
    "            combined = (\n",
    "                df[\"razaosocial_cleaned\"].fillna('') + ' ' + df[\"nome_fantasia_cleaned\"].fillna('')\n",
    "            )\n",
    "            vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
    "            tfidf_matrix = vectorizer.fit_transform(combined)\n",
    "            cls._tfidf_cache[key] = (vectorizer, tfidf_matrix, combined)\n",
    "        return cls._tfidf_cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_topk_cnpjs_from_pairs(\n",
    "        df: pd.DataFrame,\n",
    "        razao_social_list: List[str],\n",
    "        nome_fantasia_list: List[str],\n",
    "        cnpj_col: str = \"cnpj\",\n",
    "        not_cleaned_razao_col: str = razaosocial_col,\n",
    "        not_cleaned_fantasia_col: str = nome_fantasia_col,\n",
    "        top_k: int = 5\n",
    "    ) -> List[str]:\n",
    "        assert len(razao_social_list) == len(nome_fantasia_list)\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for razao, fantasia in zip(razao_social_list, nome_fantasia_list):\n",
    "            mask |= ((df[not_cleaned_razao_col] == razao) & (df[not_cleaned_fantasia_col] == fantasia))\n",
    "        cnpjs = df[mask][cnpj_col].dropna().tolist()\n",
    "        most_common = Counter(cnpjs).most_common(top_k)\n",
    "        return [cnpj for cnpj, _ in most_common]\n",
    "\n",
    "   \n",
    "\n",
    "    @classmethod\n",
    "    def _find_matches_tfidf(\n",
    "        cls,\n",
    "        user_input: str,\n",
    "        uf_df: pd.DataFrame,\n",
    "        top_k: int,\n",
    "        not_cleaned_razao_col: str = razaosocial_col,\n",
    "        not_cleaned_fantasia_col: str = nome_fantasia_col,\n",
    "        cnpj_col: str = \"cnpj\"\n",
    "    ) -> Tuple[List[str], List[str], List[str]]:\n",
    "\n",
    "        # If the tfidf matrix has already been done for the companies\n",
    "        # of a specific statem then we get it from the cache\n",
    "        vectorizer, tfidf_matrix, _ = cls._get_tfidf_cache(uf_df)\n",
    "        query_vec = vectorizer.transform([user_input])\n",
    "        similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        best_razao_matches = df.iloc[top_indices][not_cleaned_razao_col].tolist()\n",
    "        best_nome_fantasia_matches = df.iloc[top_indices][not_cleaned_fantasia_col].tolist()\n",
    "        best_cnpj_matches =  df.iloc[top_indices][cnpj_col].tolist()\n",
    "\n",
    "        return (\n",
    "            best_razao_matches,\n",
    "            best_nome_fantasia_matches,\n",
    "            best_cnpj_matches,\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def find_best_matches(\n",
    "        cls,\n",
    "        user_input: str,\n",
    "        uf: str, \n",
    "        metric: SimilarityMetric = SimilarityMetric.TFIDF, \n",
    "        top_k: int = 1\n",
    "    ) -> Tuple[List[str], List[str], List[float], List[float], List[str]]:\n",
    "\n",
    "\n",
    "        df_uf = TextRetrieval._df_by_uf.get(uf)\n",
    "\n",
    "        if metric == SimilarityMetric.TFIDF:\n",
    "            return cls._find_matches_tfidf(user_input, df_uf, top_k)\n",
    "\n",
    "        if metric not in cls._METRIC_CONFIG:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Recuperando a `razaosocial` e `nome_fantasia` dado um `user_input` e um `uf`\n",
    "\n",
    "> O método `TextRetrieval.find_best_matches` recebe um `user_input`, um `uf` e retorna uma tupla contendo uma lista com os top k `razaosocial`, uma com os `nome_fantasia` e outra com os top k `cnpj`, respectivamente. Ela também recebe uma métrica a ser utilizada, que por padrão foi decidida ser a `SimilarityMetric.TFIDF`.\n",
    "\n",
    "A funcao `evaluate_matching` recebe o dataframe de testes e gera um dataframe que pode ser utilizado para gerar as métricas de performance. Por exemplo, pode-se comparar as colunas \"top_1_razaosocial_pred\" e \"razaosocial_not_cleaned\" para verificar a acurácia da `razaosocial` retornada pelo retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_matching(df_sample: pd.DataFrame,\n",
    "                      df_cleaned: pd.DataFrame,\n",
    "                      metric: SimilarityMetric,\n",
    "                      top_k: int =5):\n",
    "    results = []\n",
    "\n",
    "    df_by_uf = {\n",
    "        uf: group.drop_duplicates(subset=['razaosocial_cleaned', 'nome_fantasia_cleaned','cnpj',])\n",
    "        for uf, group in df_cleaned.groupby('uf')\n",
    "    }\n",
    "\n",
    "    indexes = []\n",
    "    results_dict = {}\n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Processing\"):\n",
    "        user_input = row['user_input_cleaned']\n",
    "        razaosocial = row['razaosocial_cleaned']\n",
    "        nome_fantasia = row['nome_fantasia_cleaned']\n",
    "        not_cleaned_user_input = row['user_input']\n",
    "        true_razaosocial = row['razaosocial']\n",
    "        true_nome_fantasia = row['nome_fantasia']\n",
    "        cnpj = row['cnpj']\n",
    "        uf = row['uf']\n",
    "\n",
    "        df_uf = df_by_uf.get(uf)\n",
    "        if df_uf is None or df_uf.empty:\n",
    "            continue  # Skip if no data for that UF\n",
    "\n",
    "        # One fast match call\n",
    "        top_k_razao, top_k_nome, top_k_cnpj = TextRetrieval.find_best_matches(\n",
    "            user_input, uf, metric, top_k=top_k\n",
    "        )\n",
    "\n",
    "        top_1_razao = top_k_razao[0] if top_k_razao else None\n",
    "        top_1_nome = top_k_nome[0] if top_k_nome else None\n",
    "        top_1_cnpj = top_k_cnpj[0] if top_k_cnpj else None\n",
    "\n",
    "        result_row = {\n",
    "            \"user_input\": user_input,\n",
    "            \"user_input_not_cleaned\": not_cleaned_user_input,\n",
    "            \"razaosocial_not_cleaned\": true_razaosocial,\n",
    "            \"nome_fantasia_not_cleaned\": true_nome_fantasia,\n",
    "            \"razaosocial\": razaosocial,\n",
    "            \"nome_fantasia\": nome_fantasia,\n",
    "            \"cnpj\": cnpj,\n",
    "            \"uf\": uf,\n",
    "            \n",
    "            \"top_1_razaosocial_retrieved\": top_1_razao,\n",
    "            \"top_1_nomefantasia_retrieved\": top_1_nome,\n",
    "            \"top_1_cnpj_retrieved\": top_1_cnpj,\n",
    "\n",
    "            \"top_1_razaosocial_pred\": top_1_razao == true_razaosocial if top_1_razao else False,\n",
    "            \"top_1_nomefantasia_pred\": top_1_nome == true_nome_fantasia if top_1_nome else False,\n",
    "            \"top_1_cnpj_pred\": top_1_cnpj == cnpj if top_1_cnpj else False,\n",
    "\n",
    "            \"top_5_razaosocial_pred\": true_razaosocial in top_k_razao,\n",
    "            \"top_5_nomefantasia_pred\": true_nome_fantasia in top_k_nome,\n",
    "            \"top_5_cnpj_pred\": cnpj in top_k_cnpj,\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "        results_dict[idx] = result_row\n",
    "        results_dict[idx]['top_k_razaosocial_pred'] = top_k_razao\n",
    "        results_dict[idx]['top_k_nomefantasia_pred'] = top_k_nome\n",
    "        results_dict[idx]['top_k_cnpj_pred'] = top_k_cnpj\n",
    "        \n",
    "\n",
    "        results.append(result_row)\n",
    "        indexes.append(idx)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=indexes)\n",
    "    return results_df, results_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▎ | 213391/255471 [10:51<02:08, 328.47it/s]"
     ]
    }
   ],
   "source": [
    "metric = SimilarityMetric.TFIDF\n",
    "results_df, results_dict = evaluate_matching(df, df, metric, top_k=5)\n",
    "print(\"=\"* 100)\n",
    "print(f\"\\nResultados da Avaliação para a métrica: {metric.name}\"\n",
    "      f\"\\nTamanho do DataFrame de Resultados: {results_df.shape}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Razão Social): {results_df['top_1_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Nome Fantasia): {results_df['top_1_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 CNPJ): {results_df['top_1_cnpj_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Razão Social): {results_df['top_5_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Nome Fantasia): {results_df['top_5_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 CNPJ): {results_df['top_5_cnpj_pred'].sum()}\"\n",
    "      f\"\\nAcuracias:\\n\"\n",
    "      f\"Top 1 Razão Social: {results_df['top_1_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 Nome Fantasia: {results_df['top_1_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 CNPJ: {results_df['top_1_cnpj_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Razão Social: {results_df['top_5_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Nome Fantasia: {results_df['top_5_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 CNPJ: {results_df['top_5_cnpj_pred'].mean() * 100:.2f}%\")\n",
    "print(\"=\"* 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Resultados para os Dados Fornecidos no Case\n",
    "\n",
    "## Visão Geral\n",
    "- **Métrica Utilizada**: TF-IDF\n",
    "- **Tamanho do Dataset**: 255.471 registros\n",
    "- **Campos Avaliados**: Razão Social, Nome Fantasia e CNPJ\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Resultados de Acurácia\n",
    "\n",
    "### Top 1 (Melhor Predição)\n",
    "| Campo | Acertos | Acurácia |\n",
    "|-------|---------|----------|\n",
    "| **Razão Social** | 222.557 | **87.12%** |\n",
    "| **Nome Fantasia** | 179.384 | **70.22%** |\n",
    "| **CNPJ** | 88.462 | **34.63%** |\n",
    "\n",
    "### Top 5 (Entre as 5 Melhores Predições)\n",
    "| Campo | Acertos | Acurácia |\n",
    "|-------|---------|----------|\n",
    "| **Razão Social** | 241.348 | **94.47%** |\n",
    "| **Nome Fantasia** | 216.024 | **84.56%** |\n",
    "| **CNPJ** | 140.312 | **54.92%** |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 255471/255471 [13:06<00:00, 324.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "Resultados da Avaliação para a métrica: TFIDF\n",
      "Tamanho do DataFrame de Resultados: (255471, 20)\n",
      "Número de Linhas com Predições Corretas (Top 1 Razão Social): 222557\n",
      "Número de Linhas com Predições Corretas (Top 1 Nome Fantasia): 179384\n",
      "Número de Linhas com Predições Corretas (Top 1 CNPJ): 88462\n",
      "Número de Linhas com Predições Corretas (Top 5 Razão Social): 241348\n",
      "Número de Linhas com Predições Corretas (Top 5 Nome Fantasia): 216024\n",
      "Número de Linhas com Predições Corretas (Top 5 CNPJ): 140312\n",
      "Acuracias:\n",
      "Top 1 Razão Social: 87.12%\n",
      "Top 1 Nome Fantasia: 70.22%\n",
      "Top 1 CNPJ: 34.63%\n",
      "Top 5 Razão Social: 94.47%\n",
      "Top 5 Nome Fantasia: 84.56%\n",
      "Top 5 CNPJ: 54.92%\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "metric = SimilarityMetric.TFIDF\n",
    "results_df, results_dict = evaluate_matching(df, df, metric, top_k=5)\n",
    "print(\"=\"* 100)\n",
    "print(f\"\\nResultados da Avaliação para a métrica: {metric.name}\"\n",
    "      f\"\\nTamanho do DataFrame de Resultados: {results_df.shape}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Razão Social): {results_df['top_1_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Nome Fantasia): {results_df['top_1_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 CNPJ): {results_df['top_1_cnpj_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Razão Social): {results_df['top_5_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Nome Fantasia): {results_df['top_5_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 CNPJ): {results_df['top_5_cnpj_pred'].sum()}\"\n",
    "      f\"\\nAcuracias:\\n\"\n",
    "      f\"Top 1 Razão Social: {results_df['top_1_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 Nome Fantasia: {results_df['top_1_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 CNPJ: {results_df['top_1_cnpj_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Razão Social: {results_df['top_5_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Nome Fantasia: {results_df['top_5_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 CNPJ: {results_df['top_5_cnpj_pred'].mean() * 100:.2f}%\")\n",
    "print(\"=\"* 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
