{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import Levenshtein\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Union, Set, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from functools import wraps\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JACCARD_MODE = \"ngram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregando os Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confs = yaml.safe_load(open(\"confs.yaml\"))\n",
    "predictors = confs[\"predictors\"] ### Importante! O cientista poderá usar apenas estas features para criar/aperfeiçoar o modelo\n",
    "text_target = confs[\"text_target\"]\n",
    "cols_to_keep = predictors + text_target + ['cnpj']\n",
    "df = pd.read_parquet(\"dados/train.parquet\")[cols_to_keep]\n",
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparando os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Presenca de Valores `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValores NaN por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de linhas com algum valor NaN: {df.isnull().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nenhuma das colunas do conjunto de dados fornecido possui algum valor `NaN`, nao será necessária a realizacao de nenhum tratamento desse tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Presenca de Linhas Duplicadas\n",
    "\n",
    "Linhas duplicadas em relacao a todas as colunas do conjunto de dados serao removidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Número de linhas duplicadas: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "print(f\"Tamanho do conjunto original: {df.shape}\")\n",
    "print(f\"Tamanho do conjunto após remocao de linhas duplicadas em relacao a todas as colunas: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Remocao de Termos Irrelevantes\n",
    "\n",
    "Colunas com o sufixo `_cleaned` serao criadas para as colunas `user_input`, `razaosocial` e `nome_fantasia`. Nelas, termos como \"S.A.\", \"LTDA\", \"LTDA.\", \"S/A\", \"S.A\", \"Ltda\", \"Ltda.\", \"S/A.\", \"S.A.\", \"S.A\", \"Ltda\" e \"Ltda\" serao removidos baseados na seguinte suposicao:\n",
    "\n",
    " > **Usuários nao tem o hábito de utilizar esses termos ao se referir a nomes de empresas, então elas não devem ser consideradas na busca/retrieval**\n",
    "\n",
    "\n",
    "Essa remocao é importante, pois ao se calcular a similaridade entre um `user_input` e/ou `razaosocial` e `nome_fantasia`, as métricas de similaridade seriam prejudicadas na ausencia de tais termos no `user_input`. \n",
    "\n",
    "Também serao removidas acentos e stopwords da língua portuguesa, sobretudo preposicoes. Tais termos, por nao informarem sobre empresas específicas, podem prejudicar a acurácia das buscas/retrieval..\n",
    "Além disso, passaremos tudo para letras minúsculas, para evitar problemas de case-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_text_cleaning(text, \n",
    "                               remove_accents=True,\n",
    "                               remove_stop_words=True, \n",
    "                               remove_company_suffixes=True,\n",
    "                               custom_stop_words=None,\n",
    "                               to_lowercase=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "    remove_accents (bool): Remove accents and normalize characters\n",
    "    remove_stop_words (bool): Remove Portuguese stop words\n",
    "    remove_company_suffixes (bool): Remove common company suffixes\n",
    "    custom_stop_words (set): Additional stop words to remove\n",
    "    to_lowercase (bool): Convert to lowercase\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    if remove_accents:\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = text.replace('ç', 'c').replace('Ç', 'C')\n",
    "    \n",
    "    if to_lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if remove_company_suffixes:\n",
    "        patterns_to_remove = [\n",
    "        r'\\bS\\.?A\\.?\\b',           # S.A, SA, S.A., SA.\n",
    "        r'\\bS/A\\.?\\b',             # S/A, S/A.\n",
    "        r'\\bLTDA\\.?\\b',            # LTDA, LTDA.\n",
    "        r'\\bLIMITADA\\b',           # LIMITADA\n",
    "        r'\\bCIA\\.?\\b',             # CIA, CIA.\n",
    "        r'\\bCOMPANHIA\\b',          # COMPANHIA\n",
    "        r'\\bEMPRESA\\b',            # EMPRESA\n",
    "        r'\\bCOMERCIO\\b',           # COMERCIO\n",
    "        r'\\bSERVICOS?\\b',          # SERVICO, SERVICOS\n",
    "        r'\\bME\\b',                 # ME (Microempresa)\n",
    "        r'\\bEPP\\b',                # EPP (Empresa de Pequeno Porte)\n",
    "        r'\\bEIRELI\\b',             # EIRELI\n",
    "        r'\\bSOCIEDADE\\b',          # SOCIEDADE\n",
    "        r'ADMINISTRADORA\\b',       # ADMINISTRADORA\n",
    "        r'GERAL\\b',                # GERAL\n",
    "    ]\n",
    "        \n",
    "        for pattern in patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    if remove_stop_words:\n",
    "        portuguese_stop_words = {\n",
    "            'a', 'ao', 'aos', 'as', 'da', 'das', 'de', 'do', 'dos', 'e', 'em', 'na', \n",
    "            'nas', 'no', 'nos', 'o', 'os', 'para', 'por', 'com', 'um', 'uma', 'uns', \n",
    "            'umas', 'se', 'que', 'ou', 'mas', 'como', 'mais', 'muito', 'sua', 'seu',\n",
    "            'seus', 'suas', 'este', 'esta', 'estes', 'estas', 'esse', 'essa', 'esses',\n",
    "            'essas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'isso', 'aquilo'\n",
    "        }\n",
    "        \n",
    "        if custom_stop_words:\n",
    "            portuguese_stop_words.update(custom_stop_words)\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.lower() not in portuguese_stop_words]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Multiple spaces to single space\n",
    "    text = text.strip()                   # Remove leading/trailing spaces\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['user_input_cleaned'] = df['user_input'].apply(comprehensive_text_cleaning)\n",
    "df_cleaned['razaosocial_cleaned'] = df['razaosocial'].apply(comprehensive_text_cleaning)\n",
    "df_cleaned['nome_fantasia_cleaned'] = df['nome_fantasia'].apply(comprehensive_text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Criacao de Retriever Utilizando Métodos Clássicos\n",
    "\n",
    "Para escolher a métrica a ser utilizada pelo retriever, consideraremos as seguintes métricas de similaridade e erro:\n",
    "\n",
    "***\n",
    "> **Word Error Rate (WER)**: fórmula para calcular a taxa de erro a nível de palavras: \n",
    "  $$WER = \\frac{S + D + I}{N}$$\n",
    "  onde:\n",
    "  - $S$ é o número de substituições. Por exemplo, se o usuário digitou \"Empresa X\" e a referência é \"Empresa Y\", então há uma substituição.\n",
    "  - $D$ é o número de deleções. Por exemplo, se o usuário digitou \"Empresa\" e a referência é \"Empresa X\", então há uma deleção.\n",
    "  - $I$ é o número de inserções. Por exemplo, se o usuário digitou \"Empresa X Y\" e a referência é \"Empresa X\", então há uma inserção.\n",
    "  - $N$ é o número total de palavras na referência. Por exemplo, se a referência é \"Empresa X\", então $N$ é 2.\n",
    "\n",
    "***\n",
    "> **Character Error Rate (CER)**: fórmula para calcular a taxa de erro a nível de caracteres:\n",
    "  $$CER = \\frac{S + D + I}{N}$$\n",
    "  onde:\n",
    "  - $S$ é o número de substituições. Por exemplo, se o usuário digitou \"EmpresaXY\" e a referência é \"EmpresaXZ\", então há uma substituição.\n",
    "  - $D$ é o número de deleções. Por exemplo, se o usuário digitou \"Empresa\" e a referência é \"EmpresaX\", então há uma deleção.\n",
    "  - $I$ é o número de inserções. Por exemplo, se o usuário digitou \"Empresa XY\" e a referência é \"Empresa X\", então há uma inserção.\n",
    "  - $N$ é o número total de caracteres na referência. Por exemplo, se a referência é \"Empresa X\", então $N$ é 9 (contando espaços).\n",
    "\n",
    "***\n",
    "> **Distância de Levenshtein Normalizada**: é uma métrica que mede a diferença entre duas sequências de caracteres. É definida como o número mínimo de operações de edição (inserções, deleções ou substituições) necessárias para transformar uma sequência em outra. A normalização é feita dividindo a distância pelo comprimento da sequência de referência:\n",
    "  $$D(A, B) = \\frac{L(A, B)}{max(|A|, |B|)}$$\n",
    "  onde $L(A, B)$ é a distância de Levenshtein entre as sequências $A$ e $B$, e $|A|$ e $|B|$ são os comprimentos das sequências.\n",
    "\n",
    "  Por exemplo, se temos o texto \"empresa X\" e \"empresa Y\", a distância de Levenshtein seria 1 (substituindo \"X\" por \"Y\"). A normalização seria:\n",
    "  $$D(\\text{\"empresa X\"}, \\text{\"empresa Y\"}) = \\frac{1}{9}$$\n",
    "\n",
    "***\n",
    "> **Similaridade de Jaccard**: é uma métrica que mede a similaridade entre dois conjuntos. É definida como o tamanho da interseção dividido pelo tamanho da união dos conjuntos.\n",
    "  $$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "  No caso de textos, podemos considerar conjuntos de palavras ou caracteres. Por exemplo, se temos dois textos \"empresa X\" e \"empresa Y\", podemos considerar os conjuntos de palavras {empresa, X} e {empresa, Y}. A similaridade de Jaccard seria calculada como:\n",
    "  $$J(\\{empresa, X\\}, \\{empresa, Y\\}) = \\frac{|\\{empresa\\}|}{|\\{empresa, X, Y\\}|} = \\frac{1}{3}$$\n",
    "\n",
    "***\n",
    "> **Similaridade de Jaccard N Gram**: é uma extensão da similaridade de Jaccard que considera n-gramas (sequências de n itens contíguos) em vez de palavras ou caracteres individuais. É útil para capturar similaridades em sequências mais longas, como frases ou sentenças. \n",
    "  A fórmula é semelhante à similaridade de Jaccard, mas aplicada a n-gramas:\n",
    "  $$J(A, B) = \\frac{|A_n \\cap B_n|}{|A_n \\cup B_n|}$$\n",
    "\n",
    "No caso de textos, se temos o texto \"empresa X\", seus dois-gramas seriam {\"em\", \"mp\", \"pr\", \"re\", \"sa\", \"a \", \" X\"}. Seus três-gramas seriam {\"emp\", \"mpr\", \"pre\", \"res\", \"esa\", \"sa \", \"a X\"} e assim por diante. A similaridade de Jaccard N Gram seria calculada considerando esses n-gramas. Ela é útil para capturar similaridades em casos de comparação de textos com erros de digitação e com pequenas variações, como \"empresa X\" e \"emprsa X\", onde a ordem dos caracteres é preservada, mas algumas letras estão fora de lugar.\n",
    "\n",
    "***\n",
    "> **Similaridade de Jaccard Multi N Gram**: é uma extensão da similaridade de Jaccard N Gram que considera múltiplos tamanhos de n-gramas. A métrica final é calculada como a média ponderada das similaridades de Jaccard para diferentes tamanhos de n-gramas. Isso permite capturar similaridades em diferentes níveis de granularidade, desde caracteres individuais até sequências mais longas.\n",
    "***\n",
    "\n",
    "Essas métricas serao calculadas entre o `user_input_cleaned` e as colunas `razaosocial_cleaned` e `nome_fantasia_cleaned`. A métrica escolhida será a que apresentar o menor valor de erro, ou seja, a maior similaridade entre os textos, indicando a melhor correspondência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implementação das Métricas de Similaridade e Erro\n",
    "\n",
    "Vamos usar a implementação do pacote já importado `jiwer` para calcular CER e WER. Para a Distância de Levenshtein, vamos usar a função `distance` do pacote `Levenshtein`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMetrics:\n",
    "    \"\"\"\n",
    "    A class containing various text comparison metrics with input validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_cer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Character Error Rate.\"\"\"\n",
    "        return cer(reference, hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_wer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Word Error Rate.\"\"\"\n",
    "        return wer(reference, hypothesis)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_normalized_levenshtein(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate normalized Levenshtein distance (0-1).\n",
    "        Returns:\n",
    "            float: Normalized Levenshtein distance between 0 and 1\n",
    "        \"\"\"\n",
    "        max_len = max(len(reference), len(hypothesis))\n",
    "        if max_len == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        distance = Levenshtein.distance(reference, hypothesis)\n",
    "        return distance / max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_character_set(text: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Convert text to a set of characters.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Set[str]: Set of characters from the input text\n",
    "        \"\"\"\n",
    "        return set(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_word_set(text: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Convert text to a set of words.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            Set[str]: Set of words from the input text\n",
    "        \"\"\"\n",
    "        return set(text.lower().split())\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity between two sets.\n",
    "        \n",
    "        Args:\n",
    "            set1 (Set[str]): First set\n",
    "            set2 (Set[str]): Second set\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        if not set1 and not set2:  # Both sets are empty\n",
    "            return 1.0\n",
    "        if not set1 or not set2:   # One set is empty\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_jaccard_similarity_chars(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity based on character sets.\n",
    "        \n",
    "        Args:\n",
    "            reference (str): Reference text\n",
    "            hypothesis (str): Hypothesis text\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        ref_chars = TextMetrics._get_character_set(reference)\n",
    "        hyp_chars = TextMetrics._get_character_set(hypothesis)\n",
    "        return TextMetrics._calculate_jaccard_similarity(ref_chars, hyp_chars)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_jaccard_similarity_words(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity based on word sets.\n",
    "        \n",
    "        Args:\n",
    "            reference (str): Reference text\n",
    "            hypothesis (str): Hypothesis text\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        ref_words = TextMetrics._get_word_set(reference)\n",
    "        hyp_words = TextMetrics._get_word_set(hypothesis)\n",
    "        return TextMetrics._calculate_jaccard_similarity(ref_words, hyp_words)\n",
    "\n",
    "    @staticmethod \n",
    "    def _ngram_jaccard_similarity(reference: str, hypothesis: str, n=2):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity using character n-grams.\n",
    "        This handles inversions and some misspellings well.\n",
    "        \n",
    "        Args:\n",
    "            str1, str2: Input strings\n",
    "            n: N-gram size (2=bigrams, 3=trigrams, etc.)\n",
    "        \"\"\"\n",
    "        def get_ngrams(text, n):\n",
    "            \"\"\"Generate n-grams from text with padding.\"\"\"\n",
    "            # Add padding to capture beginning/end patterns\n",
    "            padded = '#' * (n-1) + text.lower() + '#' * (n-1)\n",
    "            return set(padded[i:i+n] for i in range(len(padded) - n + 1))\n",
    "        \n",
    "        ngrams1 = get_ngrams(reference, n)\n",
    "        ngrams2 = get_ngrams(hypothesis, n)\n",
    "        \n",
    "        intersection = len(ngrams1 & ngrams2)\n",
    "        union = len(ngrams1 | ngrams2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 1.0 if len(reference) == len(hypothesis) == 0 else 0.0\n",
    " \n",
    "    @staticmethod\n",
    "    def multi_ngram_jaccard_similarity(reference: str, hypothesis: str, ngram_sizes=[2, 3], weights=None):\n",
    "        \"\"\"\n",
    "        Combine multiple n-gram sizes for better robustness.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0] * len(ngram_sizes)\n",
    "        \n",
    "        if len(weights) != len(ngram_sizes):\n",
    "            raise ValueError(\"Number of weights must match number of n-gram sizes\")\n",
    "        \n",
    "        total_score = 0\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        for size, weight in zip(ngram_sizes, weights):\n",
    "            score = TextMetrics._ngram_jaccard_similarity(reference, hypothesis, size)\n",
    "            total_score += score * weight\n",
    "        \n",
    "        return total_score / total_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementação do Retriever\n",
    "\n",
    "O retriever será implementado na classe `TextRetriever`.\n",
    "\n",
    "O método `TextRetrieval.find_best_matches` recebe um `user_input` e retorna os `k` `razaosocial` e `nome_fantasia` mais similares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class SimilarityMetric(Enum):\n",
    "    \"\"\"Enum for available similarity metrics.\"\"\"\n",
    "    CER = \"cer\"\n",
    "    WER = \"wer\"\n",
    "    LEVENSHTEIN = \"levenshtein\"\n",
    "    JACCARD_CHARS = \"jaccard_chars\"\n",
    "    JACCARD_WORDS = \"jaccard_words\"\n",
    "    NGRAM_JACCARD = \"ngram_jaccard\"\n",
    "    TFIDF = \"tfidf\"\n",
    "\n",
    "\n",
    "class TextRetrieval:\n",
    "    \"\"\"\n",
    "    A modular class for text retrieval using similarity metrics.\n",
    "    Includes TF-IDF support with internal caching for fast repeated queries.\n",
    "    \"\"\"\n",
    "\n",
    "    _tfidf_cache = {}\n",
    "\n",
    "    _METRIC_CONFIG = {\n",
    "        SimilarityMetric.CER: (TextMetrics.calculate_cer, False),\n",
    "        SimilarityMetric.WER: (TextMetrics.calculate_wer, False),\n",
    "        SimilarityMetric.LEVENSHTEIN: (TextMetrics.calculate_normalized_levenshtein, False),\n",
    "        SimilarityMetric.JACCARD_CHARS: (TextMetrics.calculate_jaccard_similarity_chars, True),\n",
    "        SimilarityMetric.JACCARD_WORDS: (TextMetrics.calculate_jaccard_similarity_words, True),\n",
    "        SimilarityMetric.NGRAM_JACCARD: (\n",
    "            lambda text1, text2: TextMetrics.multi_ngram_jaccard_similarity(\n",
    "                text1, text2, ngram_sizes=[2, 3], weights=[0.5, 0.5]\n",
    "            ), \n",
    "            True\n",
    "        ),\n",
    "        SimilarityMetric.TFIDF: (None, True),  # handled separately\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_tfidf_cache_key(df: pd.DataFrame) -> str:\n",
    "        return str(hash(pd.util.hash_pandas_object(df[[\"razaosocial_cleaned\", \"nome_fantasia_cleaned\"]], index=False).sum()))\n",
    "\n",
    "    @classmethod\n",
    "    def _get_tfidf_cache(cls, df: pd.DataFrame):\n",
    "        key = cls._get_tfidf_cache_key(df)\n",
    "        if key not in cls._tfidf_cache:\n",
    "            combined = (\n",
    "                df[\"razaosocial_cleaned\"].fillna('') + ' ' + df[\"nome_fantasia_cleaned\"].fillna('')\n",
    "            )\n",
    "            vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
    "            tfidf_matrix = vectorizer.fit_transform(combined)\n",
    "            cls._tfidf_cache[key] = (vectorizer, tfidf_matrix, combined)\n",
    "        return cls._tfidf_cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _retrieve_topk_cnpjs_from_pairs(\n",
    "        df: pd.DataFrame,\n",
    "        razao_social_list: List[str],\n",
    "        nome_fantasia_list: List[str],\n",
    "        cnpj_col: str = \"cnpj\",\n",
    "        not_cleaned_razao_col: str = \"razaosocial\",\n",
    "        not_cleaned_fantasia_col: str = \"nome_fantasia\",\n",
    "        top_k: int = 5\n",
    "    ) -> List[str]:\n",
    "        assert len(razao_social_list) == len(nome_fantasia_list)\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for razao, fantasia in zip(razao_social_list, nome_fantasia_list):\n",
    "            mask |= ((df[not_cleaned_razao_col] == razao) & (df[not_cleaned_fantasia_col] == fantasia))\n",
    "        cnpjs = df[mask][cnpj_col].dropna().tolist()\n",
    "        most_common = Counter(cnpjs).most_common(top_k)\n",
    "        return [cnpj for cnpj, _ in most_common]\n",
    "\n",
    "    @classmethod\n",
    "    def find_best_matches(\n",
    "        cls,\n",
    "        user_input: str, \n",
    "        df: pd.DataFrame, \n",
    "        metric: SimilarityMetric, \n",
    "        top_k: int = 1\n",
    "    ) -> Tuple[List[str], List[str], List[float], List[float], List[str]]:\n",
    "\n",
    "        if metric == SimilarityMetric.TFIDF:\n",
    "            return cls._find_matches_tfidf(user_input, df, top_k)\n",
    "\n",
    "        if metric not in cls._METRIC_CONFIG:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "        \n",
    "        metric_func, reverse_sort = cls._METRIC_CONFIG[metric]\n",
    "\n",
    "        return cls._find_matches_with_metric(\n",
    "            user_input, df, metric_func, reverse_sort, top_k\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _find_matches_tfidf(\n",
    "        cls,\n",
    "        user_input: str,\n",
    "        df: pd.DataFrame,\n",
    "        top_k: int,\n",
    "        not_cleaned_razao_col: str = \"razaosocial\",\n",
    "        not_cleaned_fantasia_col: str = \"nome_fantasia\",\n",
    "        cnpj_col: str = \"cnpj\"\n",
    "    ) -> Tuple[List[str], List[str], List[str]]:\n",
    "\n",
    "        vectorizer, tfidf_matrix, combined_col = cls._get_tfidf_cache(df)\n",
    "        query_vec = vectorizer.transform([user_input])\n",
    "        similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        best_razao_matches = df.iloc[top_indices][not_cleaned_razao_col].tolist()\n",
    "        best_nome_fantasia_matches = df.iloc[top_indices][not_cleaned_fantasia_col].tolist()\n",
    "        best_cnpj_matches =  df.iloc[top_indices][cnpj_col].tolist()\n",
    "\n",
    "        return (\n",
    "            best_razao_matches,\n",
    "            best_nome_fantasia_matches,\n",
    "            best_cnpj_matches,\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def _find_matches_with_metric(\n",
    "        cls,\n",
    "        user_input: str,\n",
    "        df: pd.DataFrame,\n",
    "        metric_func: Callable[[str, str], float],\n",
    "        reverse_sort: bool,\n",
    "        top_k: int,\n",
    "        razao_col: str = \"razaosocial_cleaned\",\n",
    "        fantasia_col: str = \"nome_fantasia_cleaned\"\n",
    "    ) -> Tuple[List[str], List[str], List[float], List[float], List[str]]:\n",
    "\n",
    "        valid_mask = ~(df[razao_col].isna() & df[fantasia_col].isna())\n",
    "        if not valid_mask.any():\n",
    "            return [], [], [], [], []\n",
    "\n",
    "        df_valid = df[valid_mask].copy()\n",
    "\n",
    "        razao_scores = df_valid[razao_col].apply(\n",
    "            lambda x: metric_func(x, user_input) if pd.notna(x) else None\n",
    "        )\n",
    "        nome_scores = df_valid[fantasia_col].apply(\n",
    "            lambda x: metric_func(x, user_input) if pd.notna(x) else None\n",
    "        )\n",
    "\n",
    "        if reverse_sort:\n",
    "            max_scores = np.maximum(\n",
    "                razao_scores.fillna(-np.inf), \n",
    "                nome_scores.fillna(-np.inf)\n",
    "            )\n",
    "        else:\n",
    "            max_scores = np.minimum(\n",
    "                razao_scores.fillna(np.inf), \n",
    "                nome_scores.fillna(np.inf)\n",
    "            )\n",
    "\n",
    "        sorted_indices = np.argsort(max_scores)\n",
    "        if reverse_sort:\n",
    "            sorted_indices = sorted_indices[::-1]\n",
    "\n",
    "        top_indices = sorted_indices[:top_k]\n",
    "        best_razao_matches = df_valid.iloc[top_indices][razao_col].tolist()\n",
    "        best_nome_fantasia_matches = df_valid.iloc[top_indices][fantasia_col].tolist()\n",
    "        best_razao_scores = razao_scores.iloc[top_indices].tolist()\n",
    "        best_nome_fantasia_scores = nome_scores.iloc[top_indices].tolist()\n",
    "        best_cnpj_matches = cls._retrieve_topk_cnpjs_from_pairs(\n",
    "            df_valid,\n",
    "            best_razao_matches,\n",
    "            best_nome_fantasia_matches,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        return best_razao_matches, best_nome_fantasia_matches, best_razao_scores, best_nome_fantasia_scores, best_cnpj_matches\n",
    "    \n",
    "    # All metric-specific methods now return the same 4-element tuple\n",
    "    @staticmethod\n",
    "    def find_best_matches_cer(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using Character Error Rate (CER).\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.CER, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_wer(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using Word Error Rate (WER).\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.WER, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_levenshtein(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using normalized Levenshtein distance.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.LEVENSHTEIN, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_jaccard_chars(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using character-level Jaccard similarity.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.JACCARD_CHARS, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_jaccard_words(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using word-level Jaccard similarity.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.JACCARD_WORDS, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_ngram_jaccard(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        \"\"\"Find the top-k best matches using n-gram Jaccard similarity.\"\"\"\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.NGRAM_JACCARD, top_k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_matches_tfidf(user_input: str, df: pd.DataFrame, top_k: int = 1) -> Tuple[List[str], List[str], List[float], List[float]]:\n",
    "        return TextRetrieval.find_best_matches(user_input, df, SimilarityMetric.TFIDF, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Escolha da Melhor Métrica\n",
    "\n",
    "\n",
    "Serao sorteadas `1000` linhas do conjunto de dados para calcular as métricas de similaridade e erro. Para cada linha, vamos calcular todas as métricas mencionadas acima entre o `user_input_cleaned` e as colunas `razaosocial_cleaned` e `nome_fantasia_cleaned`. A métrica escolhida será aquela que apresentar o menor valor de erro para os `cnpj`s correspondentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_matching_fast(df_sample, df_cleaned, top_k=5):\n",
    "    results = []\n",
    "\n",
    "    # Pre-group df_cleaned by UF for faster access\n",
    "    df_by_uf = {\n",
    "        uf: group.drop_duplicates(subset=['razaosocial_cleaned', 'nome_fantasia_cleaned','cnpj',])\n",
    "        for uf, group in df_cleaned.groupby('uf')\n",
    "    }\n",
    "\n",
    "    indexes = []\n",
    "    results_dict = {}\n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Processing\"):\n",
    "        user_input = row['user_input_cleaned']\n",
    "        razaosocial = row['razaosocial_cleaned']\n",
    "        nome_fantasia = row['nome_fantasia_cleaned']\n",
    "        not_cleaned_user_input = row['user_input']\n",
    "        true_razaosocial = row['razaosocial']\n",
    "        true_nome_fantasia = row['nome_fantasia']\n",
    "        cnpj = row['cnpj']\n",
    "        uf = row['uf']\n",
    "\n",
    "        df_uf = df_by_uf.get(uf)\n",
    "        if df_uf is None or df_uf.empty:\n",
    "            continue  # Skip if no data for that UF\n",
    "\n",
    "        # One fast match call\n",
    "        top_k_razao, top_k_nome, top_k_cnpj = TextRetrieval.find_best_matches(\n",
    "            user_input, df_uf, SimilarityMetric.TFIDF, top_k=top_k\n",
    "        )\n",
    "\n",
    "        top_1_razao = top_k_razao[0] if top_k_razao else None\n",
    "        top_1_nome = top_k_nome[0] if top_k_nome else None\n",
    "        top_1_cnpj = top_k_cnpj[0] if top_k_cnpj else None\n",
    "\n",
    "        result_row = {\n",
    "            \"user_input\": user_input,\n",
    "            \"user_input_not_cleaned\": not_cleaned_user_input,\n",
    "            \"razaosocial_not_cleaned\": true_razaosocial,\n",
    "            \"nome_fantasia_not_cleaned\": true_nome_fantasia,\n",
    "            \"razaosocial\": razaosocial,\n",
    "            \"nome_fantasia\": nome_fantasia,\n",
    "            \"cnpj\": cnpj,\n",
    "            \"uf\": uf,\n",
    "            \n",
    "            \"top_1_razaosocial_retrieved\": top_1_razao,\n",
    "            \"top_1_nomefantasia_retrieved\": top_1_nome,\n",
    "            \"top_1_cnpj_retrieved\": top_1_cnpj,\n",
    "\n",
    "            \"top_1_razaosocial_pred\": top_1_razao == true_razaosocial if top_1_razao else False,\n",
    "            \"top_1_nomefantasia_pred\": top_1_nome == true_nome_fantasia if top_1_nome else False,\n",
    "            \"top_1_cnpj_pred\": top_1_cnpj == cnpj if top_1_cnpj else False,\n",
    "\n",
    "            \"top_5_razaosocial_pred\": true_razaosocial in top_k_razao,\n",
    "            \"top_5_nomefantasia_pred\": true_nome_fantasia in top_k_nome,\n",
    "            \"top_5_cnpj_pred\": cnpj in top_k_cnpj,\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "        # Store results in a dictionary for easy access\n",
    "        results_dict[idx] = result_row\n",
    "        results_dict[idx]['top_k_razaosocial_pred'] = top_k_razao\n",
    "        results_dict[idx]['top_k_nomefantasia_pred'] = top_k_nome\n",
    "        results_dict[idx]['top_k_cnpj_pred'] = top_k_cnpj\n",
    "        \n",
    "\n",
    "        results.append(result_row)\n",
    "        indexes.append(idx)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=indexes)\n",
    "    return results_df, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000 \n",
    "top_k = 5\n",
    "if sample_size:\n",
    "    df_sample = df_cleaned.sample(n=sample_size, random_state=42)\n",
    "else:\n",
    "    df_sample = df_cleaned\n",
    "results_df, results_dict = evaluate_matching_fast(df_sample, df_cleaned, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"tf-idf-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados da Avaliação:\n",
      "Tamanho do DataFrame de Resultados: (255065, 20)\n",
      "Número de Linhas com Predições Corretas (Top 1 Razão Social): 222172\n",
      "Número de Linhas com Predições Corretas (Top 1 Nome Fantasia): 179033\n",
      "Número de Linhas com Predições Corretas (Top 1 CNPJ): 88325\n",
      "Número de Linhas com Predições Corretas (Top 5 Razão Social): 240951\n",
      "Número de Linhas com Predições Corretas (Top 5 Nome Fantasia): 215641\n",
      "Número de Linhas com Predições Corretas (Top 5 CNPJ): 140089\n",
      "Acuracias:\n",
      "Top 1 Razão Social: 87.10%\n",
      "Top 1 Nome Fantasia: 70.19%\n",
      "Top 1 CNPJ: 34.63%\n",
      "Top 5 Razão Social: 94.47%\n",
      "Top 5 Nome Fantasia: 84.54%\n",
      "Top 5 CNPJ: 54.92%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados da Avaliação:\"\n",
    "      f\"\\nTamanho do DataFrame de Resultados: {results_df.shape}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Razão Social): {results_df['top_1_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Nome Fantasia): {results_df['top_1_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 CNPJ): {results_df['top_1_cnpj_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Razão Social): {results_df['top_5_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Nome Fantasia): {results_df['top_5_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 CNPJ): {results_df['top_5_cnpj_pred'].sum()}\"\n",
    "      f\"\\nAcuracias:\\n\"\n",
    "      f\"Top 1 Razão Social: {results_df['top_1_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 Nome Fantasia: {results_df['top_1_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 CNPJ: {results_df['top_1_cnpj_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Razão Social: {results_df['top_5_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Nome Fantasia: {results_df['top_5_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 CNPJ: {results_df['top_5_cnpj_pred'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"tf-idf-results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Análise dos Resultados\n",
    "\n",
    "Como observado acima, o `TextRetriever` implementado com o `TF-IDF` e `Cosine Similarity` apresentou os melhores resultados. Por outro lado, nenhuma das métricas obteve resultados satisfatórios para o `CNPJ` retornado no top 1 ou top 5, mesmo quando os resultados para a `razaosocial` e `nome_fantasia` foram satisfatórios, como é o caso do próprio `TF-IDF`. Isso indica que um mesmo par `razaosocial` e `nome_fantasia` pode ter diferentes `CNPJ`s associados mesmo no mesmo estado/`uf`, o que torna a tarefa de recuperação de `CNPJ` mais complexa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the CNPJ errors are distributed in relation to the other errors\n",
    "\n",
    "# Erro no Top 5 Razão Social e erro no Top 5 Nome Fantasia\n",
    "erro_top5_razao_nome = results_df[\n",
    "    (results_df['top_5_razaosocial_pred'] == False) & \n",
    "    (results_df['top_5_nomefantasia_pred'] == False) &\n",
    "    (results_df['top_5_cnpj_pred'] == False)\n",
    "]\n",
    "\n",
    "# Erro no Top 5 Razão Social e acerto no Top 5 Nome Fantasia\n",
    "erro_top5_razao_acerto_nome = results_df[\n",
    "    (results_df['top_5_razaosocial_pred'] == False) & \n",
    "    (results_df['top_5_nomefantasia_pred'] == True) &\n",
    "    (results_df['top_5_cnpj_pred'] == False)\n",
    "]\n",
    "# Acerto no Top 5 Razão Social e erro no Top 5 Nome Fantasia\n",
    "erro_top5_acerto_razao_nome = results_df[\n",
    "    (results_df['top_5_razaosocial_pred'] == True) & \n",
    "    (results_df['top_5_nomefantasia_pred'] == False) &\n",
    "    (results_df['top_5_cnpj_pred'] == False)\n",
    "]\n",
    "\n",
    "# Acerto no Top 5 Razão Social e Nome Fantasia\n",
    "acerto_top5_razao_nome = results_df[\n",
    "    (results_df['top_5_razaosocial_pred'] == True) & \n",
    "    (results_df['top_5_nomefantasia_pred'] == True) &\n",
    "    (results_df['top_5_cnpj_pred'] == False)\n",
    "]\n",
    "\n",
    "total_cnpj_errors = results_df[\n",
    "    (results_df['top_5_cnpj_pred'] == False)\n",
    "]\n",
    "print(\"\\nDistribuição dos Erros:\")\n",
    "print(f\"\\nErro no Top 5 Razão Social e Nome Fantasia: {len(erro_top5_razao_nome)/len(total_cnpj_errors) * 100:.2f}%\")\n",
    "print(f\"Erro no Top 5 Razão Social e acerto no Top 5 Nome Fantasia: {len(erro_top5_razao_acerto_nome)/len(total_cnpj_errors) * 100:.2f}%\")\n",
    "print(f\"Acerto no Top 5 Razão Social e erro no Top 5 Nome Fantasia: {len(erro_top5_acerto_razao_nome)/len(total_cnpj_errors) * 100:.2f}%\")\n",
    "print(f\"Acerto no Top 5 Razão Social e Nome Fantasia: {len(acerto_top5_razao_nome)/len(total_cnpj_errors) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como verificado acima, `64.77%` dos erros de CNPJ no top 5 ocorre em casos em que houve acerto de ambos `razaosocial` e `nome_fantasia`, mas o CNPJ retornado não é o correto. Isso ocorre provavelmente porque no conjunto de dados, em um mesmo estado (`uf`), há mais de uma empresa com os mesmos `razaosocial` e `nome_fantasia`. Verifiquemos se isso é verdade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick analysis with distribution of CNPJ counts\n",
    "cnpj_counts_per_pair = df.groupby(['uf', 'razaosocial', 'nome_fantasia'])['cnpj'].nunique()\n",
    "\n",
    "# Get total records per UF\n",
    "total_records_per_uf = df.groupby('uf').size()\n",
    "\n",
    "# Count pairs with multiple CNPJs per UF\n",
    "multiple_cnpjs_summary = (\n",
    "    cnpj_counts_per_pair[cnpj_counts_per_pair > 1]\n",
    "    .reset_index()\n",
    "    .groupby('uf')\n",
    "    .agg({\n",
    "        'cnpj': ['count', 'mean', 'max']\n",
    "    })\n",
    ")\n",
    "multiple_cnpjs_summary.columns = ['pairs_with_multiple_cnpjs', 'avg_cnpjs_per_problematic_pair', 'max_cnpjs_per_pair']\n",
    "\n",
    "# Add total records per UF\n",
    "multiple_cnpjs_summary['total_records'] = total_records_per_uf\n",
    "\n",
    "# Calculate percentage of records that belong to pairs with multiple CNPJs\n",
    "# First, get the number of records for each pair with multiple CNPJs\n",
    "records_in_multi_cnpj = (\n",
    "    df[df.groupby(['uf', 'razaosocial', 'nome_fantasia'])['cnpj'].transform('nunique') > 1]\n",
    "    .groupby('uf')\n",
    "    .size()\n",
    ")\n",
    "\n",
    "# Add this to our summary\n",
    "multiple_cnpjs_summary['records_in_multi_cnpj'] = records_in_multi_cnpj.reindex(multiple_cnpjs_summary.index, fill_value=0)\n",
    "\n",
    "# Calculate percentage\n",
    "multiple_cnpjs_summary['percentage_records_in_multi_cnpj'] = (\n",
    "    multiple_cnpjs_summary['records_in_multi_cnpj'] / \n",
    "    multiple_cnpjs_summary['total_records'] * 100\n",
    ").round(2)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "multiple_cnpjs_summary = multiple_cnpjs_summary[[\n",
    "    'pairs_with_multiple_cnpjs', \n",
    "    'percentage_records_in_multi_cnpj',\n",
    "    'records_in_multi_cnpj',\n",
    "    'total_records',\n",
    "    'avg_cnpjs_per_problematic_pair', \n",
    "    'max_cnpjs_per_pair'\n",
    "]].round(2).sort_values('pairs_with_multiple_cnpjs', ascending=False)\n",
    "\n",
    "print(\"Summary of pairs with multiple CNPJs per UF:\")\n",
    "multiple_cnpjs_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tabela acima revela que em todos os `uf` presentes no conjunto de dados, há um número significativo de empresas com o mesmo `razaosocial` e `nome_fantasia`, mas com `CNPJ`s diferentes. Isso confirma a suposição de que a tarefa de recuperação de `CNPJ` é mais complexa do que a simples recuperação de `razaosocial` e `nome_fantasia`. \n",
    "Uma estratégia para resolver esse problema seria retornar no `CNPJ` o mais frequente entre os `CNPJ`s associados ao mesmo `razaosocial` e `nome_fantasia`, ou seja, o `CNPJ` que mais aparece no conjunto de dados para aquele `razaosocial` e `nome_fantasia` em determinado `uf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Criacao de Retriever Utilizando Sentence Transformers\n",
    "\n",
    "Como a acurácia do retriever baseado em métodos clássicos não foi satisfatória, vamos implementar um retriever utilizando embeddings de sentenças. Serao gerados embeddings para as colunas `razaosocial_cleaned` e `nome_fantasia_cleaned`. Tais embeddings serão utilizados para calcular a similaridade entre o `user_input_cleaned` e as colunas mencionadas. A métrica de similaridade utilizada será a similaridade cosseno, que é uma métrica comum para medir a similaridade entre vetores de alta dimensão e é a mesma utilizada anteriormente para calcular a similaridade entre os vetores TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "\n",
    "class SemanticRetrieval:\n",
    "    def __init__(self, df: pd.DataFrame, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.df = df.copy()\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.embeddings = None\n",
    "        self.id_map = None  # maps index -> df index\n",
    "        self._prepare_index()\n",
    "\n",
    "    def _prepare_index(self):\n",
    "        # Combine cleaned fields for semantic search\n",
    "        self.df['razaosocial_cleaned'] = self.df['razaosocial'].apply(comprehensive_text_cleaning)\n",
    "        self.df['nome_fantasia_cleaned'] = self.df['nome_fantasia'].apply(comprehensive_text_cleaning)\n",
    "        self.df['combined'] = (\n",
    "            self.df['razaosocial_cleaned'].fillna('') + ' ' +\n",
    "            self.df['nome_fantasia_cleaned'].fillna('')\n",
    "        )\n",
    "\n",
    "        # Compute embeddings\n",
    "        self.embeddings = self.model.encode(self.df['combined'].tolist(), show_progress_bar=True)\n",
    "        self.embeddings = np.array(self.embeddings).astype('float32')\n",
    "\n",
    "        # Create FAISS index\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "        # Store mapping from index -> dataframe row\n",
    "        self.id_map = self.df.index.to_numpy()\n",
    "\n",
    "    def search(self, user_input: str, top_k: int = 5, uf: str = None) -> dict:\n",
    "        user_embedding = self.model.encode([user_input]).astype('float32')\n",
    "        distances, indices = self.index.search(user_embedding, top_k)\n",
    "\n",
    "        matched_rows = self.df.iloc[self.id_map[indices[0]]].copy()\n",
    "\n",
    "        if uf:\n",
    "            matched_rows = matched_rows[matched_rows['uf'] == uf]\n",
    "\n",
    "        cnpjs = matched_rows['cnpj'].dropna().tolist()\n",
    "        most_common_cnpjs = [cnpj for cnpj, _ in Counter(cnpjs).most_common(top_k)]\n",
    "\n",
    "        return {\n",
    "            'razaosocial': matched_rows['razaosocial'].tolist(),\n",
    "            'nome_fantasia': matched_rows['nome_fantasia'].tolist(),\n",
    "            'cnpjs': most_common_cnpjs,\n",
    "            'distances': distances[0].tolist()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_matching_fast_semantic(df_sample, df_cleaned, top_k=5):\n",
    "    results = []\n",
    "    indexes = []\n",
    "    results_dict = {}\n",
    "\n",
    "    # Group df_cleaned by UF and prebuild a SemanticRetrieval for each\n",
    "    retrievers_by_uf = {}\n",
    "    for uf, group in df_cleaned.groupby('uf'):\n",
    "        group = group.drop_duplicates(subset=['razaosocial_cleaned', 'nome_fantasia_cleaned', 'cnpj'])\n",
    "        group = group.reset_index(drop=False) \n",
    "        retrievers_by_uf[uf] = SemanticRetrieval(group)\n",
    "\n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Semantic Matching\"):\n",
    "        user_input = row['user_input_cleaned']\n",
    "        razaosocial = row['razaosocial_cleaned']\n",
    "        nome_fantasia = row['nome_fantasia_cleaned']\n",
    "        not_cleaned_user_input = row['user_input']\n",
    "        not_cleaned_razaosocial = row['razaosocial']\n",
    "        not_cleaned_nome_fantasia = row['nome_fantasia']\n",
    "        cnpj = row['cnpj']\n",
    "        uf = row['uf']\n",
    "\n",
    "        retriever = retrievers_by_uf.get(uf)\n",
    "        if retriever is None:\n",
    "            continue  # Skip if no retriever for that UF\n",
    "\n",
    "        result = retriever.search(user_input, top_k=top_k)\n",
    "\n",
    "        top_k_razao = result['razaosocial']\n",
    "        top_k_nome = result['nome_fantasia']\n",
    "        top_k_cnpj = result['cnpjs']\n",
    "\n",
    "        top_1_razao = top_k_razao[0] if top_k_razao else None\n",
    "        top_1_nome = top_k_nome[0] if top_k_nome else None\n",
    "        top_1_cnpj = top_k_cnpj[0] if top_k_cnpj else None\n",
    "\n",
    "        result_row = {\n",
    "            \"user_input\": user_input,\n",
    "            \"user_input_not_cleaned\": not_cleaned_user_input,\n",
    "            \"razaosocial_not_cleaned\": not_cleaned_razaosocial,\n",
    "            \"nome_fantasia_not_cleaned\": not_cleaned_nome_fantasia,\n",
    "            \"razaosocial\": razaosocial,\n",
    "            \"nome_fantasia\": nome_fantasia,\n",
    "            \"cnpj\": cnpj,\n",
    "            \"uf\": uf,\n",
    "            \n",
    "            \"top_1_razaosocial_retrieved\": top_1_razao,\n",
    "            \"top_1_nomefantasia_retrieved\": top_1_nome,\n",
    "            \"top_1_cnpj_retrieved\": top_1_cnpj,\n",
    "\n",
    "            \"top_1_razaosocial_pred\": top_1_razao == not_cleaned_razaosocial if top_1_razao else False,\n",
    "            \"top_1_nomefantasia_pred\": top_1_nome == not_cleaned_nome_fantasia if top_1_nome else False,\n",
    "            \"top_1_cnpj_pred\": top_1_cnpj == cnpj if top_1_cnpj else False,\n",
    "\n",
    "            \"top_5_razaosocial_pred\": not_cleaned_razaosocial in top_k_razao,\n",
    "            \"top_5_nomefantasia_pred\": not_cleaned_nome_fantasia in top_k_nome,\n",
    "            \"top_5_cnpj_pred\": cnpj in top_k_cnpj,\n",
    "        }\n",
    "\n",
    "        results_dict[idx] = result_row\n",
    "        results_dict[idx]['top_k_razaosocial_pred'] = top_k_razao\n",
    "        results_dict[idx]['top_k_nomefantasia_pred'] = top_k_nome\n",
    "        results_dict[idx]['top_k_cnpj_pred'] = top_k_cnpj\n",
    "\n",
    "        results.append(result_row)\n",
    "        indexes.append(idx)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=indexes)\n",
    "    return results_df, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  5.39it/s]\n",
      "Batches: 100%|██████████| 6/6 [00:04<00:00,  1.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.27it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:01<00:00, 13.21it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 15.94it/s]\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 10.57it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 10.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.24it/s]\n",
      "Batches: 100%|██████████| 18/18 [00:01<00:00, 15.80it/s]\n",
      "Batches: 100%|██████████| 10/10 [00:01<00:00,  6.08it/s]\n",
      "Batches: 100%|██████████| 44/44 [00:02<00:00, 21.38it/s]\n",
      "Batches: 100%|██████████| 11/11 [00:00<00:00, 14.77it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:01<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 13.17it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:01<00:00,  5.63it/s]\n",
      "Batches: 100%|██████████| 14/14 [00:01<00:00, 12.54it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 12.69it/s]\n",
      "Batches: 100%|██████████| 72/72 [00:02<00:00, 29.30it/s]\n",
      "Batches: 100%|██████████| 32/32 [00:01<00:00, 24.35it/s]\n",
      "Batches: 100%|██████████| 6/6 [00:00<00:00, 12.79it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.26it/s]\n",
      "Batches: 100%|██████████| 93/93 [00:03<00:00, 23.87it/s]\n",
      "Batches: 100%|██████████| 46/46 [00:01<00:00, 30.45it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 215/215 [00:04<00:00, 46.38it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  9.68it/s]\n",
      "Semantic Matching: 100%|██████████| 1000/1000 [00:14<00:00, 68.41it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "top_k = 5\n",
    "if sample_size:\n",
    "    df_sample = df_cleaned.sample(n=sample_size, random_state=42)\n",
    "else:\n",
    "    df_sample = df_cleaned\n",
    "results_df, results_dict = evaluate_matching_fast_semantic(df_sample, df_cleaned, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados da Avaliação:\n",
      "Tamanho do DataFrame de Resultados: (1000, 20)\n",
      "Número de Linhas com Predições Corretas (Top 1 Razão Social): 450\n",
      "Número de Linhas com Predições Corretas (Top 1 Nome Fantasia): 331\n",
      "Número de Linhas com Predições Corretas (Top 1 CNPJ): 158\n",
      "Número de Linhas com Predições Corretas (Top 5 Razão Social): 532\n",
      "Número de Linhas com Predições Corretas (Top 5 Nome Fantasia): 446\n",
      "Número de Linhas com Predições Corretas (Top 5 CNPJ): 281\n",
      "Acuracias:\n",
      "Top 1 Razão Social: 45.00%\n",
      "Top 1 Nome Fantasia: 33.10%\n",
      "Top 1 CNPJ: 15.80%\n",
      "Top 5 Razão Social: 53.20%\n",
      "Top 5 Nome Fantasia: 44.60%\n",
      "Top 5 CNPJ: 28.10%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados da Avaliação:\"\n",
    "      f\"\\nTamanho do DataFrame de Resultados: {results_df.shape}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Razão Social): {results_df['top_1_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 Nome Fantasia): {results_df['top_1_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 1 CNPJ): {results_df['top_1_cnpj_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Razão Social): {results_df['top_5_razaosocial_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 Nome Fantasia): {results_df['top_5_nomefantasia_pred'].sum()}\"\n",
    "      f\"\\nNúmero de Linhas com Predições Corretas (Top 5 CNPJ): {results_df['top_5_cnpj_pred'].sum()}\"\n",
    "      f\"\\nAcuracias:\\n\"\n",
    "      f\"Top 1 Razão Social: {results_df['top_1_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 Nome Fantasia: {results_df['top_1_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 1 CNPJ: {results_df['top_1_cnpj_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Razão Social: {results_df['top_5_razaosocial_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 Nome Fantasia: {results_df['top_5_nomefantasia_pred'].mean() * 100:.2f}%\"\n",
    "      f\"\\nTop 5 CNPJ: {results_df['top_5_cnpj_pred'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>user_input_not_cleaned</th>\n",
       "      <th>razaosocial_not_cleaned</th>\n",
       "      <th>nome_fantasia_not_cleaned</th>\n",
       "      <th>razaosocial</th>\n",
       "      <th>nome_fantasia</th>\n",
       "      <th>cnpj</th>\n",
       "      <th>uf</th>\n",
       "      <th>top_1_razaosocial_retrieved</th>\n",
       "      <th>top_1_nomefantasia_retrieved</th>\n",
       "      <th>top_1_cnpj_retrieved</th>\n",
       "      <th>top_1_razaosocial_pred</th>\n",
       "      <th>top_1_nomefantasia_pred</th>\n",
       "      <th>top_1_cnpj_pred</th>\n",
       "      <th>top_5_razaosocial_pred</th>\n",
       "      <th>top_5_nomefantasia_pred</th>\n",
       "      <th>top_5_cnpj_pred</th>\n",
       "      <th>top_k_razaosocial_pred</th>\n",
       "      <th>top_k_nomefantasia_pred</th>\n",
       "      <th>top_k_cnpj_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181538</th>\n",
       "      <td>deus franca</td>\n",
       "      <td>DEUS FRANCA</td>\n",
       "      <td>IGREJA EVANGELICA ASSEMBLEIA DE DEUS EM FRANCA-SP</td>\n",
       "      <td>ASSEMBLEIA DE DEUS</td>\n",
       "      <td>igreja evangelica assembleia deus franca sp</td>\n",
       "      <td>assembleia deus</td>\n",
       "      <td>47041223002376</td>\n",
       "      <td>SP</td>\n",
       "      <td>NOVA FRANCA PARTICIPACOES LTDA</td>\n",
       "      <td>NOVA FRANCA IMOBILIARIA</td>\n",
       "      <td>47998745000155</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[NOVA FRANCA PARTICIPACOES LTDA, CONFECCOES AL...</td>\n",
       "      <td>[NOVA FRANCA IMOBILIARIA, CONFECCOES ALVARO, E...</td>\n",
       "      <td>[47998745000155, 43306715000180, 6070119031004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36661</th>\n",
       "      <td>sao joao faramsia</td>\n",
       "      <td>SAO JOAO FARAMSIA</td>\n",
       "      <td>COMERCIO DE MEDICAMENTOS BRAIR LTDA</td>\n",
       "      <td>SAO JOAO FARMACIAS</td>\n",
       "      <td>medicamentos brair</td>\n",
       "      <td>sao joao farmacias</td>\n",
       "      <td>88212113030351</td>\n",
       "      <td>RS</td>\n",
       "      <td>ASSOCIACAO BRASILEIRA D'A IGREJA DE JESUS CRIS...</td>\n",
       "      <td>SAO FRANCISCO DE PAULA</td>\n",
       "      <td>61012019158366</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[ASSOCIACAO BRASILEIRA D'A IGREJA DE JESUS CRI...</td>\n",
       "      <td>[SAO FRANCISCO DE PAULA, SUPERMERCADO CARANGOL...</td>\n",
       "      <td>[61012019158366, 94739992000117, 6101201910940...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167507</th>\n",
       "      <td>bras i</td>\n",
       "      <td>BRAS DE A I</td>\n",
       "      <td>ASSOCIACAO BRASILEIRA D'A IGREJA DE JESUS CRIS...</td>\n",
       "      <td>ASSOC BRAS DE A I DE JESUS CRISTO DOS SANTOS D...</td>\n",
       "      <td>associacao brasileira d a igreja jesus cristo ...</td>\n",
       "      <td>assoc bras i jesus cristo santos u dias</td>\n",
       "      <td>61012019060986</td>\n",
       "      <td>BA</td>\n",
       "      <td>OTICAS FAM LTDA</td>\n",
       "      <td>OTICAS FAM</td>\n",
       "      <td>53796458000165</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[OTICAS FAM LTDA, BRASKEM S.A, RI HAPPY BRINQU...</td>\n",
       "      <td>[OTICAS FAM, BRASKEM, HAPPY, SALA DE VENDAS SE...</td>\n",
       "      <td>[53796458000165, 42150391003005, 5873166200518...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11988</th>\n",
       "      <td>kadrangular ebanjeio</td>\n",
       "      <td>KADRANGULAR EBANJEIO</td>\n",
       "      <td>IGREJA DO EVANGELHO QUADRANGULAR</td>\n",
       "      <td>CRUZADA NACIONAL DE EVANGELIZACAO</td>\n",
       "      <td>igreja evangelho quadrangular</td>\n",
       "      <td>cruzada nacional evangelizacao</td>\n",
       "      <td>62955505108952</td>\n",
       "      <td>AL</td>\n",
       "      <td>KASANOVA LTDA</td>\n",
       "      <td>KASANOVA</td>\n",
       "      <td>54045999000114</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[KASANOVA LTDA, MINISTERIO IGREJA VIVA, TECNOL...</td>\n",
       "      <td>[KASANOVA, MINISTERIO IGREJA VIVA, TECBAN, NUT...</td>\n",
       "      <td>[54045999000114, 49232475000100, 5142710202988...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60444</th>\n",
       "      <td>pavel</td>\n",
       "      <td>PAVEL</td>\n",
       "      <td>DIMED S/A - DISTRIBUIDORA DE MEDICAMENTOS</td>\n",
       "      <td>PANVEL FARMACIAS</td>\n",
       "      <td>dimed distribuidora medicamentos</td>\n",
       "      <td>panvel farmacias</td>\n",
       "      <td>92665611068995</td>\n",
       "      <td>SC</td>\n",
       "      <td>SCHNEIDER &amp; CIA LTDA</td>\n",
       "      <td>SCHNEIDER &amp; CIA</td>\n",
       "      <td>82646803000182</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[SCHNEIDER &amp; CIA LTDA, BERTAN ADM DE BENS LTDA...</td>\n",
       "      <td>[SCHNEIDER &amp; CIA, BERTAN ADM DE BENS, C.C. CAN...</td>\n",
       "      <td>[82646803000182, 57314495000140, 4930083000012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114369</th>\n",
       "      <td>frisxmann</td>\n",
       "      <td>FRISXMANN</td>\n",
       "      <td>DIAGNOSTICOS DA AMERICA S.A .</td>\n",
       "      <td>FRISCHMANN AISENGART</td>\n",
       "      <td>diagnosticos america</td>\n",
       "      <td>frischmann aisengart</td>\n",
       "      <td>61486650040487</td>\n",
       "      <td>PR</td>\n",
       "      <td>M ROSENMANN JOALHEIROS S/A</td>\n",
       "      <td>M ROSENMANN JOALHEIROS</td>\n",
       "      <td>76560168000547</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[M ROSENMANN JOALHEIROS S/A, M ROSENMANN JOALH...</td>\n",
       "      <td>[M ROSENMANN JOALHEIROS, M ROSENMANN JOALHEIRO...</td>\n",
       "      <td>[76560168000547, 76560168006235, 7656016800011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264852</th>\n",
       "      <td>cooperativa fronteiras</td>\n",
       "      <td>COOPERATIVA FRONTEIRAS</td>\n",
       "      <td>COOPERATIVA DE CREDITO POUPANCA E INVESTIMENTO...</td>\n",
       "      <td>UNIDADE DE ATENDIMENTO DE PRANCHITA-PR</td>\n",
       "      <td>cooperativa credito poupanca investimento fron...</td>\n",
       "      <td>unidade atendimento pranchita pr</td>\n",
       "      <td>82527557001030</td>\n",
       "      <td>PR</td>\n",
       "      <td>COOPERATIVA AGROPECUARIA SAO LOURENCO - CASLO</td>\n",
       "      <td>CASLO</td>\n",
       "      <td>83675918002292</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[COOPERATIVA AGROPECUARIA SAO LOURENCO - CASLO...</td>\n",
       "      <td>[CASLO, ALTO DA XV, NOVA ESPERANCA, PORTAO / M...</td>\n",
       "      <td>[83675918002292, 61012019138683, 6101201906454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52239</th>\n",
       "      <td>sabesp conpanhia</td>\n",
       "      <td>SABESP CONPANHIA</td>\n",
       "      <td>COMPANHIA DE SANEAMENTO BASICO DO ESTADO DE SA...</td>\n",
       "      <td>SABESP</td>\n",
       "      <td>saneamento basico estado sao paulo sabesp</td>\n",
       "      <td>sabesp</td>\n",
       "      <td>43776517058854</td>\n",
       "      <td>SP</td>\n",
       "      <td>ASSOCIACAO SABESP</td>\n",
       "      <td>ASSOC. SABESP - DIR. REG. VALE DO PARAIBA</td>\n",
       "      <td>49750839000306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[ASSOCIACAO SABESP, ASSOCIACAO SABESP, FANTINA...</td>\n",
       "      <td>[ASSOC. SABESP - DIR. REG. VALE DO PARAIBA, AS...</td>\n",
       "      <td>[49750839000306, 49750839000489, 5514498200018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98022</th>\n",
       "      <td>drogaria</td>\n",
       "      <td>DROGARIA</td>\n",
       "      <td>FARMACIA E DROGARIA NISSEI S.A</td>\n",
       "      <td>DROGARIA NISSEI</td>\n",
       "      <td>farmacia drogaria nissei</td>\n",
       "      <td>drogaria nissei</td>\n",
       "      <td>79430682014344</td>\n",
       "      <td>PR</td>\n",
       "      <td>DROGARIA SAO PAULO S.A.</td>\n",
       "      <td>DROGARIA SAO PAULO</td>\n",
       "      <td>61412110029560</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[DROGARIA SAO PAULO S.A., RAIA DROGASIL S/A, R...</td>\n",
       "      <td>[DROGARIA SAO PAULO, DROGARAIA, DROGARAIA, DRO...</td>\n",
       "      <td>[61412110029560, 61585865205390, 6158586524151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234917</th>\n",
       "      <td>farmasia nosa senhora</td>\n",
       "      <td>FARMASIA NOSA SENHORA</td>\n",
       "      <td>FARMACIA NOSSA SENHORA DO ROSARIO LTDA - EM RE...</td>\n",
       "      <td>FARMACIA ROSARIO</td>\n",
       "      <td>farmacia nossa senhora rosario recuperacao jud...</td>\n",
       "      <td>farmacia rosario</td>\n",
       "      <td>59603977003323</td>\n",
       "      <td>SP</td>\n",
       "      <td>FARMACIA E DROGARIA NISSEI S.A</td>\n",
       "      <td>FARMACIA E DROGARIA NISSEI LTDA</td>\n",
       "      <td>79430682023505</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[FARMACIA E DROGARIA NISSEI S.A, FARMACIA E DR...</td>\n",
       "      <td>[FARMACIA E DROGARIA NISSEI LTDA, FARMACIA E D...</td>\n",
       "      <td>[79430682023505, 79430682023840, 7943068203461...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    user_input  user_input_not_cleaned  \\\n",
       "181538             deus franca             DEUS FRANCA   \n",
       "36661        sao joao faramsia       SAO JOAO FARAMSIA   \n",
       "167507                  bras i             BRAS DE A I   \n",
       "11988     kadrangular ebanjeio    KADRANGULAR EBANJEIO   \n",
       "60444                    pavel                   PAVEL   \n",
       "...                        ...                     ...   \n",
       "114369               frisxmann               FRISXMANN   \n",
       "264852  cooperativa fronteiras  COOPERATIVA FRONTEIRAS   \n",
       "52239         sabesp conpanhia        SABESP CONPANHIA   \n",
       "98022                 drogaria                DROGARIA   \n",
       "234917   farmasia nosa senhora   FARMASIA NOSA SENHORA   \n",
       "\n",
       "                                  razaosocial_not_cleaned  \\\n",
       "181538  IGREJA EVANGELICA ASSEMBLEIA DE DEUS EM FRANCA-SP   \n",
       "36661                 COMERCIO DE MEDICAMENTOS BRAIR LTDA   \n",
       "167507  ASSOCIACAO BRASILEIRA D'A IGREJA DE JESUS CRIS...   \n",
       "11988                    IGREJA DO EVANGELHO QUADRANGULAR   \n",
       "60444           DIMED S/A - DISTRIBUIDORA DE MEDICAMENTOS   \n",
       "...                                                   ...   \n",
       "114369                      DIAGNOSTICOS DA AMERICA S.A .   \n",
       "264852  COOPERATIVA DE CREDITO POUPANCA E INVESTIMENTO...   \n",
       "52239   COMPANHIA DE SANEAMENTO BASICO DO ESTADO DE SA...   \n",
       "98022                      FARMACIA E DROGARIA NISSEI S.A   \n",
       "234917  FARMACIA NOSSA SENHORA DO ROSARIO LTDA - EM RE...   \n",
       "\n",
       "                                nome_fantasia_not_cleaned  \\\n",
       "181538                                 ASSEMBLEIA DE DEUS   \n",
       "36661                                  SAO JOAO FARMACIAS   \n",
       "167507  ASSOC BRAS DE A I DE JESUS CRISTO DOS SANTOS D...   \n",
       "11988                   CRUZADA NACIONAL DE EVANGELIZACAO   \n",
       "60444                                    PANVEL FARMACIAS   \n",
       "...                                                   ...   \n",
       "114369                               FRISCHMANN AISENGART   \n",
       "264852             UNIDADE DE ATENDIMENTO DE PRANCHITA-PR   \n",
       "52239                                              SABESP   \n",
       "98022                                     DROGARIA NISSEI   \n",
       "234917                                   FARMACIA ROSARIO   \n",
       "\n",
       "                                              razaosocial  \\\n",
       "181538        igreja evangelica assembleia deus franca sp   \n",
       "36661                                  medicamentos brair   \n",
       "167507  associacao brasileira d a igreja jesus cristo ...   \n",
       "11988                       igreja evangelho quadrangular   \n",
       "60444                    dimed distribuidora medicamentos   \n",
       "...                                                   ...   \n",
       "114369                               diagnosticos america   \n",
       "264852  cooperativa credito poupanca investimento fron...   \n",
       "52239           saneamento basico estado sao paulo sabesp   \n",
       "98022                            farmacia drogaria nissei   \n",
       "234917  farmacia nossa senhora rosario recuperacao jud...   \n",
       "\n",
       "                                  nome_fantasia            cnpj  uf  \\\n",
       "181538                          assembleia deus  47041223002376  SP   \n",
       "36661                        sao joao farmacias  88212113030351  RS   \n",
       "167507  assoc bras i jesus cristo santos u dias  61012019060986  BA   \n",
       "11988            cruzada nacional evangelizacao  62955505108952  AL   \n",
       "60444                          panvel farmacias  92665611068995  SC   \n",
       "...                                         ...             ...  ..   \n",
       "114369                     frischmann aisengart  61486650040487  PR   \n",
       "264852         unidade atendimento pranchita pr  82527557001030  PR   \n",
       "52239                                    sabesp  43776517058854  SP   \n",
       "98022                           drogaria nissei  79430682014344  PR   \n",
       "234917                         farmacia rosario  59603977003323  SP   \n",
       "\n",
       "                              top_1_razaosocial_retrieved  \\\n",
       "181538                     NOVA FRANCA PARTICIPACOES LTDA   \n",
       "36661   ASSOCIACAO BRASILEIRA D'A IGREJA DE JESUS CRIS...   \n",
       "167507                                    OTICAS FAM LTDA   \n",
       "11988                                       KASANOVA LTDA   \n",
       "60444                                SCHNEIDER & CIA LTDA   \n",
       "...                                                   ...   \n",
       "114369                         M ROSENMANN JOALHEIROS S/A   \n",
       "264852      COOPERATIVA AGROPECUARIA SAO LOURENCO - CASLO   \n",
       "52239                                   ASSOCIACAO SABESP   \n",
       "98022                             DROGARIA SAO PAULO S.A.   \n",
       "234917                     FARMACIA E DROGARIA NISSEI S.A   \n",
       "\n",
       "                     top_1_nomefantasia_retrieved top_1_cnpj_retrieved  \\\n",
       "181538                    NOVA FRANCA IMOBILIARIA       47998745000155   \n",
       "36661                      SAO FRANCISCO DE PAULA       61012019158366   \n",
       "167507                                 OTICAS FAM       53796458000165   \n",
       "11988                                    KASANOVA       54045999000114   \n",
       "60444                             SCHNEIDER & CIA       82646803000182   \n",
       "...                                           ...                  ...   \n",
       "114369                     M ROSENMANN JOALHEIROS       76560168000547   \n",
       "264852                                      CASLO       83675918002292   \n",
       "52239   ASSOC. SABESP - DIR. REG. VALE DO PARAIBA       49750839000306   \n",
       "98022                          DROGARIA SAO PAULO       61412110029560   \n",
       "234917            FARMACIA E DROGARIA NISSEI LTDA       79430682023505   \n",
       "\n",
       "        top_1_razaosocial_pred  top_1_nomefantasia_pred  top_1_cnpj_pred  \\\n",
       "181538                   False                    False            False   \n",
       "36661                    False                    False            False   \n",
       "167507                   False                    False            False   \n",
       "11988                    False                    False            False   \n",
       "60444                    False                    False            False   \n",
       "...                        ...                      ...              ...   \n",
       "114369                   False                    False            False   \n",
       "264852                   False                    False            False   \n",
       "52239                    False                    False            False   \n",
       "98022                    False                    False            False   \n",
       "234917                   False                    False            False   \n",
       "\n",
       "        top_5_razaosocial_pred  top_5_nomefantasia_pred  top_5_cnpj_pred  \\\n",
       "181538                   False                    False            False   \n",
       "36661                    False                    False            False   \n",
       "167507                   False                    False            False   \n",
       "11988                    False                    False            False   \n",
       "60444                    False                    False            False   \n",
       "...                        ...                      ...              ...   \n",
       "114369                   False                    False            False   \n",
       "264852                   False                    False            False   \n",
       "52239                    False                    False            False   \n",
       "98022                    False                    False            False   \n",
       "234917                   False                    False            False   \n",
       "\n",
       "                                   top_k_razaosocial_pred  \\\n",
       "181538  [NOVA FRANCA PARTICIPACOES LTDA, CONFECCOES AL...   \n",
       "36661   [ASSOCIACAO BRASILEIRA D'A IGREJA DE JESUS CRI...   \n",
       "167507  [OTICAS FAM LTDA, BRASKEM S.A, RI HAPPY BRINQU...   \n",
       "11988   [KASANOVA LTDA, MINISTERIO IGREJA VIVA, TECNOL...   \n",
       "60444   [SCHNEIDER & CIA LTDA, BERTAN ADM DE BENS LTDA...   \n",
       "...                                                   ...   \n",
       "114369  [M ROSENMANN JOALHEIROS S/A, M ROSENMANN JOALH...   \n",
       "264852  [COOPERATIVA AGROPECUARIA SAO LOURENCO - CASLO...   \n",
       "52239   [ASSOCIACAO SABESP, ASSOCIACAO SABESP, FANTINA...   \n",
       "98022   [DROGARIA SAO PAULO S.A., RAIA DROGASIL S/A, R...   \n",
       "234917  [FARMACIA E DROGARIA NISSEI S.A, FARMACIA E DR...   \n",
       "\n",
       "                                  top_k_nomefantasia_pred  \\\n",
       "181538  [NOVA FRANCA IMOBILIARIA, CONFECCOES ALVARO, E...   \n",
       "36661   [SAO FRANCISCO DE PAULA, SUPERMERCADO CARANGOL...   \n",
       "167507  [OTICAS FAM, BRASKEM, HAPPY, SALA DE VENDAS SE...   \n",
       "11988   [KASANOVA, MINISTERIO IGREJA VIVA, TECBAN, NUT...   \n",
       "60444   [SCHNEIDER & CIA, BERTAN ADM DE BENS, C.C. CAN...   \n",
       "...                                                   ...   \n",
       "114369  [M ROSENMANN JOALHEIROS, M ROSENMANN JOALHEIRO...   \n",
       "264852  [CASLO, ALTO DA XV, NOVA ESPERANCA, PORTAO / M...   \n",
       "52239   [ASSOC. SABESP - DIR. REG. VALE DO PARAIBA, AS...   \n",
       "98022   [DROGARIA SAO PAULO, DROGARAIA, DROGARAIA, DRO...   \n",
       "234917  [FARMACIA E DROGARIA NISSEI LTDA, FARMACIA E D...   \n",
       "\n",
       "                                          top_k_cnpj_pred  \n",
       "181538  [47998745000155, 43306715000180, 6070119031004...  \n",
       "36661   [61012019158366, 94739992000117, 6101201910940...  \n",
       "167507  [53796458000165, 42150391003005, 5873166200518...  \n",
       "11988   [54045999000114, 49232475000100, 5142710202988...  \n",
       "60444   [82646803000182, 57314495000140, 4930083000012...  \n",
       "...                                                   ...  \n",
       "114369  [76560168000547, 76560168006235, 7656016800011...  \n",
       "264852  [83675918002292, 61012019138683, 6101201906454...  \n",
       "52239   [49750839000306, 49750839000489, 5514498200018...  \n",
       "98022   [61412110029560, 61585865205390, 6158586524151...  \n",
       "234917  [79430682023505, 79430682023840, 7943068203461...  \n",
       "\n",
       "[233 rows x 20 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df['top_5_razaosocial_pred'] == False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
